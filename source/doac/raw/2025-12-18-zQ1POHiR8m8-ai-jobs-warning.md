You're one of the three godfathers of AI, the most cited scientist on Google Scholar, but I also read that you're an introvert. It begs the question, why have you decided to step out of your introversion? >> Because I have something to say. I've become more hopeful that there is a technical solution to build AI that will not harm people and could actually help us. Now, how do we get there? Well, I have to say something important here. Professor Yoshua Benjio is one of the pioneers of AI, >> whose groundbreaking research earned him the most prestigious honor in computer science. He's now sharing the urgent next steps that could determine the future of our world. >> Is it fair to say that you're one of the reasons that this software exists amongst others? Yes. >> Do you have any regrets? >> Yes. I should have seen this coming much earlier, but I didn't pay much attention to the potentially catastrophic risks. But my turning point was when Chad GPT came and also with my grandson. I realized that it wasn't clear if he would have a life 20 years from now because we're starting to see AI systems that are resisting being shut down. We've seen pretty serious cyber attacks and people becoming emotionally attached to their chatbot with some tragic consequences. >> Presumably, they're just going to get safer and safer, though. >> So, the data shows that it's been in the other direction is showing bad behavior that goes against our instructions. So of all the existential risks that sit there before you on these cards, is there one that you're most concerned about in the near term? >> So there is a risk that doesn't get discussed enough and it could happen pretty quickly and that is but let me throw a bit of optimism into all this because there are things that can be done. >> So if you could speak to the top 10 CEOs of the biggest AI companies in America, what would you say to them? >> So I have several things I would say. I see messages all the time in the comment section that some of you didn't realize you didn't subscribe. So, if you could do me a favor and double check if you're a subscriber to this channel, that would be tremendously appreciated. It's the simple, it's the free thing that anybody that watches this show frequently can do to help us here to keep everything going in this show in the trajectory it's on. So, please do double check if you've subscribed and uh thank you so much because in a strange way, you are you're part of our history and you're on this journey with us and I appreciate you for that. So, yeah, thank you. Professor Joshua Benjio, you're I hear one of the three godfathers of AI. I also read that you're one of the most cited scientists in the world on Google Scholar, the actually the most cited scientist on Google Scholar and the first to reach a million citations. But I also read that you're an introvert and um it begs the question why an introvert would be taking the step out into the public eye to have conversations with the masses about their opinions on AI. Why have you decided to step out of your uh introversion into the public eye? Because I have to. because since Chant GPT came out um I realized that we were on a dangerous path and I needed to speak. I needed to uh raise awareness about what could happen but also to give hope that uh you know there are some paths that we could choose in order to mitigate those catastrophic risks. >> You spent four decades building AI. Yes. >> And you said that you started to worry about the dangers after chat came out in 2023. >> Yes. >> What was it about Chat GPT that caused your mind to change or evolve? >> Before Chat GPT, most of my colleagues and myself felt it would take many more decades before we would have machines that actually understand language. Alan Turing, founder of the field in 1950, thought that once we have machines that understand language, we might be doomed because they would be as intelligent as us. He wasn't quite right. So, we have machines now that understand language and they but they lag in other ways like planning. So they're not for now a real threat, but they could in in a few years or a decade or two. So it it is that realization that we were building something that could become potentially a competitor to humans or that could be giving huge power to whoever controls it and and destabilizing our world um threatening our democracy. All of these scenarios suddenly came to me in the early weeks of 2023 and I I realized that I I had to do something everything I could about it. >> Is it fair to say that you're one of the reasons that this software exists? You amongst others. amongst others. Yes. Yes. >> I'm fascinated by the like the cognitive dissonance that emerges when you spend much of your career working on creating these technologies or understanding them and bringing them about and then you realize at some point that there are potentially cat catastrophic consequences and how you kind of square the two thoughts. >> It is difficult. It is emotionally difficult. And I think for many years I was reading about the potential risks. Um uh I had a student who was very concerned but I didn't pay much attention and I think it's because I was looking the other way. It and it's natural. It's natural when you want to feel good about your work. We all want to feel good about our work. So I wanted to feel good about the all the research I had done. I you know I was enthusiastic about the positive benefits of AI for society. So when somebody comes to you and says oh the sort of work we you've done could be extremely destructive uh there's sort of unconscious reaction to push it away. But what happened after Chant GPG came out is really another emotion that countered this emotion and that other emotion was the love of my children. I realized that it wasn't clear if they would have a life 20 years from now, if they would live in a democracy 20 years from now. And Having realized this and continuing on the same path was impossible. It was unbearable. Even though that meant going against the fray, against the the wishes of my colleagues who would rather not hear about the dangers of what we were doing. >> Unbearable. >> Yeah. Yeah. I you know I remember one particular afternoon and I was uh taking care of my grandson uh who's just you know u a bit more than a year old. How could I like not take this seriously? Like I he you know our children are so vulnerable. So, you know that something bad is coming, like a fire is coming to your house. You see, you're not sure if it's going to pass by and and leave your your house untouched or if it's going to destroy your house and you have your children in your house. Do you sit there and continue business as usual? You can't. You have to do anything in your power to try to mitigate the risks. >> Have you thought in terms of probabilities about risk? Is that how you think about risk is in terms of like probabilities and timelines or >> of course but I have to say something important here. This is a case where previous generations of scientists have talked about a notion called the precautionary principle. So what it means is that if you're doing something say a scientific experiment and it could turn out really really bad like people could die some catastrophe could happen then you should not do it for the same reason there are experiments that uh scientists are not doing right now. We we're not playing with the atmosphere to try to fix climate change because we we might create more harm than than than actually fixing the problem. We are not praying creating new forms of life that could you know destroy us all even though is something that is now conceived by biologists because the risks are so huge but in AI it isn't what's currently happening. We're we're we're taking crazy risks. But the important point here is that even if it was only a 1% probability, let's say just to give a number, even that would be unbearable would would be unacceptable. Like a 1% probability that our world disappears, that humanity disappears or that uh a worldwide dictator takes over thanks to AI. These sorts of scenarios are so catastrophic that even if it was 0.1% would still be unbearable. Uh and in many polls for example of machine learning researchers the people who are building these things the numbers are much higher like we're talking more like 10% or something of that order which means we should be just like paying a whole lot more attention to this than we currently are as a society. There's been lots of predictions over the centuries about how certain technologies or new inventions would cause some kind of existential threat to all of us. So a lot of people would rebuttle the the risks here and say this is just another example of change happening and people being uncertain so they predict the worst and then everybody's fine. Why is that not a valid argument in this case in your view? Why is that underestimating the potential of AI? >> There are two aspects to this. experts disagree and they range in their estimates of how likely it's going to be from like tiny to 99%. So that's a very large bracket. So if let's say I'm not a scientist and I hear the experts disagree among each other and some of them say it's like very likely and some say well maybe you know uh it's plausible 10% and others say oh no it's impossible or it's so small. Well what does that mean? It means that we don't have enough information to know what's going to happen. But it is plausible that one of you know the uh more pessimistic people in in the lot are are right because there is no argument that either side has found to deny the the possibility. I don't know of any other um existential threat that we could do something about um that that has these characteristics. Do you not think at this point we're kind of just the the train has left the station? Because when I think about the incentives at play here and I think about the geopolitical, the domestic incentives, the corporate incentives, the competition at every level, countries raising each other, corporations racing each other. It feels like we're now just going to be a victim of circumstance to some degree. I think it would be a mistake to let go of our agency while we still have some. I think that there are ways that we can improve our chances. Despair is not going to solve the problem. There are things that can be done. Um we can work on technical solutions. That's what I spending I'm spending a large fraction of my time. and we can work on policy and public awareness um and you know societal solutions and that's the other part of what I'm doing right let's say you know that something catastrophic would happen and you think uh you know there's nothing to be done but actually there's maybe nothing that we know right now that gives us a guarantee that we can solve the problem but maybe we can go from 20% chance of uh catastrophic outcome to 10%. Well, that would be worth it. Anything any one of us can do to move the needle towards greater chances of a good future for our children, we should do. >> How should the average person who doesn't work in the industry or isn't in academia in AI think about the advent and invention of this technology? Is are there kind of an analogy or metaphor that is equivocal to the profoundity of this technology? >> So one analogy that people use is we might be creating a new form of life that could be smarter than us and we're not sure if we'll be able to make sure it doesn't, you know, harm us that we'll control it. So it would be like creating a new species uh that that could decide to do good things or bad things with us. So that's one analogy, but obviously it's not biological life. >> Does that matter? >> In my scientific view, no. I don't care about the definition one chooses for, you know, some some some system. Is it alive or is it not? What matters is is it going to harm people in ways? Is this going to harm my children? I'm coming to the idea that we should consider alive any entity which is able to preserve itself and working towards preserving itself in spite of the obstacles on the road. We are starting to see this. We're starting to see AI systems that don't want to be shut down, that are resisting being shut down. And right now, of course, the we we we can shut them down, but but if they be continue to go in the direction of more and more intelligence and capability and they continue to have this drive to live, we could be in trouble. >> When you say AI systems that don't want to be shut down and are resisting attempts to shut them down, can you give me some examples of of this? Yes, I assume people have been experimenting with chat bots, right? You can now have these agent chat bots which can read from uh files in your computer um can uh execute commands in in the computer and so on. So with these systems, you can plant information in the files that they have access to, false information, such as emails that say that the AI is going to be replaced by a new version. So now the AI knows that we're planning to shut it down. And we can, it turns out, we can read its thought. It has these internal verbalizations that we call chains of thoughts. And then we see that it's planning to do something about it. And then it does something about it. Depending on the circumstances, it might try to uh copy its code in a different computer or in place of the new version or it might try to blackmail the engineer in charge of the change in version. So these systems understand that we want to shut them down and they try to resist. >> When someone hears that and with knowledge of how previous technology was built, I immediately think, well, who put that in the code? >> Unfortunately, we don't put these things in the code. That's part of the problem. The problem is we grow these systems by giving them data and making them learn from it. Now a lot of that training process boils down to imitating people because they take all the text that people have written, all the uh tweets and and you know all the Reddit's comments and so on and they internalize the kind of uh drives that human have including the the drive to preserve oneself and and the drive to have more control over their environment so that they can achieve whatever goal we give them. It's not like normal code. It's more like you're raising a baby tiger and you you you know, you feed it. You you let it experience things. Sometimes, you know, it does things you don't want. It's okay. It's still a baby, but it's growing. So when I think about something like chatbt, is there like a core intelligence at the heart of it? Like the the core of the model that is a black box and then on the outsides we've kind of taught it what we want it to do. How does it It's mostly a black box. Everything in the neural net is is essentially a black box. Now the part as you say that's on the outside is that we also give it verbal instructions. We we type these are good things to do. These are things you shouldn't do. Don't help anybody build a bomb. Okay. Unfortunately with the current state of the technology right now it doesn't quite work. Um people find a way to bypass those barriers. So these those instructions are not very effective. But if I typed don't how to help me make a bomb on chatbt now it's not going to >> Yes. So but that and there are two reasons why it's going to not do it. One is because it was given explicit instructions to not do it and and usually it works and the other is in addition there's an extra because because that layer doesn't work uh sufficiently well there's also that extra layer we were talking about. So those monitors, they're they're filtering the queries and the answers and and if they detect that the AI is about to give information about how to build a bomb, they're supposed to stop it. But again, even that layer is imperfect. Uh recently there was um a series of cyber attacks by what looks like a you know a an organization that was state sponsored that has used Anthropics AI system in other words through the cloud right it's not it's not a private system it's they're using the the system that is public they used it to prepare and launch pretty serious cyber attacks So even though entropic system is supposed to prevent that. So it's trying to detect that somebody is trying to use their system for doing something illegal. Those protections don't work well enough. Presumably they're just going to get safer and safer though these systems because they're getting more and more feedback from humans. They're being trained more and more to be safe and to not do things that are unproductive to humanity. I hope so. But we can we count on that? So actually the data shows that it's been in the other direction. So since those models have become better at reasoning more or less about a year ago, they show more misaligned behavior like uh bad behavior that that that goes against our instructions. And we don't know for sure why, but one possibility is simply that now they can reason more. That means they can strategize more. That means if they have a goal that could be something we don't want. They're now more able to achieve it than they were previously. They're also able to think of unexpected ways of of of doing bad things like the uh case of blackmailing the engineer. There was no suggestion to blackmail the engineer, but they they found an email giving a clue that the engineer had an affair. And from just that information, the AI thought, aha, I'm going to write an email. And he did. It it did sorry uh to to to try to warn the engineer that the the information would go public if if uh the AI was shut down. >> It did that itself. >> Yes. So they're better at strategizing towards bad goals. And so now we see more of that. Now I I do hope that more researchers and more companies will will uh invest in improving the safety of these systems. Uh but I'm not reassured by the path on which we are right now. >> The people that are building these systems, they have children too. >> Yeah. >> Often. I mean thinking about many of them in my head, I think pretty much all of them have children themselves. They're family people. if they are aware that there's even a 1% chance of this risk, which does appear to be the case when you look at their writings, especially before the last couple of years, seems to there seems to be been a bit of a narrative change in more recent times. Um, why are they doing this anyway? >> That's a good question. I can only relate to my own experience. Why did I not raise the alarm before Chat GPT came out? I I had read and heard a lot of these catastrophic arguments. I think it's just human nature. We we're not as rational as we'd like to think. We are very much influenced by our social environment, the people around us, um our ego. We want to feel good about our work. Uh we want others to look upon us, you know, as a you know, doing something positive for the world. So there are these barriers and by the way we see those things happening in many other domains and you know in politics uh why is it that uh conspiracy theories work? I think it's all connected that our psychology is weak and we can easily fool ourselves. Scientists do that too. They're not that much different. Just this week, the Financial Times reported that Sam Alman, who is the founder of CHPT, OpenAI, has declared a code red over the need to improve chatbt even more because Google and Anthropic are increasingly developing their technologies at a fast rate. Code red. It's funny because the last time I heard the phrase code red in the world of tech was when chatt first released their their model and Sergey and Larry I I heard had announced code red at Google and had run back in to make sure that chat don't destroy their business. And this I think speaks to the nature of this race that we're in. >> Exactly. And it is not a healthy race for all the reasons we've been discussing. So what would be a more healthy scenario is one in which we try to abstract away these commercial pressures. They're they're they're in survival mode, right? And think about both the scientific and the societal problems. The question I've been focusing on is let's go back to the drawing board. Can we train those AI systems so that by construction they will not have bad intentions. Right now the way that this problem is being looked at is oh we're not going to change how they're trained because it's so expensive and you know we spend so much engineering on it. which is going to patch some partial solutions that are going to work on a case- by case basis. But that's that's going to fail and we can see it failing because some new attacks come or some new problems come and it was not anticipated. So I think things would be a lot better if the whole research program was done in a context that's more like what we do in academia or if we were doing it with a public mission in mind because AI could be extremely useful. There's no question about it. uh I've been involved in the last decade in thinking about working on how we can apply AI for uh you know uh medical advances uh drug discovery the discovery of new materials for helping with uh you know climate issues. There are a lot of good things we could do. Uh, education um and and but this might may not be what is the most short-term profitable direction. For example, right now where are they all racing? They're racing towards replacing jobs that people do because there's like quadrillions of dollars to be made by doing that. Is that what people want? Is that going to make people have a better life? We don't know really. But what we know is that it's very profitable. So we should be stepping back and thinking about all the risks and then trying to steer the developments in a good direction. Unfortunately, the forces of market and the forces of competition between countries don't do that. >> And I mean there has been attempts to pause. I remember the letter that you signed amongst many other um AI researchers and industry professionals asking for a pause. Was that 2023? >> Yes. >> You signed that letter in 2023. Nobody paused. >> Yeah. And we had another letter just a couple of months ago saying that we should not build super intelligence unless two conditions are met. There's a scientific consensus that it's going to be safe and there's a social acceptance because you know safety is one thing but if it destroys the way you know our cultures or our society work then that's not good either. But these voices are not powerful enough to counter the forces of competition between corporations and countries. I do think that something can change the game and that is public opinion. That is why I'm spending time with you today. That is why I'm spending time explaining to everyone what is the situation, what are what are the plausible scenarios from a scientific perspective. That is why I've been involved in chairing the international AI safety report where 30 countries and about 100 experts have worked to uh synthesize the state of the science regarding the risks of AI especially the frontier AI so that policy makers would know the facts uh outside of the you know commercial pressures and and you know the the the discussions that are not always very uh serene that can happen around AI. In my head, I was thinking about the different forces as arrows in in in a race. And each arrow, the length of the arrow represents the amount of force behind that particular um incentive or that particular movement. And the sort of corporate arrow, the capitalistic arrow, the amount of capital being invested in these systems, hearing about the tens of billions being thrown around every single day into different AI models to try and win this race is the biggest arrow. And then you've got the sort of geopolitical US versus other countries, other countries versus the US. That arrow is really, really big. That's a lot of force and effort and reason as to why that's going to persist. And then you've got these smaller arrows, which is, you know, the people warning that things might go catastrophically wrong. And maybe the other small arrows like public opinion turning a little bit and people getting more and more concerned about >> I think public opinion can make a big difference. Think about nuclear war. >> Yeah. In the middle of the Cold War, the US and the USSR uh ended up agreeing to be more responsible about these weapons. There was a a a movie the day after about nuclear catastrophe that woke up a lot of people including in government. When people start understanding at an emotional level what this means, things can change and governments do have power. They could mitigate the risks. I guess the rebuttal is that, you know, if you're in the UK and there's a uprising and the government mitigates the risk of AI use in the UK, then the UK are at risk of being left behind and we'll end up just, I don't know, paying China for that AI so that we can run our factories and drive our cars. >> Yes. So, it's almost like if you're the safest nation or the safest company, all you're doing is is blindfolding yourself in a race that other people are going to continue to run. So, I have several things to say about this. Again, don't despair. Think, is there a way? So first obviously we need the American public opinion to understand these things because that's going to make a big difference and the Chinese public opinion. Second, in other countries like the UK where governments are a bit more concerned about the uh societal implications. They could play a role in the international agreements that could come one day, especially if it's not just one nation. So let's say that 20 of the richest nations on earth outside of the US and China come together and say we have to be careful. better than that. Um they could invest in the kind of technical research and preparations at a societal level so that we can turn the tide. Let me give you an example which motivates uh law zero in particular. >> What's law zero? >> Law zero is sorry. Yeah, it it is the nonprofit uh R&amp;D organization that I created in June this year. And the mission of law zero is to develop uh a different way of training AI that will be safe by construction even when the capabilities of AI go to potentially super intelligence. The companies are focused on that competition. But if somebody gave them a way to train their system differently, that would be a lot safer, there's a good chance they would take it because they don't want to be sued. They don't want to, you know, uh to to to have accidents that would be bad for their reputation. So, it's just that right now they're so obsessed by that race that they don't pay attention to how we might be doing things differently. So other countries could contribute to to these kinds of efforts. In addition, we can prepare um for days when say the um US and and Chinese public opinions have shifted sufficiently so that we'll have the right instruments for international agreements. One of these instruments being what kind of agreements would make sense, but another is technical. um uh how can we change at the software and hardware level these systems so that even though the Americans won't trust the Chinese and the Chinese won't trust the Americans uh there is a way to verify each other that is acceptable to both parties and so these treaties can be not just based on trust but also on mutual verification. So there are things that can be done so that if at some point you know we are in in a better position in terms of uh governments being willing to to really take it seriously uh we can move quickly. When I think about time frames and I think about the administration the US has at the moment and what the US administration has signaled, it seems to be that they see it as a race and a competition and that they're going hell for leather to support all of the AI companies in beating China >> and beating the world really and making the United States the global home of artificial intelligence. Um, so many huge investments have been made. I I have the visuals in my head of all the CEOs of these big tech companies sitting around the table with Trump and them thanking him for being so supportive in the race for AI. So, and you know, Trump's going to be in power for several years to come now. So, again, is this is this in part wishful thinking to some degree because there's there's certainly not going to be a change in the United States in my view in the coming years. It seems that the powers that be here in the United States are very much in the pocket of the biggest AI CEOs in the world. >> Politics can change quickly >> because of public opinion. >> Yes. Imagine that something unexpected happens and and and we see uh a flurry of really bad things happening. Um we've seen actually over the summer something no one saw coming last year and that is uh a huge number of cases people becoming emotionally attached to their chatbot or their AI companion with sometimes tragic consequences. I know people who have quit their job so they would spend time with their AI. I mean, it's mindboggling how the relationship between people and AIS is evolving as something more intimate and personal and that can pull people away from their usual activities with issues of psychosis, um, suicide, um, and and and u other issues with the effects on children and uh, uh, you know, uh, sexual imagery for for ch from children's bodies like we there's like things happening that could change public opinion and I'm not saying this one will but we already see a shift and by the way across the political spectrum in the US because of these events. So, as I saying, we we can't really be sure about how public opinion will evolve, but but I think we should help educate the public and also be ready for a time when public opinion will shift so we don't lose time. You know there are sometimes in history where we can we have these windows of opportunity. So climate for example, we're losing the window. Time is running out. In AI, we still have some time. We don't know exactly how much, but we have to move quickly, for sure. >> Does it not concern you that you've described a situation in which the general population will only care about this and governments will only care about this when something catastrophic has happened and the horses are already bolting and the door is already open? I mean the I think about some of the examples you've given today, whether it's the nuclear example or climate change. These are examples where catastrophe has already happened. People died from nuclear bombs before anyone gave a and climate change, I mean we're literally seeing fires in Los Angeles right now raging. >> Yes. >> It seems to be a common theme with humans that we never learn until it's too late that they'll only get on board once it's too late. >> I I think you're right. And there's some there's an urgency here I think that is greater than in those cases because with climate we have a bit more control. We could switch back. If we if we start seeing really bad things, we can reduce CO2 emissions quickly. We can do things. We know what we have to do to eventually back down to a good equilibrium. With AI, we may not have control when things start getting really bad. The AI could actually essentially take control. They could escape, if you wish. And once they're spreading in the environment and smarter than us, then we are no longer in the game. You mentioned there earlier about how there's been recent sort of cyber attacks from state actors using anthropic systems. And it sounds to me from your general position on AI and your understanding of these tools that it would be difficult to dissuade a bad actor from weaponizing these AI tools. They seem like an incredible tool to be weaponized to use for mass manipulation in a way which is completely unique and different to anything we've seen before in human history. How how should I think about the the vulnerability that we're created here? So from my understanding because I know you said that the AI's going through the cloud but does that mean a bad actor with a computer could access the system and use it for weaponization? So there are two scenarios and both of them are scary. In one scenario the bad actor accesses the systems in the cloud. That could still do a lot of damage as we saw in those cyber attacks. But it's true that in that cloud there are monitoring systems which like will prevent the worst things like we were saying no bombs. The other scenario is that somehow the bad actor gains access to the the model itself the weights as we say so then you know once you have the computer files you can download them and then there are no more protection systems and you can do anything you wish with the system. >> So there's no bombs. So there are monitoring systems in the cloud. But those monitoring systems do not work well enough. I mean, those systems are not perfect. People find a way to bypass them. And so so we saw somebody with bad intent was able to do uh attacks uh without being stopped by the monitoring system. Cyber attacks? >> Yes. And we now see an evolution. So it started with social engineering type of attacks such as deepfakes of prominent people making them say things they never say and and in order to move people's opinion in a certain way and influence elections and so on. And then we we we're going to see more of that. Then we see cyber attacks which are kind of more like traditional cyber attacks where they they try to get into a system and and and cause chaos or harm um or blackmail people. Then we can imagine terrorism. When we talk to security experts, they tell us about plausible scenarios where a lot of damage could be done by people with bad intent using a powerful AI. And then you think going a little bit further in the future when AIs could help create new pathogens. So we could have a bioterrorism scenario. So again, if you just had access to the cloud, you couldn't do that because the monitors would say oh no you don't know. But if you had access to to the AI, downloaded the parameters. you could turn those systems towards bad things. And and this is something that could happen right now because a lot of these models are publicated in the public, what is called open source. And there's a big debate right now about whether that is a good idea or a bad idea. I think there are many good things that come out of public release of of AIs but it it also poses a risk in terms of uh basically democratizing access to creating new weapons. Why do we have an international treaty about chemical weapons and biological weapons? Because anyone with very little means could cause a massive amount of damage by doing just one experiment in their basement. So we decided that as civilized societies we needed to have international agreement to prevent that from happening. The same logic would apply to powerful AI systems. Do we allow anybody in the world, any terrorist, any, you know, North Korea or any nation or any any you know group that wants to do bad things to access these powerful tools that right now are being deployed everywhere publicly. You don't need permission. You can download them. Uh and you know it's a it's a bit of a risk. Do these companies have a right or an incentive to do something about this or have they asked the public do they want open source models of of this level of power and the answer they're not asking us and it seems crazy to me. You know people are saying well we're we're in a democracy we want to vote and on issues that are like very important to our society and the companies are going they they're they're they're doing business and they're making choices that could affect all of us. They're not accountable. They they are to their shareholders not to us not to the citizens of the world. Is there a scenario where these AI systems are so bad for the planet that they have to be removed completely? Let's say we detect in five years time that actually whatever we're doing here is no good. Is there any feasible way of like getting these bottles back to the Genies back in the bottle? There probably are certain things in society where we could reverse or, you know, or, or stop doing. For example, we, we had those nuclear treaties. So we stopped, we stopped doing certain things with nuclear weapons. Uh countries didn't agree to completely stop having them but at least to do better to reduce the chances that there would be like a nuclear war and that's been pretty effective for uh for a few decades. But for AI, removing them completely from society is is becoming very hard because there's just so much incentive to use them. And many of the things they do are good. Um, I use them a lot to help me work. I'm writing papers, writing code faster with chatbots and so on. And that's what everybody is going to do. But because the use is so dispersed right now you can't really prevent their use. What could be done though is we can prevent people who have bad intent with getting access to them. So we could create a kind of AI passport or you you can imagine scenarios in which access to certain level of AI could be restricted to people who are identifiable. It's a bit strange because it's sort of you you're putting some security versus privacy trade-off. But there could be ways to do this where people who are well intended could use AIs and people are are people who are, you know, uh, known to be problematic like terrorists, we don't we don't want them to be able to use a powerful system. And so so there are ways to to to try to contain these things but I think if we if we got to see really bad effects of of AI and governments started caring a lot more could we just stop building AIs? And the answer is yes. We could totally stop building more powerful AIs because right now there's only a a few places in the world where we can build the most powerful systems. We, like with nuclear weapons, building nuclear weapons is is a challenge. You can't do it in your basement. You need a lot of infrastructure. And these things can be monitored and governments could and and and and control control the building of very powerful systems. So what is your ask of the general population today? People listening to this, I imagine they've been convinced. Well, I don't know. Maybe not. But I certainly have been convinced. I'm certainly sitting thinking shit. What should the general population be doing right now? How should they be voting with their opinions or their wallet? Talk to people around you. Yeah, that's all I can say. Raise awareness. You know, send this kind of stuff to people you know, to your friends, to people who are, you know, who have influence. Uh and and hopefully we'll get to the level where um uh politicians are going to start caring. And I don't know when or how because we don't understand well as a society what moves governments. But we've seen things shift and we hope that they can keep moving. You know one thing that happened two years ago was I was part of an effort with Mira Morathi from OpenAI and Dario Modeh from Entropic to start raising awareness in government and that it led to the president of the United States having an executive order about making AI development safer and the EU AI act. There's been a lot of activity now by governments around the world. Right now they're mostly in talk mode. They're not doing that much but they're talking and they could be the beginning of more serious action. >> A guest I had on the podcast um earlier this year which was um Aza Raskin he said to me at the end of the conversation and I asked him what the general populations do which is very similar to what I'm asking you and he said look if there's any indication in my behavior that I'm concerned about this it's that I've spent I think it was $30,000 dollars um building a bunker. Uh okay. Which really really stuck with me um and I it just I said to him are you kidding I don't have 30 grand to uh >> Well and I'm not sure the bunker is going to solve the problem. >> Yeah. >> Um and >> I think that although he was a very intelligent guy, I got the sense he was trying to explain to me how in how important it was. And I think the bunker was somewhat of a signal. >> Yeah. To the audience that >> yeah, what he's seeing would concern him enough to spend 30 grand. And, you know, look, I'm glad I'm glad that he raised awareness, uh, but for many of us, you know, I can't do that kind of thing and I don't think it'll be enough anyway. >> So, you know what we have to do is we have to turn the knobs of our society so that it pays more attention to these things and it invests resources where they are needed. You know, I'm I'm thinking about my my own research organization. We could do a lot better if we had more resources. I'm now having a team of 10 people and with 100 people we could do a lot more and faster and that's one example. I'm sure there are other groups around the world who should have those kinds of resources to to contribute to mitigating these these risks. So so that's one thing. Education, you know there's only a handful of universities that are training people to think deeply about the risks of AI and to to try to find solutions to that. We could have a lot more of that. Public education, you know, if this were to be known to a large fraction of the population, then maybe we could have better decisions being made. I hope so. I hope I hope I'm I'm contributing to that by doing this kind of show. So there's some of these levers and I think uh the the amount of resources we're putting right now as a society is is is too small. It's very small. I mean, what do you mean? Like what do you mean like it's significantly under-resourced? Yeah. A lot more should be done basically. And you you what I mean is if you look at the report from the state of AI safety report I was talking about they make a lot of recommendations. Recommendation number one is we we need more research to understand the risks. Like we need to be able to measure what the capabilities of a system are understand where the dangers lie and then find solutions to mitigate those risks. Right now, that kind of of of of work is a very small fraction of the overall activity which is all about making money, beating the competitors and so on. And I totally understand. This is the normal way market works. But for existential risks, market is not the solution. For climate change, we learned that the market does not work. We need governments to to steer us in the right direction and make sure the important things are being done. And I think what we should recognize is that uh our free market approach in which companies decide what they want to do in order to make money uh could could be very dangerous and is dangerous in in certain circumstances especially when we talk about existential risk or societal stability that could threaten democracy. So we have to complement the economic uh approach with a political approach. Is it realistic for someone like me to be sitting there with my future children and saying should I bring them into this world? We we have obviously a choice on that front. Let's assume everything continues on the current trajectory. Is it realistic for me to be sitting there saying should I be thinking about the future of my child in 2040? It's a difficult question. I have you know two kids and I'm a grandfather. Um and I I think about what you know my responsibility as a grandfather is going to look like. You know I'm going to I'm going to say if my grandson asked me, you know, in 20 years, what did you do about all the the bad things that you you you know you knew what happened? What am I going to answer? I won't have a good answer unless I do my best right now to try to change things. And so although I can't um give you reassurance that we're going to be safe um or that we're going to be okay I can say that you know all of us we have our own role to play even if we're not working directly on AI. You know we we should and we you know we we for our own mental health we should act on what we know could be making the world a better place because if we don't we feel miserable right now Despair doesn't help. action helps and maybe we'll find our way through this. maybe governments will start listening but in the meantime let's not just wait. Let's do what we can. If you could speak to the top 10 CEOs of the biggest AI companies in America or the world what would you say to them? So I have several things I would say one is technical so you know the current approaches to aligning AI should be you should stop thinking that they will scale because from what I see from the scientific literature uh these um uh ways to try to patch things um just don't scale to to much higher level of intelligence. We we need to fundamentally change things and I hope they will listen to to the kind of research we're doing with loss zero and and change the way they their system are trained. So that's that's number one. Number two is think beyond quarter to quarter. You'll make a lot more money long term and build a lot more trust with the public and governments and and make them happy in a way that's going to work well for you too. For your children. if you care um not just like a shareholder value but also you about the legacy you will leave. And so that that that approach means that we should be investing more in in these long-term issues and and not just focusing on the short term. The third thing I would say is uh get together and agree on standards about not just safety which there's been a little bit of effort but also about the societal implications of AI because as we said earlier there's going to be incredible disruption in the job market. Do we just want that to happen uh with people losing jobs and not knowing what what to do and maybe chaos in our streets and instability in terms of uh society. That's not good for the companies either. Or or do we want to try to organize society so that these changes happen in a way that's going to make people happy and and feeling fulfilled. So I think there are decisions that need to be made and unfortunately the companies although they influence governments a lot they they can cannot they cannot decide themselves uh some of the the adaptations that would be needed. They could help governments and the public think about these things and try to work together to find the path and this could be in ways that's good for them to do. We've been pretty pessimistic in this conversation not necessarily by design but we've we've certainly arrived in quite a pessimistic place. Is there an argument or a line of reasoning that would give us some hope that things are going to be okay? that the pessimism is unfounded and you know we're about to enter a a golden age? Yeah, I think there's actually uh a really clear path to hope. It's not easy but it it is it requires two things both scientific progress and political and societal action. Let me touch about the science first. So I said earlier that the current approach to building AI is problematic because uh you you can't guarantee that they will have good intentions because we don't give them explicit intentions. They somehow learn from us and we're not perfect so they they learn from the from all the crap they learn on the internet. And they, they, they can pick up bad intentions. So we're now figuring out is it possible to define a way of training AI that would give us a mathematical guarantee that they will not have bad intentions? It's not it's not easy. We don't have it, but we're working on it and there are a bunch of promising ideas. I think that will eventually work. How long it will take? Who knows? But say we have it. The question for society is will they be deployed? Will the companies accept to use this approach? And maybe governments have to force them to do that. I don't know. So the scientific part is going to happen. I'm confident the political part needs the public. The public needs to be aware that there's a problem and a solution and we can encourage them to convince our governments to convince the companies to actually go down this path because otherwise it's not going to happen. So that's where the optimism lies. >> Given the timeline of discovery and then deployment, if the incentives that underpin all of the pessimism we've discussed today are so strong which they seem to be why would we expect the companies to adopt the safer technical solution? >> They will not do it unless governments force them to. There were a lot of examples of that. You know companies don't want to pay taxes but governments force them to. Uh there are a lot of behaviors companies have which which harm societies. And if we you know fortunately we're in democracies it's it's it's the will of the people and the governments that need to will the companies to do the right thing. And this is why companies need government. Right. They want society to continue being you know what they want this the countries they're in to do well. And and for that they can't be totally selfish and self-interest. They have to care about government and and the the the common good. Will they realize that if there isn't catastrophe? We don't know. Unfortunately, a catastrophe may be needed for companies to or the public to wake up and that would be a drag. What what sort of like non-catastrophic event would move public opinion in such a way? >> Uh public opinion has moved a lot like you know and and actually uh in your podcast you know you are an example of that like you you know this is not where we were a few years ago >> when when yeah even even even a couple years ago. So since chat GPT came out we've been sort of having a shift in understanding more and more not just about the benefits but also the risks, the profound risks. And politicians have been convinced. I've myself convinced a number of them. They they get it. And I you know I talk to politicians and policy makers in Europe, in China, in Africa, in Canada, US. So it's happening. It's just that maybe it's not happening fast enough for the things that would need to happen. Things that need to happen to >> the letter about pausing um AI development two years ago >> that letter got support from from a lot of people um around the world not just researchers some of the general population and and it it did have an effect on on the kind of discussion that was happening in government. So these conversations are happening but it's not quite there yet. You know, a few weeks ago I was in the White House with other scientists. We were meeting with National Security Council and so on to talk about these issues. Uh, so, you know, governments are getting informed but they're still in the mode of getting informed and not so much in the mode of action. Um, but I've also talked to politicians and policy makers who are telling me, yes, we need these big international treaties. Yes, we need to, we need to move quickly. We can see, we can see it coming. The problem is that politicians may be four years in power or five years and the treaties, you know, multi-year, multi-year, multi-decade, kind of like with nuclear weapons. Um, so there's a bit of a a timing mismatch but maybe we have to go there. We we we we have no choice like if we we just let the market do its stuff chances are going to end up in a very bad place. I want to talk about a non-existential risk that AI presents in terms of the job market and I want to talk about that in relation to us individually. I I think from like the conversations I've had on this podcast and the work I've looked at the research that I've seen particularly, I think we're going to see people being displaced from creative knowledge-based jobs pretty much disproportionately to other types of jobs. We're already seeing it. I'm sure you've seen examples I've seen millions of them now of people building websites that take 30 seconds to build, coding entire applications in 60 seconds, which would have taken somebody hundreds of hours. Where are these people going to go? What are they meant to do? Yeah, there are a lot of professions. Another example is, you know, there's a really strong prospect of AI replacing call centers and a lot of people work in call centers and those those those people will probably move into other things but what if there are millions of them you know this could be disruptive there's a report from the International Monetary Fund about uh the the the people whose jobs could get replaced. It's about 40% of tasks currently performed by people could be automated by AI, not not tomorrow, not next year, but in the next 10, 20 years. So we're really heading for a a huge change in the structure of employment in the world. And even economists like I you know I've discussed this with economists and it's not a subject that's in their textbooks because what we've seen over the last hundred years is we had automation but it was for you know manual labor. It was for work that didn't require that much reasoning and that was a particular sector of the economy and we were able to shift people into other um you know into education uh health and and and so on. Uh, now we're we're having this potential transformation that could be about like, like you said, the creative jobs, jobs that require reasoning and planning and those jobs are not just at the the lower end of the scale. It is it is uh essentially you know uh you know about half of the tasks we do in most of the jobs which pay people. So that's a huge shift and it could be extremely disruptive >> You've cited. Do you remember like the number economists have put on it? >> Yeah. But you know they're a lot of people who are economists who do do look at this >> and >> most economists are very like don't worry. The market will adjust. We'll find a way. But I've met some economists who said, nah, not this time. This time it's really different. This this could be very bad. Suppose we lose 30 or 40% of uh jobs. All these people cannot just overnight transition to something else because otherwise those jobs would already be filled. So there's going to be a a transition. How long is it going to take? What's going to happen? You know, if you were to stop working now and you were doing a creative job, What would you do? I don't know like people might feel their life as meaningless and could spiral down into bad stuff >> We've talked about that previously >> on this podcast where we spoke about the sort of the increase in suicide rates that come from an increase in unemployment in a sort of systemic mass-scale unemployment that's unprecedented. Do you agree with like 10-20 years. So it's hard to say. The job loss story could take a few decades. The more catastrophic risks, we don't know if they are 5 years, 10 years, 20 years, you know, I suspect what we will what we should be spending time on is what you know like from me and a lot of the scientists it's the the catastrophic risk. But I also think we should absolutely care a lot about the societal disruption that could happen in sooner in say 10 years or even five or you know um and if we make bad decisions we might have chaos in our streets or dictators taking control in a lot of countries. So even in the first scenario where we avoid the catastrophe, the the super intelligence and losing control you know we can we we can do badly. We can we can have our our societies really going in a bad direction. And there's another scenario you know because of the job loss where uh countries that have the best safety social safety nets well they'll be okay. but but if you know that could be countries in Europe and Canada and parts of Asia but the US is not in in there right. They don't they don't have universal health care, they don't have the social programs like other countries. So they may have a lot more issues. And then if you think about developing countries, They they they have nothing. So you could have a massive migration. I mean, we have a migrant issue right now in the US and in Europe. What would happen if there was nothing to do in most of the world and people just want to go to to the richest countries that don't want to let them in. This is socially unstable, right? Like it could things could really go bad for society. If we, on the other hand, made good decisions, we could actually think about a world where people are creative where they they can do things other than just you know trying to survive where they can feel fulfilled and you know a lot of jobs are not fulfilling I think most people work because they need to put bread on their table and they you know they they hate their their job so what if we had jobs that were that did not have to be in the market economy but were valued socially because you you help uh your neighbors or the community or the the the environment. You know that would be we have to invent a new economy, a new notion of what provides meaning and value. And we can do that. I mean, it requires a lot of conversations, a lot of societal thought. You know, we need more philosophers and people to think about what would be societies in which most people are happy and AI is doing all the work. And I think there will be work for people because if you look at the data, you can see that 40% of the tasks will be automated. So, 60% will still be there. Now that doesn't mean 60% of the jobs because all most of the jobs use a lot of these tasks. So we're probably going to have to transition from full employment to part-time employment, something like that. And there's already some governments who have experimented with that a little bit. So there are many questions. Do we have universal income, basic income, universal basic income so that people would have like enough to survive and then they would find I think most people would find something to do. It may not be you know may not be the sort of work we have now, but we can organize society differently. We have the means to do that right because we would have so much productivity from machines, but do we have the political will to do that? Yeah it does feel like that the the the will has always been the the rate limiting step in the world. I mean you see the same with climate change. We have a problem and we're talking about it's such an interesting analogy because the carbon budget with climate change like we know what it is. We know if we emit too much carbon we're going to be screwed if we emit less carbon we won't be screwed and we can't seem to not emiss carbon so the the solution is straightforward but the will and the incentive structure is is creating resistance and I I sort of see that as like a proxy for maybe what we can expect with AI. We you've described a bunch of technical solutions for AI that you know might work given the time and then on the other side of it you've described the societal changes the political treaties the laws the policy that would need to exist. But the but the will like you said isn't always there unless there's some kind of crisis or catastrophe. Yeah, I'm worried about that. And I think we can shift the will by making I think we can help by making the public understand these issues. Where do you see AI in like three years time? What capabilities would you expect to see in three years time? Okay, so there's a lot of uncertainty. Um my best guess is that there's a 50% chance that in the next three years um we'll still be on the track where bigger and bigger systems are yielding better and better performance. Uh there's a lot of uncertainty about whether scaling by making these larger systems would actually go on because there's rumors about, you know, we're reaching some limits to that approach. So if that happens, we're still going to have better systems, but there are going to be new ideas that will be needed. So it may take longer. But my my feeling is that in in at at most a decade we're going to have very very powerful AI that could be smarter than us in many ways and they'll be deployed very fast because if you have something that could replace uh people doing work, the incentive for the market is so strong that it's going to happen right away. And so I'm worried that we are not doing enough to handle both the societal disruption and the potential of really bad risks. Have you thought about what jobs are going to be around for >> a long time? >> So most of us are not I think a lot of people would like to know that you know, if you have children, you want them to have a job. You have to think you know if your children are you know in primary school and they're deciding what to study, it's hard to know. I I a lot of people who've studied in the in AI and in computer science you know say to me my children don't know if they still should go that way because they don't know if there's going to be a job you know by the time they enter the the market. And you know for each person I think the first thing is really be flexible. We might have to learn to to to change jobs, change uh professions and so on. That is for sure. So long-term planning is going to be a challenge. So what I would say is think about what makes you happy, what you're good at. Um follow it right now but know that things could change. Uh if you're in the kind of professions that require creativity, a connection with people that's going to be around longer than the call centers because the technology to replace call centers is like a lot easier right >> Right. >> And and it could happen sooner than replacing a good TV show for example >> Interestingly something you said earlier actually like gave me a little bit more pause than the existential risks because those are so large and they're so big they're less tangible than you saying earlier that the probability of um job loss for a lot of creative knowledge workers over the next 10 to 20 years. That feels like much more relevant and real to someone like me than the idea of losing control to AI. How should I be thinking about that? I don't know. I think you know you should continue what you're doing because at your level of creativity and originality there's the next 10 years no problem >> Yeah. >> And then so 10 years well a lot can happen so >> We have a closing tradition on the podcast. The last guest leaves a question for the next guest they never know who they're leaving it for. So the question that's been left for you is when you achieve a big career milestone, what thoughts or emotions come up for you? >> Uh well I feel happy. But you know personally these days I don't care so much about milestones. I care about having a long-term positive impact on the world. And so I may have short-term milestones but I'm I'm trying to stay focused on what is really a valuable legacy I can leave. Professor, I had no idea when you arrived today what you were going to say. You've just been tremendously insightful in so many different areas. And there was a depth to the conversation, but also there was a sort of, um, it was just really simple and it made sense and you're also balanced and I could feel at times your emotion on the topic. So this was a real privilege. Thank you. >> Thank you. Thank you Steven. I really appreciate you taking the time. Thanks.