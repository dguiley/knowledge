# NVIDIA Nemotron 3 Nano 30B First Impression

Published: 2025-12-15

Today we got a pretty exciting video. Uh over the weekend I have been testing the new Nvidia Neotron 3 Nano 30B A3B. So this is a new model from Nvidia. This is like a hybrid mixture of experts models. I have been playing around with the 30B model because I thought that was mostly uh available to you. This will be available today on hugging face and other platforms if you want to try it out. And this is like a 30 billion parameters models with 3 billion parameters active at a time. So it's super fast. You can see it has the four times throughput versus the this the previous version of Nimbleron. And the efficiency is better on the reasoning token. So 60% fewer. And the context window is huge. It's a 1 million context window on the 30B model. So that's going to be interesting to play around with. And of course this is open. It has the fully open weights. It has also shared a lot of its data sources if this is something you is interested in. So a super fun model. It's very fast. I've been using it through the API from Nvidia and I had a blast with it. So I wanted to share something I have been playing around with and you can see the speed of this because I've been testing it around in open code. Uh this is kind of like an open-source version of cloud code and we're gonna try to build a few stuff today to see what we actually can do uh in performance with this. Can we build some simple apps and stuff in open code? So yeah, let's just head over here play around a bit with this model and see what we can do. So I just wanted to start over here on cursor just to show you what kind of limits I have set this to now. So I set the context window to 1 million and the output to 40,000 tokens. Right? So, this is pretty interesting for like a small model like this. And I have been testing it out. I haven't done any like specific tries. Uh, but as you will see now, it's so fast in open code. And I was thinking, should we run open code in the terminal? Just for the video sake, I think we're going to run it in this terminal. Uh, even though I kind of prefer uh have it here. So, I thought we can start by trying to build a simple Let's see if we can follow the instructions. So uh I went to Nana Pro here on FAL AI. I grabbed kind of documentation for Python for this model and I just pasted it in a documentation file here uh nanobanana.md. We have 200 lines here with yeah uh Gemini model here on nano banana on fal. So if we do open code right I don't know if you played around with this super cool open source you can see we are on the Neotron 3 nano 30B 30B 3B active from Nvidia perfect so you can see down here we are in our apps nano3 directory so what I can do now is we can start by reading the documentation so nano banana so let's read that okay and now on the right side here you can see what we are doing you can see the context so 13,000 tokens is just 1% used. That's pretty pretty interesting. So, I wanted to start with something simple. I think we want to start to just figure out how we can write some Python code to just generate an image and maybe get the image back and save it. So, uh let's prompt something here to Nano uh Neotron 3 Nano here. So I told you we can do create a nano pi that takes a text prompt turn it into image we save in / images. So here we can do plan mode too. We can test that out. So we're going to do a plan mode here in open code and we can see the speed now when this starts going. That is incredible right? I don't know how fast that was but you can see. So let's switch back now because we already have the plan uh implement plan. Okay. And you can see again it's just going to go off thinking tokens just speeding away. we do the right tool. Uh so one thing I wanted to see is how is it to following tool calls and stuff and it's pretty good. It sometimes make this error but here you can see we did some writing here. Uh but sometimes it misses a bit but I would say the tool calling is very good. It always picks the correct tools. That's kind of my um test so far. So let's see what we got now. So you can see we got our nano pie here, right? And there are probably a few issues, but let's see how this runs now. It was so fast. We even kind of didn't register that it was finished. Uh, but I I I bet there are some issues here. Uh, nano.py. Uh, okay. So, we have some argument issues because we need to put it in this. Yeah, this is a bit of a nasty format. Let's try to change up that a bit. So, let's just do just a standard format. 1K 916 PNG user only enters prompt because I don't want to set up everything. So, let's see if we can do that edit here now. Okay, so we did some writing here. That's interesting. And the speed is just uh I can't really compare it to anything else. Uh but of course, I'm running this on the API now, but you can run this locally. It's a 30B model. So, that's pretty cool to have this locally with this 1 million context window. So, let's see. Now, we are still doing some changes. We are doing a lot of reasoning tokens, but it's pretty efficient since the speed is so high. Okay, so let's see where we are at now. So, let's do this. Okay, so we need to follow this up with a prompt. Uh, let's try that. So, let's just do Taylor Swift selfie. Okay, we need to set our API key. I kind of forgot that. So, I have my API key in the env here. Let's see if we can load that from there. So, uh, in just good practice, I like to clear this and start in a fresh context window. So let's just do in at nano oops at nano.py uh use uh env to load uh fal key from uh at uh let's do at env. Okay, let's try that. So we can't read this, but it should be able to do the load.env. Okay, that's good. [laughter] I like this speed though. Okay. So, see uh let's see now where we are. So, let's clean this up. Let's try it again. Uh okay. So, we have an issue with the foul client. Let's try to fix that. So, again, [snorts] I'm just going to start a new one and uh let's try to fix this in at nano uh fix error error and paste in this. Okay. So, let's give it a chance to fix this. Since it's so fast, it doesn't really matter too much. Okay, great. So, you can see image has been saved. So, if we go here, we should have that selfie image. Okay, perfect. So, that is working. So, now let's see if we can actually do anything with I was thinking stream lit. I haven't tried this. Uh but let's try it out. So, I'm just going to do a prompt that is going to be something like So, I'm just going to do at nano.py. It's working. Saves the images in image. Please create a streamlit UI. We can run local for this workflow. So let's see if we can do this. I think we need to maybe create a plan first. Uh and then we try to implement this. Okay. So this is going to be our plan. And I'm going to go to build execute plan. Okay. So you can see we did the right thing here. That's good. Okay. So let's test this out now. So I think we're going to do streamllet uh run app.pot. pi. Okay, so we are popping up. Okay, so enter a prompt. Let's try it again. Okay, so let's hit generate. Okay, so we have an issue here. Let's try to fix that. So I'm just going to grab all this and let's see here. Yeah, we have some issues and let's try to fix that. So let's do again in at app.py. We get error. Okay. And paste it in. Create a plan to fix. And let's do plan mode. Oops. I think the plan mode here is a good way to go. Okay. So, let's try execute plan. It's so fast. I can't even read what's happening here. Okay. So, let's try again. So, if we go here and let's run the app again. Uh let's just try on the same prompt. Okay, generating image. This looks promising. So we just have to await a bit I guess and hopefully the image will pop up here too. So yeah, that might be it. So if this works now, that means that we generated like All right, that was pretty fast. Yeah, we got it. Here's the prompt. Taylor Swift selfie. Okay. Yeah, that's nice. Okay. So, I want to do one more uh thing with the Neotron Nano3. So, let me think of something we can test here. So, the most important thing for me uh at least I think this is important is how good a model is to actually call the correct tooling, right? So, this model should be trained on tool calls. So, we want to do a small test to see how that works in open code. Let me just close those. So, you can see we have a bunch of different tools here. We have uh edit, write, read, grap, glob, list, patch, to-do, read, and write, to-do, write, to-do, read, and web fetch. So, I want to try to string a bunch of these tools together just to see if we can actually do like a string of tools and actually get the correct output. So, I'm going to head back to Open Code. Uh, did I start a new one? I think so. And I'm going to come up with a prompt here that tries to do multiple tool calls. uh that has to end in something, right? So, let me come up with something here. Okay, so let's try to be a bit ambitious here. So, we need some information about the Nvidia 5090 GTX card. Do a web search. That's one tool call. Find five bullet points. Uh save this info in 5090.txt. That is going to be a write and um a read, right? And or write and uh write at least. Uh after that we need to find the five last days of the closing bitcoin price. Write some python code with the open gecko API to fetch the price. Store this as btc.csv in CVD. Then also create a python code to run mattplot lib for a graph of the bitcoin price. Run this generated python code. So this is a lot of operations. So here we're going to try to see how uh it performs in doing all of these tool calls. So, it's so fast, right? You can't really keep up. So, what I'm wondering out if we are actually going to lose the we are going to lose the the context here over time that we kind of forgot the plan we were going to do. Uh, but we should have a million context window. So, my plan now is just to let this run and see how long this takes. And I'll take you back if we have something. And if it can do this, I think this is a very good proof of that uh the tool calling is working very well. And yeah, I'm just going to let this run and I'll take you back if we have something. Okay, so it says the task was completed successfully. The 5090 txt file contains five bullet points. The fetch BTC.py script was executed to fetch the Bitcoin data. Save it as BTC and generate a price graph. Okay. So, let's go see. Uh, we do have the Okay, we do have but that is all the same date though. Uh, but still we got something. Uh, we have the PNG for the graph. Okay, so that was just straight graph. Uh, let's say we need the five dates maybe, but this was a pretty good and we have the 5090 uh expected to launch with 25 GB. I guess it already had launched but we have some information here. So I would say this was pretty impressive. Let's see if we can actually adjust something on the fly here. And uh we needed the Bitcoin price for five different days. Maybe we can change that. Good job. Uh but we need the BTC price. And let's tag the BTC price for five uh last days a dates. Uh update and fix. Okay. So, let's see if we can do that, too. Because now we just got the same date. We want five different days. So, let's see if we can do that. Yeah, that looks better. Okay. So, if we blow this up a bit now. Yeah, this looks more correct. We are at 92. Yeah, I think this is correct. So, we got there in the end. Yeah, perfect. Uh, okay. It's still going. I don't know what's happening here. Uh, but um, what can I say? I think you should go to he hugging face and or other platforms and just try this out because I've been really enjoying this model to be honest and it's so much fun to play around with the 1 million context window. That is something I haven't really done on like a small model like this. And with the throughput I have had on this API, it's been super fun. And I've been trying to do these small things like how fast can I do this? So I went from like putting in some prompts and having the the small app or code ready in like 5 seconds. So that's pretty fun. So yeah, find the link. You can follow the link in description. I'm going to put a link up there where you can read some more information. You can maybe find the link to hugging face. So yeah, go check it out. Nvidia Neotron Nano. I've been playing around with the 30B model. So yeah, thank you for tuning in. Have a great day and I'll probably see you again