# 5 Gemini CLI Features Ship Apps Top 1 Percent

Published: 2025-09-25

There's a lot of talk on the street about the 90 different obscure features of tools like Claude Code, Gemini CLI, and Codeex, but in reality, there's only really four or five that will really move the needle if you are a vibe coder. So, instead of wasting time wondering what's really worth it, I'm just going to break it down for you. We're going to go through each of these five features in the context of a real project. We're going to talk about what it is, show a live demo, and talk about why you should consider actually implementing this in your own workflows. So, this video is going to be perfect for you if you are new to the world of terminalbased coding tools, or even if you're more intermediate or experienced, but you're looking for a few golden nuggets that you might be able to take away because terminalbased coding tools are very powerful, but it's important that you use them thoughtfully. So, that being said, let's get into it. So, first up, we have checkpointing. The thing with vibe coding is that its strengths also become its weaknesses. When you're moving very quickly and you're working on vibes, you end up inevitably introducing mistakes. Now, the thing is telling the system to just undo what you just did doesn't always work. It doesn't always go back and actually remove all of the files that it made or undo all of the changes that it did. So insteps the concept of checkpointing where we can basically stash different stages or states of our chat with the language model and revert back to them. So we're inside a project here and let's say we need to refactor one of these pages cuz one thing I notice is that anytime I try to load this dashboard page, there's like a notable drop off in the speed at which this page loads. So what we do is we kick off the Gemini CLI with this flag for checkpointing. So we can ask the system, look at that dashboard page. It's loading very slowly. I want you to give me performance improvement recommendations. And we'll let that go. All right, guys. So this thing finished and we did fix that page loading issue. But in typical Vibe coding fashion, we didn't decide to start a new chat and save these changes and commit them. And we just continue to go on and ask the next question. And lo and behold, we now have this broken page. So, let's say we don't want to now go down this rabbit hole of having this super broken piece. We can just go back and revert our changes. So, if I was to go through now and type /restore, we're going to see that we have a specific place that we can restore to. So, now we can see it has restored the project to right before we made that initial tool call. we can go back and everything is working as we need it to. So now this is pretty awesome because we can create these like stashed states of our app and then we can continue on to try to build stuff on top of it and we always have this point that we can very easily revert back to if something goes off the rails and we don't have git commits. But how do we maintain continuity across our coding session so that we can always come back and reference what we did in this specific chat later. So this gets us to concept number two which is chat saving. When you are vibe coding odds are you are switching context a lot. So we might be in this feature right now but then we're quickly jumping in and making other changes in other places. And what often happens is that we need to eventually make our way back to this feature and maybe dial other things in. Now, that's all fine and dandy, but what inevitably happens is that you encounter situations where you need to be able to go back and reference previously done chats. Maybe that's to see what solution it found to a problem that you're running into now. Or maybe you think that you might have actually broken something inadvertently and you want to go back and reference exactly what was done. Well, these command line tools like Gemini and Claude Code, they never used to actually store chat history in an easy to retrieve way until now. So, let's say in our previous checkpointing example, we actually did drastically improve our load speeds on this page, right? So, if I was to pop back to all prompts and then pop back to the dashboard, it loads more quickly. Now, the thing is, we may want to use this same pattern of how we solve that problem in a new problem down the road. And so it's one thing to commit the changes that we make to history and then go back later and look at a git diff and see what was made, but it's an entirely different thing to be able to actually read through the chain of thought and what was specifically done and why it was done to make that happen. So we can save our chat and easily go back to reference it later. And the way we do that is by typing in / chat save and then giving a tag that will help us understand what this was about. So this could be for example performance optimization dashboard. And now I have this checkpoint saved with this specific tag. So why does this matter? Now let's say I go out, it's 5 days later, I've been doing a bunch of stuff and I want to be able to go back and actually reference this for some reason again. Maybe I'm running into another issue and I want to use this solution. Maybe I've realized that something's not working as expected and I want to see what it did in this specific chat that might have caused that problem. Tons of reasons you may want to use this. So, what I can do now is I can start my Gemini instance back up. And when I type in / chat, I have this option now to list save or delete. We're going to see we have this stored conversation from previously. So now if we come back and we do slash chat resume and we give that tag ID, it is going to load us immediately into that chat and we can just continue where we were. Now, this is going to be helpful if we want to actually pick this chat up and continue with it. Like maybe we literally need to go do something. We want to make sure we stash these changes and we want to continue this chat, but we can't do it right now. Again, lot of different reasons you may want to use this. But we can now go through and see all the different changes that it decided to make. So, it may seem simple, but this is a huge quality of life and productivity improvement. So this chat saving really helps us transform from random experimentation to a systematic way to actually save and remember why we built certain things the way we did and what happened along the way again that we may need to go back and reference. So every solution that you discover here now becomes a reusable asset. So saving individual solutions is really cool, but how can we save entire conventions and ways of working? Well, that's where we get to tip number three, which is the combination of an existing feature plus a way of using it. And I call that the Gemini file enhancement. So, typically when you're working on a project, what you're going to do is you're going to come in and you are going to initialize it. So, for example, this is going to create a new Gemini.markdown file. It's going to analyze what our project is all about and then it's going to populate this file with the conventions and patterns and ways of working from our app. So if we were to go into this, we can see it's in this state currently pretty basic, right? It tells us how to start the project and then it gives us some basic conventions, right? Here's how authentication is handled. Here's how our database works. Here's how data gets fetched and so on. Now, this is where you would want to come in and really load up conventions that you want the system to really understand. Maybe that's how you name functions, how you want your file system to work. a lot of different ways that you could use this type of file. But what I notice a lot of people do is that they make this one time and they don't really go back ever to update it. And if you are great, then you use this as a living file, not a one-time creation. So for this example, I kicked off a new project. It's a super basic chat interface that allows you to upload an image of yourself and a target physique. And then it tells you a highlevel plan of what you need to do to achieve that. Right? So if I were to hit upload two images and hit analyze this image, it is going to use OpenAI and it's going to return me back a highle plan of what the gap is and what I need to do. And there we go. Right? So current physique is lean with ab definition. Muscle tone is present, but room for improvement, particularly in these areas. The goal physique is this way. And here are all of the things that need to happen. So if we were to pop back into our initiated Gemini file for this project, it's talking about what the front end looks like. Okay, it's using Nex.js with TypeScript, Tailwind, what the back end looks like. So it's Python using fast API, right? Just a few other basic conventions. Now let's say what we wanted to do next was we wanted to come through and we wanted to build an agent for this. So again, it's a personal trainer and let's say we want to now take this plan that we have and then use some intelligent agent functionality to build this into something real that the user can interact with over time. Right? So if it's me and I'm the user and I'm doing the exercises, I'm giving feedback to the system. It's able to understand how I'm progressing, what I'm experiencing, and tailor that plan to me along the way, knowing what the goal physique actually is. Well, let's go build that real quick and then show how we can update our Gemini file based on what we built. So, what I actually did for this is I had Claude go out and do deep research about Crew AI and the different elements of the framework and how things should generally be structured, how it should handle memory, how it should handle tool use, what does the hierarchy of the agents need to look like, all of that type of stuff. I had it pull that all together into a research report and then I gave it what my basic idea was for this agent and it popped out an entire plan of attack. So, I'm going to take this thing. I'm going to go build it and then we're going to look at what we built. All right, guys. So, we have this new little agent feature where we can upload different photos of a current physique and a goal physique and then tell it to analyze and it's going to go through and we're going to see in a second that it gets a response from OpenAI and this is basically saying here is what the gap is, here's the timeline to achieve it and so on. And then you can say generate transformation plan. And now what we're going to see is that we have an entire like agentic system that's popping off in the background where we have a head performance coach, we have a nutritionist, we have all a strength coach, we have all of these different tools that are kind of being executed in order to solve this problem of how do you make that that transformation happen, right? And so at the end we're getting like a weekby-eek breakdown of exactly what the exercises should be and and all of that stuff. So, it's not perfect, but I mean, we basically just one-shotted this this thing. So, not expected to be perfect. So, now if we were to go through and actually look at like, well, what needed to be true for this thing to actually work? We have this entire personal training crew implementation which has a head coach, it has a nutrition specialist, it has a strength coach, recovery specialist, all of these different specialists that all have access to different tools that they can use to do what they need to do. So, of a strength coach, they're going to have access to like volume calculators, strength calculators, how to actually periodize the training, right? A lot of domain specific stuff that doesn't matter for this video. But point being, now that we have like, you know, some pretty involved functionality around this like agentbased system, we want to update that Gemini file, right? We want to enhance it so that it understands how the app is currently working. And so what I've done is I've pasted in this new message saying, "Hey, I want you to go back and I want you to analyze my codebase and update that Gemini markdown file to reflect the new project conventions." And I'm kind of giving it some direction on how to do that. So we're going through the project context, coding conventions, development workflows, and then any project specific rules. Okay? And so this is going to go through and it is going to make those updates. And now if we were to go back and look at this, we can see we have an entire section about crew agent development. So understanding the general agent design principles, task management and how delegation works, the different tools it has access to and how its memory works. All of that stuff now is going to be given on a high level anytime we move through the system to build new things. So in this case, I have a pretty clear hierarchical agent where we have this head coach that is basically in charge of everything and orchestrating all of those other agents and it's going to use the agent that it needs in order to get the full context that it needs in order to solve the problem of the user's query. So, the reason it's important to now enhance this markdown file like we did is that if I go in and I want to start adding in new agents or modifying how certain agents function, I don't have to worry as much about the system entirely undoing the way that this is meant to function. So, this is a living documentation type of approach. We don't just initiate the file one time and then assume it's good because it knows, hey, we're using Nex.js JS when we introduce new technology, new patterns, new ways of working. We want to make sure we update that file. But that being said, there are still going to be specific moments where very specific facts about our project need to be remembered. And that's where we get to the next feature, which is the memory feature. So now we've just stored a bunch of important stuff about, you know, Crew AI and a few other things in this file, but these things are like really broad strokes. So for example, this section may give a highle understanding that we are in fact using crew AI to build these agents and there are specific ways that it needs to work, but it doesn't have any granular detail about our actual agent system. So what do we do with very specific and important facts about how our system actually works? Let's look at some examples. So inside of Gemini CLI we can use this memory command and then we can use the option to add and then we can give it basically what the fact is that we want the system to remember specific. So in this case, something that might be useful for us is that we have this personal trainer crew which uses crew AI with a hierarchical process where the head coach is GPT4 and it orchestrates three different specialists all using GPT 3.5 Turbo. That would be relevant for this system to know basically at all times. Now what else might it be important for this thing to remember? It also executes six sequential tasks with context dependencies. It has to analyze the athletes profile. It has to design a macro cycle for the training. It needs to make sure the nutrition is paired with the actual fitness. It needs to generate weekly programs from that. It needs to create the recovery protocols from that. And that all needs to happen in a specific order. So that is important for the system again to remember. So now if we pop back over into that Gemini markdown file, we're going to see that all of these memories have been added with the specifics of what they mean. So again, the first two we did together and then there were a few other ones that are relevant. We have two different operation modes. One where we're generating an actual training program from scratch and one where we're basically just having a conversation back and forth about a specific topic. And again, this is important because later on when I go to change something and make an addition, for example, to how the training program generation works, I don't want it to go accidentally update a different operation mode like the quick consultation mode and then me be sitting here saying, "Hey, why does this thing not actually work? It didn't do what I asked." When in reality, it just built it in the wrong place. So this strategic memory management where we're saving important facts about how our system works is what enables you to build complex interconnected things and not lose track of what is what. So now we have both things right. We have the systematic overarching patterns as well as precise facts about how things work within those patterns. So the last thing is that as our project grows, we need to have a system to properly manage the context that the system is dealing with at any given point in time. So one mistake that a lot of people make is that they clear out the context entirely when they're still working on something within the same chain of thought or the same fundamental thread. So there's a misconception that like a lot of tokens, this thing uses a lot of tokens, like that's a bad thing. And it's simply not true that using a lot of tokens is always a waste. So it's more about using exactly the number of tokens that you need and nothing more. But we still do want to retain context so that the job gets done properly. So instead of always clearing context, it's often a lot more valuable to compress the context, but tell the system exactly how you want it to compress. Let's take a look at that. So let's say we wanted to add a new API endpoint and what it does is it integrates with our crew AI agents and it basically has this function that it's a blood work analyst. So a user can upload a PDF of their like lab work and it integrates those findings into a specific nutrition or fitness plan. So we can kick this off to go build out that API and system for processing and then we'll come back in a second once it is done. All right, so it just went through and we built out all of this stuff. up. So, it went through, it read all the files. It updated files, edited files, replaced files, wrote to new stuff, right? It went through and did everything it needed to do to get this PDF parsing working. Now, the thing that we're missing at this point is we don't actually have a front end to be able to upload it. And so, we have this like button that was originally made for images. I'm going to just simply update this so that it can accept PDFs or images. So now if I were to pop back into cursor, I obviously don't want to clear out the context of all of this stuff, right? Because I I need to actually have an understanding of the API endpoint, the functions, the arguments it accepts, what it responds with, all of that stuff. I need to know that. Now, in this case, we're using Gemini, which has a pretty goodiz context window. So there is still a ton of context, and we could just go in and probably start chatting right here. But if we were doing something that required a lot more changes and was a lot more token intensive, we might be at that point where, hey, we have 13% context left. Do we really want to go try to build this thing right now or should we try to free up the context? So, what we can do is we can come through now here and we can type /compress, hit enter, and then this is going to go through. It's going to compress our entire chat history down to just a high-level summary of what it did, which usually includes the files it created and what those files are used for. And what we can do is we can use that information to go build out the next feature, which in this case is the front-end UI that this feature uses. All right, so now that this thing is compressed down, we obviously saved quite a bit on the actual token cost. Now the thing is if you're in a tool like cloud code for example, you can actually expand this out and see exactly what the summary is that the system is saving. I'm sure that feature will eventually come to Gemini, but I haven't found it yet. So now we could come here and we could finish this feature having the full context of everything that was there. So, the reason that we do something like that where we're compressing it is that if you were to think about this, if we were to just go ask it now to build this new front-end feature and generically say it hooks into the backend that we've already created, it's going to have to go through and it's going to have to read all of these files again to understand what they are, how they function, how they connect together, and then how that's going to connect to the front end. So it's a lot simpler to just compress down the context and then use that knowledge of what we already built when we go to build the next thing. So again, this is particularly helpful when you just had to go down like this slight rabbit hole maybe of making rather big changes and you're still on the thread of trying to solve that fundamental problem or build that fundamental thing. And so there becomes this question, do I need to abandon this context altogether and go do something new or should I just summarize that context and then continue the conversation? So this isn't really just about token efficiency. It's about knowing when you need the context of what you just built in order to get the current job done with quality and a lot more quickly the first time. So there you have it. Five features of Gemini CLI that actually matter to vibe coders. If you think there are any that are more important than these top five that maybe I left out, let me know because in my opinion, these are the top five. So, if you like this video or you want more in-depth tutorials on how to actually build things end to end with these types of tools, make sure you've subscribed to the channel and that you are in the free school group because we're going to start releasing a lot more long form two, three, four hour full builds of things there. So, that's it for today. I will see you in the next