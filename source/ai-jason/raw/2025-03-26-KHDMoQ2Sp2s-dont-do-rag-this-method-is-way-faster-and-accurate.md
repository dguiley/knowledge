# Dont do RAG This method is way faster and accurate

Published: 2025-03-26

this video is sponsored by headon one of the best open source platform for logging monitoring and debugging your large L model applications so large L model at default don't have real time data about what's going on in the world also don't have access to a company private data but for majority of use case you kind of want to bring those external resource to L model and let it work Beyond just training data and traditionally the most popular method has been rack which represent for retrieval augmented generation that basically means you create a big knowledge base that contain every sing you want the L language model to know and when there's a user question or user query we're do a search in your database to retrieve the most relevant information which is a portion of the whole database and then send both those retrieve data as well as user question into the context window of lar L model but recently this is one term that became increasingly popular called cak and cak represent for cage augmented generation this represent for an alternative approach instead of doing a search and put only portion of the rant data as contacts the key difference of C is that it pre-load the whole database into model's contact window all of it this method has totally changed how do I build agents and large Lang model applications that's why today I want to take you through what exactly does this method mean when to use rack and c and most importantly let's try to build an mCP that using this cak method to pass any external docs and retrieve the most random code example from the whole DOC website without further Ado let's firstly dive a bit deeper into C this approach of preload all the data into contact window do really makes sense 24 months ago cuz back then state of art large L model contact window is only 4,000 token so you can't really feed anything more than a small PDF but this number has dramatically increased for the past 24 months for now most of the flagship model support around 100 to 200,000 tokens contact window and Google's Gemini model it support even up to 2 million tokens that is 488 times more than what we have 24 months ago it's roughly translated to around 1.5 million words and put this into context for a normal novel it has about 990,000 words and even Book Like War in peace they only have 587,589 which one so for all the rack pipeline it start with a data preparation process like this to turn your data into a vector database that contains small chunks of data and then when there's a new user question we'll also turn users questions into embedding so that we can use this embedding to search inside the vector database and return list of chunks that is most relevant to what the user is asking and this is typically what we call topk results and we understand both those chunks as well as user question to the large langage model prompt and get it to generate accurate answer but there are few challenges about building such rack system on one hand it is a bit complex to setup it does require some setup to turn your knowledge into a vector database in this pipeline it also introduced quite a bit of latencies for example there will be time span on actually turning user crashing into embedding and retrieve the most revant chunks from the vector database if your use case is user can upload a few PDF and start chatting with PDF it will need to wait for turning all the PDF docs into a vector database but most challenging problem is that retrieval accuracy how can you making sure the chunks you return here contain all the information the large L model will need to answer the user's question for example for the first step of splitting your large dock into small chunks if you just do a very basic and simple implementation of each chunk is like 500 characters you might have situation like this API DOC website but the actual complete code example will be breaking down into different chunks so when large L model receive just one part of chunks it won't have the full picture of what the code example actually is on the other side if your knowledge base contain financial report across different years and user just ask a question what's the latest Revenue it might return information of revenue from past years as well which can also confuse the larange model and sometimes users question can also be quite complex if you just simply turn the question into embedding it might not return all the information that needed so there's a whole bunch of techniques that people are using to resolve those problems like metadata filtering query transformation to break down the question into multiple different small queries reranking and bunch of other things but K on the other side because we're fitting the whole documents into the larg Lang model we don't need to worry about whether we retrieve the right information to feed the large Range model because it must be somewhere and because we don't need to do all those vectorizing retrieval implementation the code implementation is going to be extremely simple here's a quick example of how can we build Chad with PDF use case with a Gemini model all you need just this 10 lines of code passing on the doc URL as well as a user message and will just work out of box but you might have concern about if we feed a big document to the large L model will you be able to retrieve information accurately as we want and how about the cost and speed since every time we're feing such a big prompt to the model well those three aspects are exactly three parts that has changed dramatically for the past few months especially with Google's Gemini model so for the first one the retrieval accuracy typically we're test about something called the needle in the haast stack which basically means you are feeding a huge amount of tax to large L model and ask to retrieve certain information that exists in specific parts of this tax and see if it retrieves things properly on This research from Google even with Gemini 1.5 Pro it already demonstrate near perfect recall of specific information in vast context of up to 1 million token and with the recently release of Gemini 2.0 according to this test from vecta they comparing hallucination rate of top large L models Google Gemini 2.0 is achieving an extraordinary result and we can expect this number to keep getting better at with the new models came out on the out side how about the cost and speed these two things also changed dramatically thanks to Gemini so even though some model like GB 40 the cost can be a bit higher with $2.5 per million input token but with model like Gemini 2.0 Flash the input price for milon token is only 10 cents which is almost 96% cheaper and with this price point you can really feed a whole bunch of data to the model context window without worrying about the build you will get and meanwhile the speed also improved a lot in some of test I did and log on headon I can feed the whole developer docks from fire CW to gini 2.0 and for that amount of contact window the cost is only 0.006 and get results within 3.4 seconds which is very impressive and if you haven't heard about headon before headon is one of the best open source platform for logging monitoring and debugging lar L mode application in Productions it give us ability to see exactly how people are interacting with our lar Range model applications track cost arrow and latencies so we can optimize for performance and I can also do a bunch of very Advanced interesting things like Auto catching the response if the promp is same to save cost and improve speed set up customer properties so I can segment different type of request and the best part is that it is extremely easy to set up all you need to do just adding this part of HTTP options to your existing gem call then all the requests will automatically be logged on the platform I can see exactly what type of request and prompt user are actually giving our applications track which part is creating most of the latency so if you're building and launching your large L model applications I highly recommend you set a headon to capture all the user request so that you can optimize for the cost speed and Improvement and now let's get back to building your C pipeline so in general my current go-to approach is I would try to avoid rack and just feed all context to the large Lage model and see what kind of results do I get and based on that I can start doing some optimizations bra as a message do make sense and we can often use that to handle really big data set because even though the models context window keep getting bigger and bigger so as company data is also ever growing so if your use case require dealing with database that is really diverse really really big then rag is still pretty good approach even though there also approach you can take with KAG as well assuming your database very very big that can be feed into just one large L model context window what you can do is that you can just use traditional search to search for which data are most likely contain the information that is related to the query based on either metadata file name and other stuff and only feed those future data to large langage model or you can even do something more fancier for those data you can do parallel large Range model call and in the end have another larg Range model call to summarize the result to get the best information out of all those data points so what I want to do is I will take you through a quick example of how do we apply those C mes to some of the really big database use case and the example I want to show you is how can you build mCP to enable cursor or Wing surf to deal with external docks much better where this mCP can receive the doc URL and return only the most relevant code example based on the stock so first I will install all the packages that we need import all of them and also set up environment keys so we'll be using gini 2.0 flash model so I get API key from Google AI studio and we'll also use filec as a service to script in the API docks as well as headon to log and track the latency and cost of every single large range motoc call before we dive into that I want to give you a quick example of how can you build this basic chadow is PDF use case with Gemini 2.0 as I mentioned before it is extremely extremely simple all we need to do just created a client of Gemini model and here I swapped HTTP options to headon URL so that all those lar Range model call will be automatically logged there and then I would download This research paper here feed both this PDF file as well as a prompt here to the model and you can see here it returned the results which looks correct based on what we see on the paper here and if we go to headon you can see that this request that we just sent cost only 0.00001 and we got a result within three seconds which quite impressive next we're going to start building this external doc mCP pipeline the first thing I would do is I want to give a quick test so I will call Gemini directly with this prompt help me gener request to script this website using file if I run this one you can see the API and point a return here is incorrect and that's because Gemini didn't really have the context and knowledge about the latest documentations of f so this a great example we can use to test I'll quickly build a proof concept here firstly I will use file CS map URL and point which will return all the sub Pages under a certain domain and if I check the lens it return 153 pag in total so of course we can try to script all the pages here and send to large Lang model as context but it will create quite a bit of latency not just from the model generation but also scraping takes quite a long time so what I plan to do is that I will use Gemini to filter out only URL that is relevant to the API reference so I only need to script relevant page and from this point you can see a future down to only 27 pag that is relevant still substantial amount of context but much smaller than 153 then I can pass on all this 27 links to the filec batch script URL endpoint and download the markdown of every single page so this process does take a while but what we will do eventually is save the stocks locally so next time we don't do script again it will be much faster cool so you can see that we get all the markdown content for all the pages which is quite a big context I'm going to feed all the markdowns above along with the same that we sent to Gim before and let it generate result so with this one you can see get result poply it pick up the right end point as well as a request body format and if I go to headon you can see for this request where we download the whole API DOC website across 27 page it cost around 0.006 and get a result within 2 seconds which is really really fast so this is how simple and easy it is now to give large L models knowledge and Gemini also have this contestation feature which means for those those large contexts we can actually create a cage then the next time when user try to create the same doc we can just pass that cage to the model class so it will be even faster in terms of generation but unfortunately at moment this contestation functionality does support Gemini 2.0 model so I will leave this part later when it is available and I also turn this pipeline into an mCP server where it has this function called retrieve API doc users can just give the request and share the doc URL and this mCP will handle all the work in terms of retrieving the most rant code example from the doc itself so my cursor and Wing server can work with external docs much better and there are also optimizations I did where it will automatically save all the script markdown file locally so we don't need to script again again if you're interested in using this mCP I have put mCP server repo in the AI Builder Cloud community and building with a step-by-step process of how to set up as well as the notebook example that I took you through in the video today so if you're interested you can click on the link below to join the community or continuously posting interest learnings and tips about building production ready large L model application as well as advanced AI coding tips but more importantly you have this community of top AI Builders who are launching their own AI set so you can come and ask any question you have both me and other Community member can just come and share any insights we have I hope you enjoy this video thank you and I see you next time