# DeepSeek Inside Story What Media Missed

Published: 2025-01-31

too many exciting things happened over the past weekend I thought we had to make a podcast episode to cover this deep seek has been at the top of everyone's mind even Donald Trump's mind right which is very surprising I think for the first time after Huawei has a Chinese company made the president's speech so quickly so promptly when something happens so I think as a Chinese person I feel quite excited about all the development here so yeah to get inul why are you excited about deep seek I think they made some very profound technical discoveries that is giving us a lot of Hope when it comes to the future of AI so for a while now we know that we've hit a wall with the internet where we've consumed the entirety of it and there's no more public data so other than finding digitizing new data or getting access to private data there is a much chance of improving things in that Direction but with deep seek we see that there is a really promising way of continuing to scale up through compute as opposed to data we we already knew this because 01 has started evangelizing the idea of reinforcement learning and Chain of Thought which are very computer intensive but not as data intensive however with deep seek we see that it's actually feasible with less than you know tens of billions of dollars so it it's it's a it's a deeply optimistic result for technology in general yeah reminds me of uh Sam's comment about there isn't a war maybe he wasn't talking about 01 maybe he already got a preview of deep seek R1 to go oh yeah it's cheap and good at the same time yeah to to me the most exciting thing is the capability the price to Performance ratio of these new models are just crazy because if it wasn't open source I'd say this is Chinese companies trying to compete trying to underprice their opponents by taking a loss to take a market because they have the Arsenal to be able to survive the loss in the short term but looking at the model and I'm sure I want to ask you is how they able to get this so cheap later but the fact that they able to is quite crazy it enables so many more applications which wasn't feasible because even let's say has half of 0's capability 01 cost $15 per million input token and $60 per million output R1 is like 30 cents oh was it 50 cents per million input token and then $3 per million output the price is just insane I think being able to have such level of intelligence we shouldn't really care about okay did it really beat 01 I think it do didn't have to it got close that's all that matters the fact that it got that close with a 20th of the price I think that's what we should be focusing our attentional and that's really what got me really excited and price calculation are you referring to the biggest 600 billion parameter model or is this calculation based on one of the distilled models because they distilled models into Lama and Quin oh yeah this is the six the original model the 600 bilon parameter model the price that they are currently charge and from what I've learned from most um Chinese media interviews and looking at their at least all of their public statements they always say they make a profit on their current token pricing whether it was for V3 V2 or R1 and because they're also a Quantum trading firm uh I think they probably they're probably not lying there because there's no incentive for them to really in most places impressive then yeah and I think because deep seek V3 should be half the price or roughly half the price of R1 I can only expect the distilled models I haven't checked the pricing for any of the distilled models to be a lot cheaper there what else are you also excited for I'm excited because when 0an came out there was so little information that we knew about how it works internally so we already had the glimmer of hope that there's other ways to scale but we had absolutely no visibility into it and even though deep seek hasn't open sourced the training data set they still open source the model and also they published the pretty detailed technical report so I feel like it's just intellectual candy that they threw our way and it's really really exciting candy I really like that it's really exciting to read the paper and it it just feels like a good book you know a book that Captain you that's what it feel like in research terms it finally feels like there's more players in this field because in the past year it just felt it was mostly okay there's anthropic there was openi and then there was llama and llama only became more relevant middle of the year and then suddenly someone else actually came through came much closer to the frontier models and especially at the pricing point and more importantly coming from China I think this is probably what most people did not expect that makes me quite excited at the end of the day I think democratizing AI you really have to have all nations trying to compete at the same time so this is not about hey us China which one wins I think that to me matters a lot less but just the fact that oh people and teams from different countries of origin going different path can get close to similar results and then that makes me even more excited about well AGI at the end of day when Sam was saying yeah things are accelerating it's going to be here soon I feel like okay maybe he's just chanting for the shareholders to invest more but now seeing deep seek it's like okay maybe we are accelerating things are moving a lot faster than what most people are expecting so yeah that part I'm super excited for it reminds me of the Space Race I'm sure that Russia put a lot of pressure on the US and overall both countries had to win and yeah competition yeah in a lot of ways this war maybe it is quite similar to the Cold War right trying to get the first person onto the moon and then see which nation will get to AGI first it'll be exciting I the race has just got more interesting it's harder though because you could fake the moon landing maybe but um when you open source models it's harder to fake things that that that's true I mean makes us even more excited right and makes it more interesting for people to replicate I already know a lot of teams and just well I mean even whether it's open source teams or I'm sure big tech companies are doing very similar things trying to replicate what deep seek is already doing in R1 and in their own whether internal or open source models I think we should see some more quite exciting things coming up and also anthropic as well and I wouldn't be surprised if they release a reasoning model within the next 60 days if not 30 yeah I bet meta is scrambling internally um but interestingly I also saw hugging face made like a public call to contributors because they want to reproduce the entire pipeline including the training data set and I thought that was interesting didn't they pop I think I saw a tweet maybe this morning where they met mentioned they produced at least some of the Benchmark results or at least got very close I think it was the math benchmarks and some other ones with the existing data so the fact that with this is a week right [ __ ] it's just been a week oh man it's nine days more precisely it's just been a week and people been reproducing the results and getting some similar Benchmark numbers I think this is just crazy also the beauty of Open Source right which reminds me why did we only started seeing things on Twitter this weekend because the model was published on January 20th if I'm not wrong and my Twitter started blowing up only on the weekend so what what's up with that Gap my take is they at the time when the model was released it only existed in web chat but over the weekend they released the oh no it was just before the weekend they released their iOS app and also included web search alongside of that reasoning model which is something that no one else has done even 01 I never got why chat OPI never allowed 01 to do the web search because it feels like they have access to the same embeddings maybe it's requires further fine-tuning or maybe it's too resource intensive don't know why but having a reasoning model also tied together to web search that feels powerful and also in general when you see that thinking tag going on for a long time and you feel like you've asked the right question I always feel proud when I see 01 01 thought for 51 seconds before giving you the answer said yeah I asked a hard question similarly I think for deep seek when you have access to real- time information plus that reasoning capability I can see why that gets people excited even for consumers how often do you use 01 as opposed to 40 or 401 meanie whatever your default is I never exceeded the rate limit for 01 and that is 50 queries per week and then for the $20 tier I only ask strategic questions at 01 because most times you don't want to wait that long for that thinking time to get things correct you're just doing searches maybe you're just asking a coding question and for is or Claude is already more than capable for doing that but if I have something that requires okay I'm thinking about this business strategy here's the outline help me think of the points I've missed or help me further strategize the content this is the type of more thinking or logical related questions I like to ask 01 but otherwise it just doesn't feel that effective um but deep seek their thinking model I try to use that more because you can see that thinking trace and then reading the full thinking traits it's also a great way to learn the logic of okay maybe this is a way I should also think Ah that's a point I've never missed and this is a a way of thinking about the problem that I never thought of so that also find quite intriguing to just improve my own thinking capabilities yeah what about you I rarely use 01 and and that's because at this point in time Chad GPT for me has almost entirely replaced Google and most of my queries are super fast and super dumb so I I do not need a lot of intelligence I just need a little bit of access to the internet and uh um a little bit more intelligence than than Google so because of that I don't use 01 very often I think that brings on an interesting point right for most consumers and most Everyday Use case speed and experience matters a lot more than that 100% correctness if you're already the expert in the area right yeah yeah I think that's quite exciting I've been reading a lot on Twitter about just how people are sharing the history of the team but I feel there's still a lot of information that people never really talked about um people generally think that oh deep seek just became a thing overnight and oh why was it this team that was doing it but in reality I don't feel like this is a coincident having done a bit of research back into a team is the CEO and founder so he's been in the ml trading space since 2008 ever since he graduated he's been trying to use ml in trading then he had to well didn't do so well before 2015 so that was seven years of a lot of trial and error and then in 2015 he created his own hedge fund um called High Flyers in English I think has a much better Chinese name called Juan but I don't have a good translation for it maybe I'll post one in the comments below later but it's um they were doing really well and became one of the big four quantitive trading firms very quickly in China and at their Peak they managed over 10 billion in US dollars in terms of assets back in this was 2021 I believe so they had already a lot of top talents in terms of Quant Traders as well as a lot of ml Talent they were accumulating gpus as early as 2019 team so some of this was even before the sanctions actually came through and then people were like oh yeah how did they get access to all of this this stuff they had a lot of this for what they were originally going to build for trading and I thought oh man this was their pivot designed by God for them to be having to be able to privately fund all of the data centers to be able to to have the talent to go straight into this without any external source of funding I think that's also what makes deep seek quite exciting because without external interference and without external constraint from investors and with a capability and a lot of now I think Chinese maybe the whole confidence of whole of China behind deep six team so I'm sure they can do quite a lot and that's quite different to a lot of pure private companies that's gone through fundraises but yeah and looking at the CEO he's been they got into the llm space back in 20 mid 2023 and then quickly released deep seek V2 in a few month time and then 2.53 followed at the tail end of last year which all did really well but I think one thing was consistent was the efficiency the efficiency has just been ultra high throughout the history of their model development they never released the expenses model right and um unlike a few other um Chinese companies also they were the reason they were also able to do this out of many companies in China they are only the one out of the big four which is the big four tech companies in China for people who don't know that's bans there's Alibaba there's tensent there's also Buu which is not as well heard of recently they were the the Chinese Google for quite a while and they were quite big was is the Tik Tok owner is that right sorry who of these four companies owns TiK ToK by Dance by Dan right yeah so those were the four only four companies that had more than 10,000 gpus in China and then deep seek was the only other and so they were already really well positioned to do this um yeah so with all the infrastructure talent and everything set up it seemed like the perfect pivot so it looking back at history none of this feels like a coincidence it's just it was per the stars were lining out for them to do this thing and they had the right perseverance Talent money resources and infrastructure to go straight into this so yeah and I thought that was actually like quite exciting is I think people shouldn't just think oh yeah deep SE got lucky and they did this thing no the guy has been in this space for almost 20 years now so he knows his [ __ ] but talking about efficiency I know Julia you read through the 50 was it 50 pager V3 and R1 paper back to back right there's deep tell me more like how are they able to do this thing at such high efficiency I'm still mind-boggled by the 30 cents and 50 cents cost tell tell us more well obviously there's many many many layers of depth um I would say that the biggest Innovations come from two directions one of them is this idea of scaling via compute and the other one are just very smart um implementation efficiencies so when we talk about scaling via comput um that's as opposed to data you know because we've been training on the entirety of the internet we hit this wall where there's just no more data to consume um so scaling via compute is just another dimension of growth where we just give the model more floating Point operations as a way of going bigger um and the second Innovation is a bunch of very very efficient implementation details um for instance one of them is how they implemented reinforcement learning they managed to reduce the number of parameters required by half so reinforcement learning um is pretty inefficient when it comes to compute during training and it actually requires training not just your main model your main llm but training other models as well one of them is called the value model not super important but but the point is they managed to approximate that model so all of a sudden they had half the parameters that a model usually has which is huge because if you're training a model that has 600 billion parameters you would rather not have to train double that real you had to use so much compute even in the values model that's super interesting yeah and then um there's other things like deep seek R1 is actually a mixture of experts model and that's why it it's so huge and it has 600 billion parameters I I got to ask a really dumb question I've heard mixure of model mixer of expert so much and I see that mentioned so many times in the paper what is it yeah it's a particular type of architecture that is quote unquote sparse what that means is throughout the model there's multiple paths and each input or user instruction will fire a specific path so when you do inference you don't actually use all of the 600 billion parameters you will use some path through the model and you can think of each of these these paths as an expert that's why it's a mixture of experts ah right so can I think of this so the most models that we see now is a extremely powerful generalist model which requires most of the network to function to be able to give you the full results actually the the entirety of the network so if you think of llama Or quen all of these are dense models that means every single weight is going to be used when um doing inference but when it comes to deep C R1 certain paths are just absolutely not going to be used so you're not you're not using 600 billion parameters during inference but they exist and you need to store them and you need to train them oh right but does that mean it also makes inference a lot more efficient as well because it only needs to activate a part of the network yeah it's it's more efficient than having to activate the entire network but billion parameters is still quite quite a lot so that's why they they take this mixture of experts model and they distill it into the smaller ones from ranging from 1 to 70 billion or whatever the range is for llama so that that's another efficiency gain there that with such a powerful model they managed to train students that are meaningfully like by 100x by 10x smaller yet performed comparably to the bigger model when you say um you know training the student you mentioned a distillation can you just tell us give the audience a bit more context on what distillation is yeah it it it's super simple you take a super big model that performs very well you generate training data with that model and then you train your smaller model on that data so there is some sort of information transfer that happens from the big model to the small model and the metaphor here is that the bigger model is the teacher because it kind of transfers knowledge to the student okay and the intuitive question I always have then here is that what given the same training data set why can't you just train the smaller model on the exact same data set and I've heard it performs worse in most cases why is that so I think your question is why do I have to produce the training data with the big model why can I just use data to train yeah to train the small model directly yeah the way I like to explain this is to think about the very first application of distillation which was introduced in the contest in the context of mnist have you have you heard of that data set uh no I haven't it's basically um images of digits uh handwritten digits and their corresponding label so you would get digits from 0o to nine and the task is to just recognize those digits um and and for the first time for that data set they showed that distillation distilling the student is actually better than just training the student on Raw data and the reason is the teacher actually produces knowledge or or uncovers knowledge that is not explicitly in the data set for instance a one and a seven kind of look like together sorry kind of kind of looks similar right but when you look at the data set there's nothing nothing explicitly telling you hey in general ones and sevens are easy to um get wrong but when the teacher tells the student or or when the teacher produces the training data for the student instead of saying hey this is a seven it says hey there there's a 90 Pro 90% probability this is a seven and there's a 30% probability this is the one whereas the real data set would just say hey this is seven so because of that smoothness of the probability distribution now the student lets gets to learn from the teacher some information that the teacher has uncovered about the world which is that ones and sevens are kind of similar and because it it gets that information very explicitly shoved into it it doesn't have to do the work to discover that so it's it's really like a good teacher that kind of spell spells things out for you now when it comes to distillation for llms the exact same principle maybe doesn't apply because we literally just show the text to the student model we don't necessarily show the distribution but the argument there is that it's not visible to us humans how the text generated by a model is just easier to kind of quote unquote model or represent in mathematical functions than text that we would produce so it's subtly kind of easier to learn right right and in a lot of ways it feels like we can let the teacher figure out what is the best way to teach the students rather than hey here's a wot go figure out yourself by falling on your face exactly yeah very good metaphor yeah okay very exciting so mixture of experts the distillation what else um a lot of hardware optimizations and these were present actually in deep seek V3 they don't really mention I didn't realize yeah there's attention for instance uh which is part of the Transformer architecture it's a building block in the Transformer architecture is quite expensive and um yeah they just come up with very creative ways of caching things and and um multiplying things in a much more efficient way that takes advantage of the properties of the hardware um and then one final one is they figur it out how to do training and floating point8 as opposed to 16 which is what models use in the US oh that's another 2x efficiency gain right just for free across the board even if we ignore Moe and the rest of the stuff exactly all all of these are kind of orthogonal and they just like pile on top of each other okay right so in based on what you said we are likely already seeing a five to 10x model efficiency increase even given it's a 600 billion parameter model right given what you just said using floating point ande or I mean distillation get gets you even a step further on top of that on cheaper models I know Julie has already made a great video and which I've Linked In the description and also up there go check it out if you want to go dive deeper into this now that we understand why deep seek is so cheap and so efficient and how they've actually trained thing I think would be interesting to discuss now being a Chinese founder who's currently based in Europe I know you're a European founder currently based in the US so we've got the full spectrum of views how do you think this makes the competition and the relationship between the two countries more interesting I think competition is good and one of the things I love about being a European in the US is just seeing capitalism in full bloom and I don't think that capitalism can exist without competition so I think this is very welcome of course you want to win as much as possible but if I have to lose once in a while for the sake of capitalism then I'll do so it's a capital it's the world's capitalism in this case right not just the UK's yeah absolutely yeah it's worldwide capitalism yeah for me this is quite exciting where I don't think anyone expected China to be able to produce this level of model especially given the sanctions and when I read into the further I look into this the further it makes me think that the sanctions may have forced China to be this efficient in a lot of way I mean not that China didn't want to I'm sure if we look at the way China has always operated whether it's becoming the world's Factory or leading in a lot of renewable energy Tech say solar panels EVs and others and even highspeed rail they were really great at scaling things up really quickly and efficiently so in in a lot of ways I feel like this is at the heart of how China excels in the um in the current world so so this would have been path I think people would have chosen anyway but given the sanctions this may have been the only path that was possible for people to go down so I I see also a lot of people I remember we were discussing you mentioned to me up blam me Biden for applying the ship sanctions because they forced China to do a better model well I think that's maybe on the extreme side I'm sure China at least having confidence in China as I do would have come up with a good model maybe in a very different way without doing is but very interesting to see the competition and like how do you feel like being in the US what is the sentiment like how do feel people feel about oh it's a Chinese company that's doing this I don't maybe if it's other European companies I don't feel like it would have made a as big St or even if it was you know the Llama 3.2 or llama 4 came out yeah well I think we all live lives on two separate levels one of them is the day-to-day and the other one is more on an ideals kind of philosophical level so if I think about my day-to-day I'm actually I only stand to gain from try to making progress as I said before I'm getting the intellectual candy about reading super interesting research I become hopeful that my profession being a technologist still has a future on a more philosophical kind of ethical level there's definitely a little bit of a fearful Vibe so for instance I was reading the um uh what are they called the the terms of service or is there a better word for it where basically they described what they do with your data and um I saw that they track your um keystrokes let me find the exact terminology because I don't want to be making stuff up one second they say we collect keystroke patterns or rhythms I actually tweeted about that because neither open AI nor anthropic or collecting that kind of information and it sounds to me like that would really allow you to identify people to the you know their individual level that felt very unnecessary but I also wonder how much of this is legally may just they just got a really good lawyer who's like covering all of their bases you never know but that gives me a little bit of pause so if I say something against hu jingping and then I go on and then I I I just happen to swipe on a Chinese phone are they gonna know it's me I I don't know um that's that's not a problem I face on a day-to-day basis so I don't know if I care about it too much oh what I find interesting there is that obviously these terms of service is public for anyone to read but none of this stopped consumers from downloading and using the app at the at the end and it reminds me of the case of how Tik Tok got popular six seven years ago and how teu the ultra Cheap shipping and like Chinese produce uh company that's doing really well and also it's still one of the top top six in the US and now deep seek going on top so if it wasn't Tik Tok being banned so top six would have had CH three Chinese apps in the US app store which feels like wait isn't this completely opposite of what the US gov is actually trying to do but in a way it made me also realize more that consumers just care about experience they care about cost when you commoditize something so much and make the experience decent privacy is way less of a concern it's a national security concern for the government but it's not really a concern for most individuals I think it's for us techies we have a lot of concern around okay I'm actually going to read terms of service how are they going to use my data I care about it or I want to run my model locally deployed so I don't expose anything but for most people it's like oh it's cheap it's free it's good why not because what I find funny is that unlike most other Chinese companies deep seek did not try to hide their Chinese origin because even in the developer section most the time you'll see the app name and developer name right underneath they didn't even bother Translating that into English name in their apple developer registration and then just straight up Chinese characters even that didn't stop us consumers from downloading thing and then that in a way got me quite excited where I think there's no borders for good consumer apps when there's good Tech and good prod product it will just have that WI spread adoption because it's just so good and in a way it feels like the chat GPT moment from you know two or so years ago where it was released it was free and then everyone swarmed into using it and very similarly I think similarly for deep seek and given the amount of usage I think they are also implicitly building up a data mode that they did not expect I am going to say though I'm not looking forward to a future where China feels so confident about its dominance in the AI world and in the AI research world that they're just going to publish their papers in Chinese and be like good luck I know you're G to try to translate it so why would I bother yeah selfishly because I don't know Chinese I'm not looking forward to that but I say if the models are so good and the Chinese are so confident the model should be good enough at translating that very accurately word for word meaning to meaning into English without any issue including images right good point okay so maybe the future is not that dark after all yeah and like you said before it's very exciting to have these things so spread out and so democratized when everyone is able to just read into this I think this is something that I still don't quite gets from Deep seek which is why they did everything open source um because it's such Advanced models it's not like they were six six months to a year behind most Frontier models why would they do this like what's your take on that not that I know for sure but I've heard that drug dealers give you the first dose for free to get you hooked so CH gbt did this right yeah true exactly um so I can imagine a future in which you're just um you start trusting deep seek because they're open source and then comes the commercial version whatever it is and now all of a sudden there's some sort of psychological trust that they felt over time and you're you're more likely to buy the commercial version especially if it's so much cheaper than open AI so ultimately it's probably a trust exercise yeah and in a way I was also thinking about about this the other day where this may be a slight overgeneralization but I do feel most people have very little Trust on Chinese apps or Chinese Tech especially if it was closed Source right I still have a Tik Tok on none of my phones ever sometimes really good it is so good for wasting your time problem it it is just the ultimate version of Doom scrolling if you think Twitter is Doom scrolling it's not Tik Tok is true Doom scrolling about my point because people have so little Trust on most Chinese apps I feel like for most Chinese companies and this is not just deep seek right Alibaba open source there q1 and all the q1 coders there's 70b and 11b models all I think performing really good on benchmarks right I think they also made a stir when they were all released but just not as big as current deep six1 the same for 10 cent the same for uh buy Downs in terms of a lot of Open Source models I feel like this may have been the required step to really gain trust all the Western Community of going hey we're not just going to tell you metrics and you think that oh we are boasting Great Tech when we don't have anything it's open source run it for yourself reproduce for yourself you all know for sure I think that in a way has been a really smart play in in terms of getting giving people that confidence like you said said you know doing deep seek the app for free is like giving people the drug the first dose of the drug for free by the drug dealer maybe open source is exactly the same thing it's giving the developer the research Community the same first portion first dose of that drug to maybe even attract Talent on top of this not just to gain trust but also attract Talent when I was reading a interview with um the Deep seek CEO from I think just six month ago they mentioned deep seek V2 all of the research team were purely Chinese and then hide from China and even himself he doesn't believe that he they have any of the top 200 talent but they believe they'll be able to train or have the best talents in the future I think that's to me is also like quite exciting and then now with the St they've made with the rest of the world I also feel like attracting Talent on top of building trust may have been their major purpose whether it's by coincidence or not but super exciting interesting yeah marketing and recruitment for sure um You didn't tell me how you feel as a as a Chinese uh living in the Western World how do you feel proud what what's going on in your head I well I mean as I grew up in China I definitely feel proud that a Chinese company is doing taking great stride in such Frontier Tech because even in my mind in most places China wasn't known for the place for innovation in most places it's really good at producing things it's really good at replicating things but RAR do hear people talk about China as oh hey they're leading the frontier in something this happened a few times in the past two decades I feel EV EVS have been a thing highspeed rail have definitely been another production I think to most people it doesn't feel high-tech enough to feel like it's leading of like the Silicon Valley is doing so this for the first time feels like China came out with something that is leading or maybe completely surprised and most of Silicon Valley even this time that to me has been quite exciting because in a lot of ways um this is again I know I'm quoting the um the Deep seek CEO quite a lot because he did say a lot of interesting very interesting guy I'll link the original Chinese article the interview article in the description so people can read into it through translate but a really worth a read because it's not that China is lacking top talent it's that because China isn't doing enough Innovative Tech the top talents aren't being discovered and so most people are going into oh the traditional big Tech the big Tech of China and then going into those traditional Industries then because the slight lack of research inside those Innovative Tech so you don't see the same level of advanced research actually coming from China but now with deep seek I think they just showing hey we've got the talents we didn't even need to Source globally think about what we can do if we can I think this is also another statement they they're trying to make with all of this open sourcing or just open sourcing is another soft influence to the research and development world as well right so going back to your original question of and how do I feel very proud as a Chinese person that a Chinese company is able to get there but I'm sure that any advanced Nation with good Tech Talent I'm sure you know com countries like India will definitely get there hopefully Europe will have something better in the coming years you know fingers crossed because I do live in the UK now so I do want Europe to still be in a really good place or maybe that's why you moved away from Europe because you lost [Laughter] faithe the new caps on the bottles though does the UK do that too or just the rest of Europe yes it's got into the UK as well it is very annoying in general I am super excited about where this whole thing is going I'm sure other Chinese companies will be also releasing more interesting stuff but in general what dsek is doing forces the whole industry to move faster and enables them to move faster because of the open source nature of the thing and that to me is what feels exciting and in general having more competition is always good whether it's between countries between companies or between different technical approaches since we're talking about the future and predictions are you buying Nvidia stock now that it's down um personally I wouldn't my take is the the other reason I'm not sure that this was talked about significantly in Twitter or other places is Huawei at the end of last year also announced their Ascent was it 100 C I can't remember the exact model number AI chip that they were making produced purely out of China and one thing that's they've purposfully built the chip for was inference time compute so not as efficient for pre-training but very efficient for inference time usage and the people were claiming this not verified but just looking at a lot of sources claiming that deep seek R1 and V3 are running on this new infrastructure so I think the hit on Nvidia stock wasn't just because of deep seek making things more efficient think making people think that you don't need as many CP gpus anymore more but also the fact that China may be able to self-supply a lot of these chips additionally may be able to produce these for a lot cheaper think Huawei for 5G Tech or China for electric uh electric vehicles because they were able to internalize this because they have the supply chain so they're able to get this out way faster so maybe Nvidia finally has a new competitor but there's a number that I know like people wouldn't start worry about yet because looking at the reports they only have apparently 20% yield which is extremely low for chip making and apparently you need around 70% to make the chip production actually viable so they've still got a long way to go but Huawei still said they were going to stick scale up production at the beginning of this year so we'll see how they do you're buying stock and I'm definitely like supporting Huawei in this not because it's a Chinese company but more as it's interesting competitor that has been restri handcuffed quite a lot but purely as a interesting interesting future prediction there's a lot of opportunity there what about you um I tend to agree with Kathy's take if you've seen it on Twitter which is the the paper basically show that we can't scale via compute so we're going to need more compute and you know televisions used to be very very expensive too and only a few people bought them but then as they become cheaper more people buy stuff so overall I'm pretty sure that the market for television is bigger in the end so I assume that's what's going to happen with chips as well just because they're cheaper doesn't mean that they're not going to make a profit they're just probably going to sell more of them and and you and I will be training deep seek in on our desks that moment I'm definitely very excited for I think my personal take is that that's why Apple stock actually grew during this and during this the whole Tech sell off if I remember numbers correctly Apple stock Rose by 4% and then Nvidia fell by 16 whether Apple planned for this or not is that they were a huge proponent of local deployed llms even with apple intelligence they were talking a lot about how running most things on your Mac on your iPhone and also you know only when it's needed Shifting the compute into the cloud which is still private according to Apple so then this plays very well with Apple's way of doing things where in most cases I see I was made a tweet about this a few days ago of I believe the majority of user AI queries were will likely be served with locally deployed llms in within two years time just because like you said you wouldn't use1 you'd use 4 most of the time because it's efficient it's fast it's sufficient right and I think this will be the similar case if the distilled deep seek models can get so good that you can literally run a local model on your iPhone why would you need gpus in the cloud to run this but like said it will run most of the cons consumer payload but I don't see this thing actually running most of the Enterprise payload even though that won't be the majority of the queries but it will consume probably 90% or maybe even 95 99% of the compute so it's not a question I guess my statement is not about how we won't need more compute but more on locally deployed llms and local models becomes way more Vi viable like you mentioned find you your own model Maybe just running all code with all context of your file structure everything you know maybe the million token Gemini flash model type models running locally that would be amazing we able to shove in so much context into our local model without worrying about privacy and such speaking of consumers how do you think these chat interfaces will create stickiness because I am one of the biggest chat GPT fans as I said it's replacing Google for me I really make Google searches but the moment deep seek was released I'm like oh okay I'll switch to this tab now and again I'm I'm one of the stickiest consumers of chat GPT how do you think people will will produce more attrition and also will they is maybe the chat interface is doomed maybe maybe there just a game that's not worth playing um I think we can look back at how web 2 did really well I mean the previous big Tech Google gives us the stickiness because of the amount of personalization and data they hold about us and then so using Google is just so effective and even today I would say if Google came out with a model as good as GPT 40 I'd instantly switch back or with the same kind of ux because it already has all of my Google Drive my Gmail my calendar and if it can literally you know book me Google payments everything right book me things into end I more than happily use Google but most times it's just Gemini is still not quite that yet both in terms of ux and capability at least from a consumer perspective so I feel like personalization is the key here because chbd kind of started doing this but don't feel like they've done enough which is the memories feature they try to remember segments about your preference of oh say Ruka ask because Julia asked a lot about um let's say machine learning therefore she's always an interested machine learning person you know show more DET technical details here or she asked a lot about um maybe how to create YouTube videos here's her interest in the future here has more context I can give her specifically on that I think that's what was missing from J gbt they felt like they had a start but never really pushed forward similarly for gpts the gbd store felt like it was going to be a huge thing um whether it's because the market didn't need it or whether it's for some other reason I don't know but it just didn't have any like you said the sticking the personalization I feel is the main thing that's missing the level of integration nor did chbt try to integrate say like zapier or the equivalent to connect a lot of the services boom straight in your CHP done right and they're still just slowly adding in natively built features so really if another company comes up with a good Tech good product it's so easy for them to get users and maybe that's why deep seek released the app so quickly perhaps they didn't even want to but then decided oh actually people are so interested let's do this because it was so easy what about yeah um I agree with you um I actually use Chad GPT almost like a co-founder I tell it my business ideas and and it's a it's finding the right balance between being yes and but also um not agreeing with me all the time however a lot of times I have to repeat the situation that I'm in and the idea that I'm working on and it I don't have a lot of transparency on what it remembers and what not because it clearly remembers things across SE sessions but is just not very transparent to me what exactly so so basically I'm I'm craving that sort of personalization that you mentioned that sort of memory that that carries over from query to query and it's not there yet for sure yeah and feel that you you get so much context just by analyzing the queries it's just like Google knows so much about you about your personal Behavior or your your intents your desires your habits just by looking at the queries even if they didn't care about the search results I feel like that is a level of analysis that's required by these companies and maybe that's why deep seek is also collecting a keystrokes is they want that data mode fine but um we'll see what interesting stuff they come out with because I also remember for deep seek um the CEO actually said they weren't interested in doing consumer apps but they could very easily if they wanted to if they saw opportunity this to me is the exact moment of where they saw opportunity and then shipped something extremely quickly what about developers how do you build loyalty among developers personally speaking I don't see how you build loyalty amongst developers to your own API service if you're open source so this is the thing I still haven't got unless maybe they do have some kind of In-House optimization that or Hardware tuning they never released and then they're able to just undercut the price by 20% say without with while still making the same profit margin maybe that's the only way and then you know having separately deployed Western models by a different operator and then you know a Chinese deployed version separately maybe that can also help gain Trust but in general I don't think they need to do much other than keep the price low or I mean keep building more efficient models and keep things open source what about I mean like when you are building applications would you now switch your open API integration straight into changing that to deep seek with a new API key now there's so much infrastructure that makes that switch extremely easy if if you think of hugging face or light llm all of these interfaces completely hide the underlying model so from that point of view from from the technical difficulty of switching it's almost zero it's sometimes just the flag to pass in the right clawed version so yeah no I have no loyalty at the moment then if we go bit further down that rabbit hole are we saying that Frontier models actually have a harder time compared to applications to get sticky users because of like you mentioned that infrastructure that I mean just that infrastructure and it's so easy to make the switch and unless they built in some kind of underneath the hood optimizations at the end of the day it's just Cost and capability it's just price to Performance ratio right we don't really care about the 03 that got the 90% on the arc AGI with was it $30,000 or $300,000 something on that order of magnitude but people care about oh I can do this for 10 cents yes to me the strongest proof that llm foundational llms are commoditized is how well the comparison with food uh matches llms so think about it food is also super critical like you you need to you need food to survive the same way llms will probably be critical for technology because everything will run on them however a lot of people are making very very small margins out of food so just because they're addressing a very core necessity it doesn't mean that they're going to be sticky or they're going to be very profitable that's exactly the same with you know Agriculture and food so because the comparison is so Eerie to me is the ultimate proof that it is a commoditized industry yeah do you think we'll have a very similar War to how Cloud I mean Cloud happened a decade ago where AWS was the first and then I mean this still the top right and but when other providers actually came along with in similar Tech and similar price they were able to grab a lot of the customers that way do you think this case is similar I think that's a complicated question if you think about agriculture it didn't happen that way there's still a lot of small players yeah if you think about Cloud it kind of converged to four big ones I I honestly don't know I can I can see both Futures happening where as I said before you and I are training our models on our desks and then and we're selling them on some low margin Marketplace but I also see a future where 3 years in um llm training stops being so sexy they stop um attracting so much good talent and then only four companies Will Survive that's a possibility too yeah I just hope at least it's four companies it's not one as long as it's not one I'm definitely happier and also it's four companies ideally distributed by four different nations that would be the best possible case yeah I agree what are your criticisms of deep seek what are things that you didn't like what did they botch about this launch I I'm not sure if they aim for this to be a consumer launch because everything felt like oh they saw a moment and then just decided to go with it go with the flow um with everything they did because they did similar kind of really soft releases of new things every sing every other few month and then just two days ago I'm not sure if you saw they released their image generation models that apparently is on par with Dow E3 didn't even make a tweet about it it was just released directly on GitHub with the paper I like okay is is that do AI companies just give us stuff for free now is is that a new Norm if so I'm happy but it's they've always been the type of company that's really embodying the build it they will come type mentality which it's yeah it's I don't feel like that works too well in this world is it's good at create you it's in your favor to create more Market hype I do want Leon to be more out there to be you know like chanting more about deep seek I think that would be amazing and for us to learn a bit more about the team the company behind it to see more interesting launches and then to see them getting into more interesting consumer side of things but really I mean this is asking for the cherry on top of the cake now and I'm just bitching about not having a tasty enough cherry on top despite having amazingly tasting Black Forest gattle right I was watching the CNBC um mini documentary they made about deep seek really good uh but they couldn't reach anyone from the company to interview and that was surprising to me why would they not want to ride the waves why would they not want to be famous I mean open sourcing something means you want to be out there yeah in a way maybe this deeply rooted in the company oh I was doing the similar research I saw the CEO only ever did did two public talks ever since the past well now 10 years since the company's Inception or at least at least the Quant trading firm's Inception so maybe that's just part of the company culture to just build and then let developers believe in what we have in a way I think that may have worked quite well uh I mean that may have worked quite well for them because now people are like flocking toward deep seek to go oh yeah you feel more trust when they don't give you so much marketing Spiel and I know people dug up Sam alman's interview from was it 2023 saying yeah don't compete don't compete with us I mean I may have botched the original wording but it's basically don't compete with us you won't win yeah he does that when I was in YC I did Summer 24 and he came to our batch and I heard that that speech yet again just don't try to compete with us right wave and I'm like isn't that convenient to tell a huge room of ambitious people to not compete with you would you have said that if you were still the partner or the CEO whatever of YC I don't know I think we talked about the launches as if it was a bad thing but I mean given the users they have the attention they have whatever the launch was however botched they thought it was or the world thinks it it was it was amazing it worked right any criticisms on your part my criticisms are in the same vein um the main one is the paper is not structured very well it's very hard to read but again they made all of this information public and it's amazing techn technological innovation so I'm just also asking for the cherry on top so it wasn't super straightforward to follow um the second complaint but it's a feature request more than a complaint is the fact that they didn't open source their training data or the yeah they don't they don't have a lot of clarity in the paper as well on how they produce the training data and um I think there's a little bit of controversy around them using chat GPT and not admitting it in the paper at least from the CNBC interview that I watched at some point the model was claiming to be chat GPT and made by open AI I don't know exactly if that was through the web space and they fixed it in the meantime anyway uh there were claims like that which means maybe the paper isn't 100% truthful and also maybe that's why they didn't release the training set um but I'm happy to see that hugging face has um an effort is calling contributors to try to reproduce the entire thing into end yeah I mean naturally as a Chinese person normally I would want to say yeah I want 100% defended no no they didn't copy anything but if we look at history there's definitely incidents of similar things that's happening and in a lot of ways I think this is only the smart way to compete not saying it's a right way to compete but it's a smart way to compete for most countries and even I think looking back at history us only introduced their IP laws after a lot of Espionage of getting European Tech back in was it the 18th or 19th century I can't remember the exact timing so you you protect yourself when you ahead when you're behind why follow the rules exactly yeah so the setting the moral standard here I think will be very difficult because the top players will want to protect themselves and then the people who are catching up will do whatever they can to do it and again just never saying if this is a right way to do it but I can definitely see why they're doing it yeah do you know if it's against opening a terms and conditions to train on De data generated by them must be right we should just check now if I were open AI I would would I would add this in today just go hey because I think they can right let's see is it does OP AI allow training on their models are you asking CH GPT yeah deep seek needs to build a desktop app i' switch over very quickly I think even if I paid for every single query by crus it would still be cheaper than the $20 per month more likely okay there we go answer open eyes terms of service explicitly prohibits using their service to develop models that compete with their own product so yes what does it mean to compete if you're open source are you competing with open AI very good point this is probably another part of the smart play right because in a lot of way is you can still you can hate a company for stealing things for their own use but you can't hate a company that's like Robin Hood that's stealing from the rich and giving it to everyone for free so thinking on that point question what's the impact for startups in particular and I I mean very early startups does it impact them at all I say quite a lot because of the cost efficiency um because previously the well I mean because deep seek took reduced the cost by a nearly or more than order of magnitude for the exact same capability this should enable say a lot of startups who might not have funding to be able to may even bootstrap and then test and build things just inhouse for the startup that already has funding they can do either provide better user experience experience for the same price or just massively reduce their cost base in a lot of ways or maybe even make you know replacing some of the previous manual labor that felt too pricey and then making that completely viable given the low cost so I think cost is the major playing Factor here and then that should hugely benefit a lot of startup I think we'll probably see even more of an explosion of consumer startups building on top of this now that people can see oh foundational models is way more compe even more competitive or building infrastructure there is even more competitive you should just like we said build consumate Tech you know build applications that actually has a user mode dominate your Niche and expand from there I know that's the YC Mantra right yeah I I also agree that theep see is a good reminder of how important the surface or your ux is again if the infrastructured layer is getting so commoditized all that matters is that you have a distribution Channel or you have a ux that is sticky and people will not just leave when the next shiny thing comes out because there's some intrinsic value in the network effect in the community that you've built or in the quality of content that you produce so it's just a reminder that GPT rapper quote unquote um is less of a joke than it ever was yeah funny that right a year ago everyone's saying yeah GPT rappers will all die when open ey gets so good turns out to be the compl complete opposite yeah open ey is is kind of limping a little bit yeah oh the really funny comment I saw is um chat gbt lost its job to AI yes I actually retweeted that tweet I thought it was incredibly funny but in general we love seeing competition it's such an exciting space to watch right now what are your predictions for the coming year H based on what we're discussing like purely on the model and application front yeah I like to think that open ey is cooking something um you know there's a lot of rumors about GPT 5 and how it's meaningfully better than 01 I don't even have enough uh imagination to figure out in what ways it could be better better other than just like cheaper and faster 01 but I'm always excited to hear what they they come up with because it just beats my imagination that the whole idea of reasoning models um was pretty Innovative I think in 2024 when open AI came up came with it and yeah I don't use 01 on a on a regular basis but I'm just I don't know if consumers are going to Ste still keep benefiting from these llms get getting better and better you're like for consumers we've already reached the Peak at which we don't appreciate any more improvements beyond what we're seeing so it's probably going to Mo higher up the chain to you know companies and Enterprises will actually benit more and more from these incredible compute Powers yeah I definitely agree with that it feels like now 2025 should be the year of AI applications given so many Frontier models are being so it's just commoditized right now when you're at the cost of deep seek V3 and R1 there's almost no reason to not building some kind of ux improvements leveraging AI whether it's leveraging AI to build or building AI natively into your own appliation um and because of that challenge for foundational models to gain that stickiness people in I mean doing foundational models I'm sure they will venture further into the consumer space CH gbt's operator I think is a great example of hey here's the thing that we build on top I can see people if they found operator to be reliable and effective they'll stick with chat gbt even if a new model comes out and but I'm sure there will be open source Alternatives that try to glue everything together to create operator perhaps from deepsea combined with I think saw recent project called browser use also recent YC company looks quite exciting open source like computer use equivalent just compose everything to create very similar things but in general yeah consumer I'm super excited for Consumer application development super exciting session and I can't believe that was an hour that would gone by so quickly and we should do this often this been it was really fun chatting