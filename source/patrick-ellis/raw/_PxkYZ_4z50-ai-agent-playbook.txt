If you found it hard to get 10x productivity gains out of these AI coding tools, you're not alone. Savvy software engineers have been adopting powerful new tools like cursor, claude code, codecs, and others, but they find themselves in these endless loops, fighting against hallucinations, and ultimately seeing that these models are doing everything but what we ask them to do. They wonder why they're not seeing that order of magnitude improvement in productivity. Sound familiar? The models, tools, and other infrastructure are finally getting good enough to enable this level of productivity. But you have to know how to set everything up and how to orchestrate the right framework. And Microsoft just asked me to speak to a group of their elite software engineers that are reinventing the way software engineering is done. In this video, I'm taking you inside my talk at Microsoft where we're going through the exact orchestration playbook I gave them. the workflows, tools, MCPs, frameworks, and the overall philosophy that we've used to create an orchestration layer that surrounds our coding agents to give them the context and tools and validation steps that they need that has led to the huge productivity gains that we've seen over the past few months, especially. By the way, I'm Patrick, a CTO and co-founder of a startup that is dead set on scaling as far as we can with as few people as possible by leveraging productivity games like the orchestration playbook I'm about to show you. It's been really fun being one of the first startups to have a significant amount of revenue going through a truly AI native app. We launched this about two years ago and have since worked with companies like Coca-Cola, Disney, Google, Microsoft, Nike, and many other amazing brands. We've also been heavy users of Claude code since it came out back in February and have invested a lot of time into our orchestration framework, the MCPs, the context management and other frameworks and tooling in order to get the absolute best performance out of cloud code in addition to cursor which we've been using for far longer and other AI agents. So with that, I'm very excited to show you my talk at Microsoft. And I hope these ideas are just as valuable for you as they've been for us. enjoy this insider look few of the core workflows that we use across the team here. Um and then I'll go through some engineering workflows and then give you guys some kind of strategy uh some highle ideas that hopefully are very tactical as to what we've learned and then a little bit more um supplied like insights as we've been going through this. I'm thinking of our entire team and especially myself more and more of orchestrating an environment and creating an environment where these LLMs can run and have the context they need to help again not just with engineering and product but the entire organization over time. >> Quick question on the communication. What's Bolt useful for for internal communication? >> So, okay, excellent question. Our CEO, our CPO are both not engineers for them there's a couple amazing use cases with Bolt that helps and I would classify that in the communication realm. So the first thing is they're able to iterate to design to think through product features with the same constraints that I'm thinking through on the engineering side which maybe doesn't sound like a huge deal in Microsoft's context. I'm doing my best to kind of you know I'm at the very polar opposite end of the spectrum with this tiny team but in our context it's incredible because it saves me hours of having to oh hey you know we can't do that or you know this uh we could potentially do but it's got these other considerations allowing somebody to create a you know next.js JS uh app in React using you know whatever backend can be a really helpful way to allow them to kind of understand uh without communicating to me but in the interface of bolt what's possible. >> I see. So you're using it for like ideation helping them build like interactive prototypes and stuff just to share ideas and just to communicate with the tech people. >> Exactly. So the end result is instead of like a Figma doc or some other you know UIUX uh uh schematic I get actual code that you know compiles um and runs. So that's one big advantage just to help me not have to advise them and then also for them to communicate more details through fleshed out CSS for example as to what exactly they're looking for. And then the second big thing is being able to build prototypes and to get those in front of our users and well actually maybe a third one which is internal tooling. So, uh, our CEO like without me even knowing was, uh, was prototype or prototyped a couple things and was bringing that in front of some of our partners to just get their feedback on, hey, uh, you know, I haven't talked to engineering yet, but here's an idea. Is there any interest there? And that is so much faster and more effective than, um, well, we weren't even doing it before, but having a Figma doc, for example, and running that through our customers, uh, for them to have, uh, something that just communicates this idea that our CEO has much more tactically. And then uh the internal team uh use of that has been really helpful. I I'll show a couple screenshots of these, but them being able to basically uh run with and build internal tooling. So I can think of that as communicating all the workflows that like our operations team. I mean they know that much better than I do. uh I'm just you know I try to have as much context for all parts of the company as possible but at the end of the day they're the ones that are interfacing with these systems understand the constraints understand what our users or our customers are asking for and they're able to build internal tooling uh much more competently than I can in the sense from like a product perspective and then I can just go in you know refine things from a security and uh you know integrating with different APIs that we have perspective actually touching on I think one of the biggest helps especially from what I perceive to be uh this context which is how do you eliminate the the bottlenecks that will come as these codegen tools get better and better and better and those bottlenecks are around communication at least in our context and I've got to think even more so at a larger company uh than us so just thinking of how can one person reach in and do a lot more and also how can uh we use these tools to basically have a bunch of meetings or communications or docs or slide decks that just no longer need to happen because the communication loops groups were so fast and they uh basically were happening against LLMs instead of meetings or talking uh with other people. Um again, an important part of that is having the right context so that those LLMs actually are uh interfacing and and representing the thought that we'd want to have correctly and not just uh with whatever their pre-training data is, which can be pretty good, but it's not as uh powerful obviously as like the domain that you're working in. >> It's the new version of you're saying this meeting could have been an email. Now you're saying could have been an LM. >> Yeah, in the future, maybe in 6 months, I'll just I'll send uh Thomas my my AI, you know, version of myself. Uh so, um I'm just trying to really think outside of the box on ways to do so much more work, you can imagine, like weeks worth of research essentially, um and distilling the best ideas, whether it's from a conference or internal meetings or whatever the case might be, uh to get really actionable applied to our situation, ready to go insights. Here's a few other ways that we were able to provide context into that orchestration layer. So the first is granola. This is synonymous with any voice recording app that's able to basically take transcripts of what's happening and then record um a summary. At this conference I was just at the AI engineer world's fair down in San Francisco. being able to record all of the meetings I attended, scrape the website for the 224 talks, the descriptions, uh, information about the speakers and company, record the talks, going through all the material that we learned that day, feed all that into using Granola into deep research, and then notebook LM's a helpful tool as well to just basically uh, query all that information or create a podcast uh, based off of it. an incredible workflow to just mine all of that information and get actionable insights, especially when you pair that with your written OKRs or other business objectives. So, kind of again summarizing a ton of data, taking our own personalized information about the the company, our current objectives, current projects we're working on, and allowing the power of Genai to distill that down into very actionable and even in our case uh in light of our tech stack uh uh you know, libraries that we could be using. Another cool use case are other frameworks. So of course SWAT analysis, BRDS, PRDS like product request docs that can be a really helpful way to take a bunch of information ask whatever model you guys are using and to filter it through a framework again apply to your situation. So corpus of data framework that you want to use to help shape up and get the model to think uh and the process and the reason through the information in a in a structured way. for example, like a SWAT analysis where you have four quadrants to split out actual information and then applying that to your specific situation. Those three pillars have been super helpful for product and market research using deep research tools to understand industries, competitors, uh customers and even one cool experiment has been getting two models that talk to each other just through a very very simple like uh use of the API to basically have like one pretend to be an you know principal UX researcher or uh customer development like interviewer and then the other to be you know uh CMO of a midsized um healthcare company or whatever you persona you're looking for. And it's pretty remarkable how much information these LLM have. Again, just thinking of the pre-training, which is essentially absorbing as much of human knowledge on the internet, books, you know, YouTube videos, uh, other resources, uh, as possible. So there there's quite a bit of context if you're able to mine it through tools like that. So that's something we do a lot to inform our product strategy, marketing, uh, and just to get ideas for new products. And then, of course, we'll go and validate that with real, uh, user information. Operations, uh, there's a platform in ADN. I don't know if anybody's heard of that. Okay, it sounds like yeah, not a number of folks. Super helpful as a pretty technical way to orchestrate through a guey um different workflows. Zapier's also got an interface like this. A lot of different, you know, companies are are coming out like Gum Loop's another one that we've been experimenting with, but just a way to allow interfacing with all of the LLMs, different APIs you might use again in kind of a Zapier-esque way, but the allow other parts of the team to uh to build little workflows. I found this though to be really helpful even for me to uh like one example of what I was working on yesterday a little bit was how can I write one piece of media? uh YouTube script or uh LinkedIn blog post and then have that basically translate or well one to have a ton of help fleshing out those ideas kind of through the process I mentioned earlier and then two um to basically format those into other you know types of posts that way I can just speed up my own internal workflow of trying to share some of these ideas out in the public and you can imagine there's a ton of different workflows internally that we're able to use these platforms for one of the biggest themes that we keep coming up against as we've been building is the little bits of friction that might not seem like a big deal actually are a really big deal in terms of allowing us to get these models to go from just helping us with basically like what we would use uh being searched for in the past but to go a lot further and to start to become actual agents that are working on our behalf and then eventually um like proactive asynchronous agents that are just doing work for us without us even asking. So we're kind of you know inching towards that. One example of lowering friction is I'll use something called Super Whisper. There's a few of these platforms, but it's a really amazing speechtoext platform where I'm just uh dictating. I I've got a hotkey. Hold that down on my computer and then I'll I'll mention a bunch of what I'm uh looking for. It's got a local model that's does an excellent job of translating that into text. And then it uses uh GPT 4.1 mini in my case, but you can have anything. And it knows where you're at. If are you in an IDE? Are you in a terminal shell? Are you in a word document? And then it will uh uh kind of transform the text uh or like uh reinterpret what I'm saying and clean it up to make it more articulate but fine-tune for that environment. Just amazing how much that just feels incredible in terms of me able to just speak and then know and like trust that that output is is is solid or go in there and make a couple quick edits. Uh and again if you think about that you know voice can be and at least for me a really powerful way to interact and orchestrate and control these models. So just thinking of all those bits of friction Dario the you know CEO of Anthropic was just saying recently sonnet 3.5 you could kind of trust it to do about 10 minutes of work sonnet um 3.7 you can trust it to do maybe 45 minutes of work this is in like a a software engineering context and then a sonnet 4 and especially opus you can trust that to do maybe a couple hours worth of work the friction points become do they have the context I need for the project I'm working on for a company for me specifically and Then what's the friction and like how much do I have to interface with these models for example in the context of a code agent so cloud code or uh open's codeex or uh 03 pro any any of these models that you might be using in order to get them to run and to have that you know 45 minutes or a couple hours you need to be able to give feedback if you're working on a UI app for example using something like Microsoft's Playright MCP which I'm an absolute massive fan of is a tool that these models can use to go and navigate around your browser to take screenshots to look at the network logs, console logs, to get all this context about what they're doing, designing these UI mocks, uh uh to build out a great interface. If you give the model all of that context, it can just continue to iteratively uh again have that short feedback loop where you're not involved and come up with much more competent outputs than uh not having those tools. One uh kind of funny way I like to think about this, stealing this from uh Anthropics CPO who was also one of the co-founders of uh Instagram, but is this idea of closing your eyes for a couple seconds or like 30 seconds and then just imagining that you're the LLM and you only have what the person just like prompted and like gave you. And there's not a whole lot you can typically do in that context, right? Uh with just the prompt that like I'm writing to chatbt, for example. It's like okay well that's that's cool man but you know what does the UI mock look like you know what's the context for you know uh like the the business objectives for this quarter what are the KPIs I need to adhere to and you know design to what's your style guide you know what's uh like what kind of stylistic um you know preferences you have in the codebase like what kind of open source packages can I use it goes on and on and on in terms of the context that you need in order to execute on that task well so I think having empathy for the LLMs and really putting yourself in their shoes even through that thought exercise. It's remarkable how much that can help. And then yeah, giving it tools as well. >> Okay, let's talk about tools as the next step because that's a big one. When you're going to feed this context, you have business plan up there and templates for marketing. How are you still, what format are you storing this business plan and I'm imagining I got my LLM up and I'm talking, you know, am I doing copy paste out of some word document I had somewhere and it's just a couple of sentences that we've all agreed on and then or How are you feeding the extra context? Because that seems really interesting and quite actionable for the work we do. >> We're truly like collectively on the cutting edge right now. I mean, there aren't good solutions for a lot of these things. And it's it's going to events like the AI engineer worlds fair talking to speakers just obsessing over anthropic especially but uh you know OpenAI's API docs to try to gleam okay what are the workflows at the internal >> cloud the system prompt for cloud came out a week or so ago and there's a lot to learn from that. >> Exactly. I mean that's such an excellent example of you know uh I'll answer your question but just to take this like quick detour um looking at what these teams especially teams like the frontier labs where the product is interfacing directly with the researchers that are doing like the post training and I mean everything with the LLMs because there's this like awkward divide between how much can the the raw model do then like what should the product be on top of that uh for example like you know the the chatbot uh type interface or chat interface of openi Um so understanding uh from all that collaboration that's happening and just gleaming like the way that things are phrased or kind of like claude code uh that was used internally for I think nine months before anthropic released it uh almost four months ago and just understanding the workflows and how do they envision agents is so interesting to study that and then apply that because it's way bigger than just code agents and it's way bigger than in cloud code's case a CLI code agent uh and cloud code is basically cursor but in the CLI in case anybody's not familiar. Um, it's it's it's it's showing the the path for how these agents will work and what kind of uh tooling anthropics exposing. So, one example, just just making my way back to you your question is uh they just came out like again a couple days ago. All this stuff is happening so incredibly fast with a mode that you can double shift into and um what will happen is it it's it's a uh like a planning mode. So, you're basically able to work with the model. they came out with a checklist like a month ago because I was using different workflows before that and then now you've got this iterative uh model where you can discuss you know product requirements and like have it do research and kind of iterate with it come up with a plan okay that and then go back into coding mode so these tools are slowly uh gaining some of this capability but to answer your question uh XML and markdown files are easiest XML because most of their training data on the pre-training side was ingesting just massive amounts of XML So I I'll use XML tags quite a bit to kind of you know help summarize. The models are getting so good now though that you know some of those tricks are a little bit less uh important. I still use it all the time and I I do think I get quite a bit more performance. And if you look at any of these >> just bullet points XML is already formatted in the way the LLM is trained. So >> exactly and specifically just like XML style tags, you know, just to like give context for breaking up the prompt. >> I just to continue off that question. Um do you do each one of these steps manually or you do you have a pipeline for all of this? like do they Yeah, just kidding. >> Well, I mean, so great question and yeah, these are connected. Uh so um right now some of these there's a bit of a pipeline for like on the engineering side using MCPs which I can explain a little bit more in a second but um a lot of it honestly is uh I mean it's just changing so fast that I'll get a manual workflow working and then I'll try to through NAND or you know a similar type process or a script which of course all these models are phenomenal at coming up with you know Python or bash scripts or whatever you're looking for or um in my case using notion as a as a data store essentially and that is interfacing with markdown or having a a reference library of prompts and like markdown files that I can use. >> Um, so yeah, it's a lot of kind of manually stitching things together and trying to automate it. >> So, how do you pass this down to your team then? Do you just kind of create a shared central data source for communicating, hey, these are the problems I found that work really well. Share them back with me or do you have like a better way of approaching that? >> I mean, great question. I'm literally thinking through this right now for for this big workshop tomorrow. So, it's very precient question. I would say to date using Loom or any platform to quickly just like hey here's this new workflow I'm using. Uh you know giving them a couple different markdown files. Uh so markdown is my primary. I just have like a context folder I'll throw in a repo. Then I'll put any uh context. So uh you know docu uh documentation different like a PRD different workflows uh instructions on how to use different things. I can explain that more in a sec but that just to uh finalize that question. Um and then for the team I can like take that folder uh and then with the loom that explains all the context for how to go about using this workflow I can communicate to that to them. But I think a much better approach is uh two different areas that I'm I'm thinking through right now. Um one is basically building out notion documents uh in a way that these LLM can query through them and have essentially the markdown files uh all ready to go there. So a person can go in edit what they need to edit and then quickly reference that in uh chatgbt or claude or or cursor is a lot of what our team besides me uses to to iterate through. >> One thing just to kind of mention this is a really really powerful tool that connects back to this is MCPs. Is anybody here familiar or like have probably heard that buzzword flying around? I think it's the single most uh asked question I've seen on the internet the past couple months is what is MCP? So uh model context protocol is what that stands for. Basically it's a standard that anthropic le but now open AAI um Google a lot of these other big platform providers are behind MCP. What I see it as so helpful for are two main things. One is MCPs are an incredible way to package up and give context to these LLMs for how to use tools. So what is a tool? In this case I'm thinking of notion. So an MCP I can go in I can copy a little bit of JSON which uh I'm I'm sure in the near future we'll have much more elegant solutions to this. As a side note, right now at the conference they talked about a registry of MCP registries. So there's so many different platforms listing out all the MCPs that there's literally a registry of all of those. That's how like messy the whole MCP environment is right now because the spec came out in uh November and it's you know quickly getting adoption. But um I I'm imagining going into one of those, grabbing the notion MCP for example, and what is that? It's basically a wrapper on their API that gives the LLM a bunch of context as to, hey, here's how you can read documents, you know, create documents, you know, blah blah blah. Also, here are some additional resources or workflows that you can use. And there's a bunch of other advanced uh new parts of the spec which I won't go into that allow all kinds of cool much more dynamic stateful things to happen over just a raw API. But why that's cool is now I can go into chat GBT or cursor ask it to use you know some name that I come up with for a workflow and then it can just iteratively do that. So what's especially neat in the context of an enterprise and I think more relevant to you all is being able to encapsulate tool use internal workflows prompts um essentially the markdown files I was mentioning earlier all within an MCP and uh you could create these very easily. uh the models are excellent at uh taking in the documentation to create them but then you have you know some standard workflow that you have for creating a PRD or um interfacing with you know whatever uh again I don't know your all tech stack but um workflow you have before you get to like a PRD to create a feature uh so like linear or gr for example get all that context that's needed and then transform that into a standardized way that can then get fed to the the models or soon to be the agents uh to be built upon. So um all this to say right now it's Loom and giving people the source markdown documents that I'm using but in the future and I think for you all to really think about is what are the MCPs that we can use to encapsulate all that tool use and that process um into one little easy to implement uh script that other people that are not technical can bring into Claude or you know chat GBT or cursor or copilot. I'm I'm sure if it's not already implemented will soon to be able to build those things out. So I think that's uh that that's what I would recommend. >> Do you find um Thank you by the way. That's a good answer. My question is around the nontechnical people. >> How do they feel about like adding the context through some markdown file and do they you know how do you train? How do you get them to shift the way they think of working? >> We've almost died so many times. I mean the last you know two and a half years have been phenomenal with AI but before that that in our DNA is just this we have to adopt new things and we have to to move and uh use these tools. So that's just part of the cultural DNA. I would say though it what's amazing is giving people demos of how powerful these things are and how much time it saves them >> and Nate and is really good for showing people what's going on in a system in a visual way. I I think that's an excellent example. Uh being able to I mean it just looks neat. Also, you can like see your entire like day workflow laid out. Um and then to see the outputs and now that these LMS are are so good. Again, I'm um I'm still just absolutely blown away by Opus 4 and Cloud Code. Although again, lots of great models besides like anthropics. I don't mean to be uh uh you know championing one or the other over the other here. But um I think just the raw ability for these to produce and basically to augment your work is is what's been the most helpful of oh wow instead of spending 3 hours doing this I can spend 10 minutes you know prompting with this chatbot um or you know chat GBT and using this MCP that you know Patrick just created for me um I'm able to uh get this incredible result. So I think it's it's at that level now to where that kind of drives adoption. >> Thank you. >> Yeah, >> you mentioned different models. These models have uh kind of uh their own uh uh style for different things. Yes. Especially for us have this coding that I use them a lot for. I don't know if you experienced that or you use that to your advantage or any any thought on that area >> just to kind of detail that a tiny bit. Uh Claude sonnet 3.5 was a little lazy according to some people and then 37 was way overeager. I would go and you know refactor a bunch of unit tests and do all kinds of adjacent things that you did not ask it for. Sonnet 4 and Opus 4 are much more steerable and they they can do all kinds of stuff but they will really really adhere to what your prompt uh uh is. The cloud code team talked about when they were training um and testing Sonnet 4 and Opus 4. There was actually a few regressions they noticed, but come to find out it was just uh part of their their their eval or their the prompts they use to test the performance of the model. there were a couple like mistakes that the earlier models didn't realize but that model did. So it actually was a good thing. Uh the models were adhering to the the examples. Uh uh so to your point the context, the prompting, the um tool use like all the things that you feed into it, I mean it changes when the when these new models come out. So I think the best solution to that is having uh eval trust is an like kind of the the best platform currently for that. but it's ways to kind of denote like a unit test like what are the you know the the prompts and then what are the expected outputs based off of tools and like other information that you're giving it. So my answer would be having eval that are able to uh give you you know however many nines of uh um predictability in terms of the outcomes and then unfortunately for now at least us having to go in and uh tweak you know the prompts the eval on a per model basis in order to account for those different um levels of detail eagerness all these other metrics that a model has. So these are a few examples of different workflows that we use on the engineering and product side in order to uh to speed things up. So I I covered MCPs at length, but there's also external LLMs. Some of my favorite are Microsoft's own playright to use browser the you know browser use GitHub. I'll actually use the GH CLI tool. It's you know same functionality. It's just some of the models are actually better with that tool versus um uh the GitHub MCP fire crawl to turn any website into markdown and uh a few of these other tools here. So playing around with some of these tools and just seeing what they're capable of is is really helpful to gather ideas and to drive adoption. >> Anything you want to say high level on Genai UX? >> You can think of it two ways. One is LLMs using your product or website or software, which is of course, you know, just barely starting, but it's going to happen more and more and more. The most obvious version of this right now is SEO dying in favor of um like LLM SEO. Uh there's a few terms for that, but it's not even like uh um you know uh consolidated yet. As LM are using the software we build, reading our documentation, discovering our API endpoints, they're going to be making more and more of the purchase decisions. Right now, they're already aiding us in those purchase decisions if we're using uh chat GBT, for example, to gather context for whatever we're trying to decide on, you know, an open source library or an API or anything else. But in the future, I I definitely see these agentic uh you know, end to-end workflows where they're using whatever you know, Stripe or a cryptocurrency of some sort to actually uh make purchase decisions. So, how do you make them discoverable? A lot of what we were talking about earlier with like markdown files, how do you instead of having a bunch of pages of docs with all this confusing CSS and, you know, images? Again, if you're an LLM and you can see images, but you're not going to look at all of them because it takes up too much context. How can you just like have one giant MD file or one standard that is coming out as an LLM.txt file that's basically a sitemap but meant for the LLMs that kind of as uh efficiently as possible get them all the context that's a lot more easy to to read. So yeah, which is again all these are really powerful uh frameworks. So that's one idea of uh Genai UX. The other idea though is how do we design platforms for people using Genai? One, I mean there's a ton of different paradigms for this, but one example I'll just give sprinkling intelligence throughout the app in chat GBT. One of my favorite examples of this is how it just automatically renames your chat uh with context about whatever you're talking about in a, you know, short distance. So, there's a lot of cool UX, you know, paradigms that way. >> Okay, great. >> These are a couple examples of different things we've uh uh like vibe coded. I'll give you guys a 30 second rundown. This was a new slideshow that we'll use at large events. So imagine this being on a you know big projector or screen somewhere needs to be highly performant. Uh it was really fast to get from zero to like 90% as you know t is typical with uh these this was also a couple months ago. So the models are significantly better now which I can explain. Um but what still took a lot of time was diving deep into WebGL uh you know virtualization of um uh the UI in order to basically you know I can't just render like 10,000 images or videos right? I'm going to crash any browser. So, how do I uh handle the performance? At the time, using Sonnet 3.5 and 3.7, it just wasn't uh quite there. So, I had to manually go in and do a bunch of that performance optimization myself. Um, still really fast though. It took uh you know, it was probably a fourth of the time that it would typically take. Um, I think we got this out in uh about three or so weeks, maybe four weeks. um with you know a bunch of testing and having to do a lot of uh research with just one engineer. Um one example though this was 5 hours uh our API our like an initial repo and then also taking some documents and some written uh PRDS and just being able to compile that super quickly into a working SAS dashboard that had again integration with our API uh authentication um you know it looks beautiful and and works really well. Um, one thing I'll mention here too is it was looking great, but um, I just asked it to use design principles from uh, you know, a couple companies I really admire and then I had it uh, use deep research uh, uh, to kind of distill and like like compile like what that actually means, format in that into a markdown file and then I took our like 15page style guide that was the PDF. I converted that into a markdown file, just thrown it into an LLM and you know having it do that and then I I asked it to use all those principles to basically take our uh Tailwind and Chad CNbased UI and to make that look much better. And it was shocking the difference of just a ton of little like animations and like C or you know UI polish. Of course, I would sit with it and and spend more time to kind of distill that down. But one point I wanted to make is for for um SEO for UI UX design for understanding a well structured API interface to like your backend services uh to think about off one engineer being able to go and explore so many different domains that they're not experts in and quickly get like pretty incredible uh results. Uh think about especially with this tech stack being Nex.js and React and uh you know a bunch of like modern things. It's going out, it's finding all these incredible new open source projects and it's coming back with an initial pull that's much better than what our internal team probably would come up with and it's it's basically instant. Then of course if you really get good at prompting and pulling from these these uh modern like San Francisco, you know, VC backed startups, you can get just incredible results. Uh same thing with SEO, same thing with a lot of other areas. So any role, not just engineers, you can use these LLMs uh to, you know, go pretty deep horizontally, but the magic in my experience is the the horizontal connection, whether that's eliminating communication or allowing somebody to get just a much better much more well-rounded uh end result by uh just even asking it to like improve the SEO. Of course, you want to be a lot more specific than that, but it's it's remarkable that what you can come up with. Okay, so lots of highle ideas, lots of tactical experiences that I've I've given. These are uh I try to just quickly kind of distill down what would be the next steps I'd recommend or I literally am recommending to our team right now. The first is to just adapt AI in your workflows. I've mentioned this a number of different times, but the best way to understand what these models are capable of, their character, the way that these foundation model labs are working with their product teams to bring the intelligence and the capability of the models into reality is by using them and understanding uh what they're capable of because it is pretty remarkable if you have a great model, if you give it context, if you use uh MCPS and other tools and give it access to that, if you give it tools specifically like within an IDE or a llinter or static analysis or again to take screenshots so that it can just iterate and have this feedback loop before needing to talk to you again. Amazing things can come out of it. So yeah, trying the different models uh I love the earlier comments about uh the nuances between the different sonnet models. uh that kind of information is actually really helpful or at least having somebody on the team that uh can speak to that and then especially using these agentic coding tools that are just coming out. Um codeex cloud code cursor agent is I mean again new as of like uh very recently and then open hands is is a really cool orchestration framework along with um fac or uh yeah factory as well. And then I would encourage you all to just spend 30 minutes or even five minutes vibe coding a personal app using bolt.new or lovable or any of these uh you know kind of end toend uh frameworks just to see what it's capable of. I helped run a hackathon at AI house recently and it was really amazing to see like what people were very surprised with specifically with Bolt uh in terms of what you can come up with. If you're an engineer, claude code with Opus 4. I mean, honestly, even if it's your own personal money, I would I would just I would give it a shot because it's it's uh it's pretty amazing what that model is capable of. And specifically, there's a few documents um there's a best practices with cloud code uh uh article that Enthropic came out with two months ago and then a mastering cloud code in 30 minutes YouTube video with their recent launch when uh Opus and Sonet 4 came out. whether or not you use Anthropics products just that mindset and the way that they use these tools and the way that the tools are being built is super helpful to gleam. So yeah, 30 minutes vibe coding or using a tool like uh cloud code and then your education stack is becoming so critical with things updating so fast and changing so quickly. uh for me at least with a small team uh going into different parts of Seattle and getting in these like founder groups where it's a bunch of technical uh early stage you know engineers essentially uh talking about the new tools that we're finding or going down to San Francisco for the AI engineer world's fair. I mean honestly talks like this you know is is a great way to go about it. Um and then having great podcast YouTube uh channels to follow articles and again just reading the documentation as boring as that sounds is actually not. There's all kinds of nuance if you're really looking for it. That's super interesting on Anthropic and OpenAI's website. And I feel like they do a horrible job like getting the word out that there are some great resources. Uh I will say uh recently I think it's it's been getting a little bit better on anthropic side. But yeah, so that's what I'd recommend. Um and I've got a list of uh specific recommendations I can I can go back to. And then from an organization perspective, I think empowering each person to build more of the stack as I was mentioning earlier. So you might be shocked what one uh engineer from like a UI, UX, um SEO, marketing, copy. Of course, these models are also usually really great about copy. Uh for my personal website, I just took my LinkedIn, converted it to markdown. Um and then I had this other big application I filled out that kind of gave my whole story a little while ago. So I I did that as well and just amazing what it came out with it, you know, with the first poll. Of course, I iterated with that and then like manually rewrote stuff, but just to have like the white be uh page problem taken care of was was phenomenal. So, just seeing what you can build and exploring much wider than you're used to. Um the second thing is identifying the largest bottlenecks. So, as I mentioned earlier, likely I'm I'm guessing this goes to communication and how do you get one idea and get through all the stakeholders and get back with uh everything checked off, ready to go, insight from all the different areas. How can you do that instead of having a ton of meetings and you know um uh sending a PRD back and forth? How can you just come up with uh like a vibecoded uh P or um uh sit down and get an actual working like MVP together with one person and then iterate from there or give the right amount of context into the LLM so that that initial MVP or PC is much much closer to on target because you've taken the time to uh you know get the product engineering uh you know customer comments uh style guide everything together. And then the last thing I'd mention here is on that point building the orchestration layer engineers in the future and from my perspective and from the conference's perspective is uh much more of an orchestrator like we're not in the IC mode that we used to always be in. We're taking a step back. I mean even at a tiny startup and we're orchestrating this like sea of agents. I would not have said this until Opus 4 and cloud code. I really do feel anywhere from three to I mean maybe up to like 20 but like 10x uh you know more productive because and I mean that sounds crazy but you can imagine you you collapse all these you know areas of communication if you have the tools the context the right models you have these feedback loops so the agent can just go and go and go and go without having to come back and ask you and then you you seat it with the right uh you know with a very clearly laid out like uh text stack uh as to like where you want to go with those elements I mean you can move really fast and ship things in, you know, a day or even like hours. So, I think building that orchestration layer, building the ability to do that and thus getting the speed that we're all going to need. Unfortunately, I I think one of the only moes moving forward, at least for now, that's uh persistent is is the speed of execution, at least from what we're seeing just because it's easier to make products. So, that's um meaning that we're needing to adapt and you know, leverage these new tools and get these efficiency gains as much as possible. Uh, so having the orchestration layer is one of the best ways I can think of to to speed that up and to allow that to happen. >> Patrick, are you going to take this video and do something with AI on it to learn about your next presentation? >> I I I will definitely transcribe this and uh there's a lot of thoughts that, you know, takes so long to kind of put together when you're sitting down at your computer writing. Honestly, just talking and especially your guys' questions so helpful to see to uh kind of bring all those ideas together for context for articles or anything. >> I love it. >> That's one of my favorite new workflows. I just it's you know same ideas but just so much easier to to take them out and the questions really give so much depth uh because like where I maybe skipped over something quickly now you've got that context embedded in the the transcript as well >> you have mentioned a wide range of tools and and models how do you make sure the governance of those models the privacy the security of those all things for your product >> to be honest this is probably one of the biggest differences between like our startup and like you know it's like a lot of stuff it's like h like I don't you know I mean obviously user data we're super protective on and like PII we take very seriously but anything outside of that is kind of like um you know again speed is the biggest mo that we see right now I think sticking with the big you know providers like OpenAI anthropic I mean it's a little hard it's like well clearly openai you know ripped through from my perspective a ton of YouTube and like other sources so I don't know how much you know you can fully fully trust them but uh in that case if you go from like a 20 to $40 plan I for enterprise then they've got a lot more guarantees. Um but yeah we just roll with the uh uh you know enthropic and open AI uh Grock uh and a few other models but avoiding of course like deepseek uh would or other models like that would probably be a good way to go or at least pulling one of the like you know uh post-trained versions on hugging face that's got some of the uh that stuff outside of there and then hosting it yourself could be helpful. Some of you mentioned about validation for marketing specific you mentioned like we ask it to do this all all these tasks and then we validate it >> but you didn't repeat that for other uh like other applications. What is your view on like validation in general? Is it practical or is it something that is necessary to do >> in >> with this like that you go fast in switching between models? How how is it critical >> your eval? One of the like hacks that we're able to do is essentially like human in the loop. Uh so for us it's being able to kind of validate a lot of the output at our scale at least uh that comes out. Um of course if anything's built into the app like our image processing pipeline then that's obviously way above what we could manually do. Um so in those cases having evals for uh there's amazing text based evals or or you know code based anything that a traditional LLM would use again with like brain trust. Um and then some like image evals that we've created for example to just make sure that if somebody's got glasses that's represented in their uh you know the image that's like presented. Um but beyond that um there's uh I'm laughing because last year and like this was repeated this year people will talk about uh just like feeling the vibes of these models and like outputs. So like honestly again in a startup context that's a lot of what we're doing. But um I think it's just taking the time to understand the character of the models uh to create eval where we can uh to safeguard and to make sure we're actually measuring the difference between new model updates and you know looking for regressions that kind of stuff. Just like really paying attention to okay what's the 8020 of the 20% of areas we really need to focus on. I I don't see it is possible even like uh to evaluate everything because we don't know the answer that like the user might ask a question and we are blind to their privacy private data and we don't know if our model works well or not. So those are the biggest part of development with AI. One other note I'll just add on that is uh when you're orchestrating agentic workflows, I think there are there are a lot of patterns that you can borrow uh whether you're using like OpenAI's agents SDK or um uh lane chain or any of these other platforms uh to basically add validation steps and to kind of uh have like experts that are fine-tuned for specific outcomes. Um like you know fine-tuning even just one of OpenAI's models is not as hard as it seems. I mean with like a hundred you know pairs of data you can you can get pretty far. uh but having like you know kind of defining these specific um roles and then fine-tuning to that can really help get to you know a high level of nines in terms of predictability for outcomes. So that's another tool I'd mention. I think though expectations around software I I think are are going to change to a certain degree or have already where we just we're working with this more ephemeral uh less predictable uh like thing or agg uh you know in terms of software versus like very cut and dry predictability. So, I think it's going to become more acceptable in some cases. >> Yeah, man. >> Yeah, absolutely. Thank you so much for watching. If you found value in this video, odds are you'll also love this video I just released encapsulating the framework and the playbook that I came back with from the AI Engineer Worlds Fair in San Francisco that just happened. It is by far my favorite conference for all things AI engineering. Also, make sure to like and subscribe to stay in the loop on future videos just like this. And please comment if you have any questions or ideas stemming from this video. Thanks.