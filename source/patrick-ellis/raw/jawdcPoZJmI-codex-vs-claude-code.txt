The brand new releases of Codeex and Cloud Code 2.0 using Sonnet 4.5 have completely overhauled my AI coding workflow. My decision came down to these nine critical differences that I'm about to break down for you right now. In this real world comparison, stress testing both models as I've been building out a massive web application this past week. The results were amazing. In fact, Anthropic is claiming that Sonnet 4.5 is the first model that's been able to build out their claw.ai dashboard end to end. At the end of this video, I will share the exact workflows with these models that I am currently using and seeing amazing results from. If you're new here, I'm Patrick Ellis, the CTO and co-founder of an AI native startup based here in Seattle. We're fortunate to work with companies like Google, Amazon, FIFA, and Disney. And we've been battle testing Cloud Code heavily since February and Codeex off and on since its release back in May. And I've been using these models non-stop since their last release. With that, I hope the biggest lessons from these hard one insights and all my research can help you decide which model is best for you and in which ways. Also, I'm about to catch a flight, so please excuse this more conversational, less edited than normal video. So, the first of the nine critical differences comes down to the model itself. Sonnet 4.5 has beat the benchmarks. It is officially the best when it comes to SweetBench verified in the other metrics that we typically look at. One of the coolest things with Sonnet 4.5 has been its eagerness or its um its grittiness I would almost say where it's really really good at managing its own context and using GP and other bash tools. So you can kind of just set it loose in a codebase and it's really good at figuring out its own plan, grepping around, gathering context. It will even uh create artifacts internally and use heavily different markdown files where it can save its memory state. The other difference is the speed. Sonnet 4.5 is a lot faster than codeex. I originally didn't think this would matter much, but as I was playing around the model, I just realized the iteration and the ability for me to move in a synchronous capacity was a lot greater with Sonnet 4.5. And that that does add up in terms of the amount of output that you're able to to get in a date. Another thing that sonnet 4.5 uses heavily are in creating internal scripts and internal tests and other ways to create validation loops so that it can very quickly identify is it going in the right direction or does it need to be rethinking or redoing its work. I have been working on this big web app which involves a lot of refactoring uh migrating over data models from a legacy Python Django stack to a new uh postrest.js JS Reactbased stack and that has involved actually a lot of different areas to test these models and in that there was a uh kind of a oneshot approach I was trying to make that codeex was struggling a little bit with opus definitely wasn't able to do but throwing sonnet 4.5 in there it was able to just go in figure out a bunch of context and come back with the winning solution whereas opus and codeex were requiring me to take a few more steps to really break found the problem and to break the research steps out into individual pieces. Whereas Sonnet 4.5 was able to just figure out and bring and aggregate all of that together. So if you combine the ability for it to dive deep into these different code bases, gather context, create a plan, create these fast iteration loops in terms of val validating whether an approach is right or not, that creates a really independent and just all-around amazing model to be thrown into most tasks. It's not as good unfortunately as codeex at just sheer front-end uh work. I would say mostly on the UIUX design side. If you just give a generic prompt without much guidance when it comes to style, I I definitely prefer the outputs from the codeex coding model, but where you've got additional context as to the UI output. You're looking for a tool like playright in order to take screenshots and give context to create a a loop of iteration. With Sonnet 4.5, in that case, it does perform very well. I would also say that bigger architectural reasoning and deeper thinking, so like more meaty, more back-end focused tasks, Codex does tend to be a little bit better. Sonnet 4.5 is also, I believe, the only model that is aware of its context window. Cognition actually had a great report on this where the model will understand when it's getting closer to its context limit and document more of what it's doing and spend less time reasoning in order to try to get an answer out before the context window ends. With Sonnet 4.5, at least with what I've been working on recently, I feel like it does an incredibly good job of continuing to uh fill up its context and and summarize it well and just manage context really well overall. I found myself almost not concerned about when it was compacting which allows me to just let this run for much longer and to complete much more meaty tasks. OpenAI was also talking about codecs running on the upper end of around 7 hours on a task where Enthropic mentioned Sonnet 4.5 running on the upper end of 30 plus hours. Another piece with the models is I really feel like it depends on your specific situation especially the codebase that you're working in and also your own preferences. I feel like I'm seeing people's reactions all over the place between codecs and and Sonnet 4.5. I really feel like you just need to experiment with your own tech stack, but for me, I give the win to Sonnet 4.5 easily. All right, so the second big factor here is the actual CLI agent or the harness itself. So I'm talking about Claude Code 2.0 or Codeex confusingly with all the different codeexes. Uh the actual CLI tool here. And here it's pretty slam dunk. I feel like the Cloud Code CLI tool is pretty far superior. There are so many nicities. Uh just way more features with cloud code. For example, this is like a very specific one, but the add directory/comandir that has been so helpful for me to connect multiple of our services together. I can just run that add a directory and allow Sonnet 4.5 to just go crawl figure out and map out this other directory and apply and migrate or integrate services. Another one has been the ability to run slashcontext to understand how big the context window is and what's filling it up. Anthropic has also just released a memory tool and other tools around agentic coding. So they've got the uh just also released claude agent SDK which was just renamed from the claude code SDK but in there they've got examples of amazing slashcomands uh such as security reviews and code reviews. Enthropic is building this whole ecosystem around cloud code. You can build a lot already with what Anthropic has has built. But the one big advantage that Codeex has is it's open source and with the community development surrounding it along with your ability to bring your own model in theory, I know that OpenAI is going to be investing heavily in bringing the quality of codecs up to par. One of the biggest downsides as well with the CLI tool for codecs is the lack of sub aents. That is a really really powerful way to manage context to only expose different MCPs or uh system prompts or other specifics for a given task to a sub agent. Being able to delegate out and not pollute the main context with defining all those tools or the context that's gathered during a task that the sub agent is doing. I would love to see that within codeex, but I'm sure it's coming soon. For the CLI agent, I definitely have to give the win to claude code. All right. Now, for the IDE integration, the Cloud Code integration with VS Code is pretty basic. It's basically just a wrapper on the Cloud Code CLI. As far as I can tell, I don't see anything that's unique to that environment beyond just the GUI that's over the command line capabilities. But the codeex integration with VS Code is actually really robust. And of course, when I say VS Code, any of the VS Code forks such as cursor can use these tools as well. With codeex, not only do you have intelligent ways to inherit context from the IDE, for example, being able to see what tabs are open, what files are open, and have more sophisticated tools for grepping around and grabbing context. But you're also able to hand off local tasks that you're working on to Codex Cloud, bringing all of your session context over with it. And you're also able to pull down things that you're working on in Codex Cloud to the local environment and apply your diff and everything there. That is a killer use case. And they will be eventually bringing that to the CLI, but for now it's just in the IDE extension. And I absolutely love that idea of being able to really quickly go between working on something, delegating it up to the cloud, and then continuing your work. So for the IDE integration, hands down, that's got to go to Codeex. And I love the future vision they've got here. And that brings me to number four, Codeex Cloud. So this is an incredibly exciting and absolutely truly game-changing new paradigm for software engineering. And we're just starting to really get a feel for it. And this is the ability to use the cloud interface to connect with your repo like through GitHub, spin up a container and then create a swarm of agents. So it could be one to four different agents and go out and basically do what you're doing on your local machine but in the cloud. Anthropic sadly does not have any competitor to this. There are some GitHub integrations which I'll mention in a second that Codex also has. Greg and others have talked a lot about this idea of having an abundance mindset. this idea that you can just kick off tasks and explore through codecs. So, for example, let's say I want to work on a migration uh or do a bunch of security updates or migrate a legacy service over to a new architecture. I'm able to just kick off a bunch of these agents to go out and do that research and to come back with actual solutions with real code that you can look through. So in terms of doing research and summarizing different parts of the codebase, exploring different approaches to a problem or being able to actually get working solutions, the codeex cloud is an amazing new paradigm I'm so personally excited for. So I I find myself most excited about in using this most for exploration and learning, but it is also very effective for any easier tasks. So a lot of like smaller front-end changes. Um, I mean, you can get some pretty amazing and meaty results out of this. I just find myself typically wanting to have a little bit more control and to guide the model a little bit more, in which case I think the CLI or IDE integration is a little bit better. But as the models get better, as we have more context and build out these orchestration layers around the models, I think this idea of spawning these cloud agents in a whole swarm to go off figure out solutions and run for, you know, days or weeks or whatever, that is a compelling future. Another thing that Greg talks about is the idea of having the unblocking of your local machine. And this is where the integration with Codex cloud really works well with the local IDE integration for example uh where you can be working on something and just decide to delegate it and free up your local machine in order to work on whatever needs your direct attention. Anthropic doesn't have a cloud environment. So this is a huge win for Codeex. All right, the fifth difference here is the GitHub integration. Both Cloud Code and Codeex have really robust integrations with agents. So being able to atclude or at codeex in your GitHub issue or comment and ask it to implement different changes in addition to having a review process both being able to call and invoke reviews by tagging the agent and having an automatic system where whenever you push changes up or create a PR those get reviewed. My last video was all on different workflows that Enthropic has open source that are amazing in terms of the code review process. And with the push I mentioned earlier that they have around their their SDK and the robustness of the tooling, I definitely prefer a lot of the integration with cloud code. I feel like it's a lot more extensible to really build out robust workflows with MCPs and everything else, but I love what Codeex is doing and clearly they're investing heavily in this as well. I also will say I prefer the outof-the-box reviewer that Codex has. It's really good at finding nuanced bugs. Credit mostly to the Codeex model. And it doesn't pollute the comment or your thread with a bunch of extra uh fluff. It really just tells you the issues that need to be resolved. I will say Claude's security review agent, the slash command and the actual agent uh that I showed in the last video does a great job of being succinct as well. So in this case, I would give a slight edge to Claude Code for the GitHub integration, but honestly, they're both great and I'm absolutely loving the ability to go in and make extensive agents that run right in GitHub. The sixth big difference is the vision and future roadmap and kind of the way that the two companies Anthropic and OpenAI are approaching these coding agents. And surprisingly, it is a pretty big difference. OpenAI is much more AGIP. They've talked about that being the reason they've gone straight for Codex Cloud and really trying to build towards this goal they actually have for the end of the year, which is to build a fully autonomous software development agent that can run asynchronously in like a cloud type environment. Whereas Anthropic is very heavily focused on getting real world feedback from their users. OpenAI is really pushing for the endto-end solution where you are delegating a bunch of your tasks to this kind of end state for engineering and this idea of abundance of kick off a bunch of these agents and let the a massive amount of intelligence help inform our decision-m versus us relying on our own mindset. So something that you would imagine seen in like an AGI type environment. Whereas the cloud code team is much more tactical and concrete and focused on simplistic uh robust very industrious solutions that work really well right now. And that's where claude code especially in its agentic harness is a very versatile very simple approach. So instead of using embeddings and advanced like ragesque searches it's very simplistic. It's using GP. It's using files like markdown files and it's just searching for and uh uh you know using the tools that are already there and like the CLI type environment. For me personally, Anthropics approach to this seems to help me a lot more on the day-to-day. Although I do absolutely love the vision that Codeex has and that OpenAI has. I just feel like it's a little bit further out and it's harder with the limitations that the models currently have to fully buy into this vision of just delegating a bunch of work out to codeex. But if I had to give a ranking, this one's kind of a tie. I would say right now the anthropic vision is much more helpful, but I'm so excited for the OpenAI vision. All right, number seven. I just want to give a couple pro tips in terms of how to really get the most out of these models. One is with codecs. There is a codeexconfig.tomo file and in here on when you're using codec cli you want to make sure to enable if you're okay with the security risks of this access to uh the web search as that really helps in terms of finding documentation and enabling more um agentic style running so that the model can run for longer. Another tip is to really audit your context to make sure you're not poisoning the context with conflicting information or you know an MCP can have like 25 plus different tools. It really depends on the MCP and each of those has context and a bunch of uh you know tool names and descriptions and all of these things that can really not just bloat the context but also confuse the model as to how to go about a task or which tool to use. So, it's really, really important to make sure that you're only exposing the tools that are needed in a given time. And this is one reason why I love sub agents as a way to manage context. All right, a bonus number eight before I get to my personal workflow that I'm using now is to use deep research and Gemini's deep think to help you process through and create uh PRDS context for the models to create sub aents and to really help document and kind of go out and gather intelligence for any specific task that you're working on. One amazing example is using the deep research tools within any of the platforms to go out and gain expertise on any specific workflow that you're not an expert in. So for example, maybe you want to get deep into SEO. Uh you can do a deep research report on the best practices around SEO for like a worldclass SEO strategist and then have any of the models. I love Gemini Deep Think for this, but any of the models summarize that into a very actionable checklist or system promptes format that you can then turn into a sub agent or you could turn into a part of your claw.md file or just like a PRD or a task that you uh uh use as a prompt in order to get the model to implement that workflow. But you can do this for, you know, front-end design, architecture, data modeling, uh QA and testing, any workflow that you think of. It's really easy to go out and to gain like a really high standard base of the collective knowledge of how to do that task well and then to bring that and package that up into a way that your model can basically on your behalf learn and then go out and execute. And it's amazing too to help more junior people on the team quickly get up to speed in different workflows. All right, so number nine, what model am I using and what best workflow have I found? I was using Opus. what 4.1 very very heavily up until the codec release and then I switched primarily over to codecs. I found that the medium and the high uh levels to be uh pretty phenomenal and a lot better for most of my workflows than Opus 4.1. But of course roughly a week after that when sonnet 4.5 dropped I have switched and I am fully over in that camp right now for the majority of my programming. Kind of concluding everything I've talked about here. I will use codecs in the cloud to go out and do a bunch of research exploration, generating some basic scaffolding and building a skeleton app to really get a sense of what the tasks and projects are going to look like. I also use it to go out and collect a bunch of information that I can then feed into PRDS for Sonnet to implement. And then I'm using Sonnet 4.5 exclusively in terms of anthropic models to go through and implement a bunch of these changes. But I will have Claude code review and do security audits on anything that is generated by Codeex and then I'll also have Codex review and audit anything that's generated by claude code. You don't necessarily have to do this, but I feel like the slightly different perspective the models have is very helpful there. I find that Sonnet 4.5 is very much of like a uh startup entrepreneur, somebody that's learned by doing a lot of testing and iterating and very tactical, very hands-on, very kind of in the weeds and figuring things out and not afraid to get its hands dirty. where codeex is more of a uh academic like a little bit more of a being able to think and kind of I' I've heard people describe this as uh you know measure twice cut once where sonet 4.5 is more of a you know cut twice measure once potentially in some situations to quote somebody on X I find that that characteristic is actually what I want a lot of the times with engineering like I want sonnet to go out and try a bunch of things and figure its way out but then for analysis I want codeex to really be able to think strategically and systematically and with a lot of depth and to spin up a bunch of them through the cloud to go out and explore these ideas. So, I find that that workflow, that pairing of the tactical buddy by my side that I can steer and make sure that I'm using my experience as a CTO and uh an engineer to guide it, paired with the asynchronous and uh kind of AGI pill future of gathering context and information of course delegating work as I as I want as I can is an amazing pairing right now. So, I am just beyond excited for this workflow and what I've talked about before with the orchestration framework and being able to build and surround these models with the context and the tools and the examples, the validators of of what success looks like is even more important as we enter these worlds where these agents are running for longer and longer and can do more and more work on their own. They need and they're hungry for and they're good at understanding that context. We just need to create the environments where they can truly thrive. And with that, I hope you found a ton of value in this video. And thank you so much for watching. It really does help if you subscribe if you're interested in seeing the loop for future videos. And if you like this video, you'd probably really like one of these two videos. This focus on anthropics open-source agentic workflows and how to turn cloud code into an amazing designer using Playright. Thanks for watching.