# Supabase Cursor 10X Database Workflow AI

Published: 2025-03-27

Over the past decade, I've spent a ton of my time wrestling with production databases. But over the past few months, I built up this AI powered approach that reduces that down to just minutes to a point where you rarely ever have to touch the raw database underneath. Let me show you how I achieved this directly using AI coding agents and superbase. First things first, let's align on the core principles inside progress. So there's schemas, tables and columns. So you know the core premise is extremely simple. In this video we won't dive into the technical details of Postgress. So I'm assuming at least basic knowledge from everyone who's watching. But if you have any specific questions about Postgress, feel free to throw them down in the comment below. I'll be happy to feel the further questions. So we'll dive straight in. Superbase is this amazing rapper outside of Postgress. to say it's a rapper is probably underplaying its importance. So it has this crazy layer of utilities around Postgress to make managing and using Postgress extremely easy so that you never really have to think about the database underneath it. One of the most amazing things they provide is this local command line tool that fully matches I'd say probably up to 99% of the features that they provide in prod which makes local testing extremely straightforward because you have a easy way to reproduce the production environment and then so you can do all sorts of testing as if you were in prod without having to worry about messing up your actual production deployment there. So I've actually first found out about Superbase only half a year ago when a friend actually mentioned to me and for the longest time I thought it was spelled as superbase not super power base. So for us the first thing that we're going to do we're going to dive into the command line to show you how you can get everything started. The first thing that you want to do with Superbase is you can follow their guide for the login credentials. We won't show the details here, but you can start the Superbase local deployment with Superbase start. What this does is it will start up all of the Superbase functionalities directly on your local machine using Docker. And you can see it printed out all of the API endpoints that you can pass directly into your application. And the most important one here to note is just a DB URL and that you can use directly to connect into Postgress. And if you're using the Superbase SDKs, you can use the Anon key, the service ro key, and a whole bunch of other configurations to connect directly to it. The thing that I'd strongly recommend you do is use the local Superbase deployment wherever you can because well, you can just do whatever you want. you can easily mess up. You can do testing. You can mess around with data without ever having to worry about messing up your production development. And there's probably too many videos out there that shows you, oh yeah, look at this. You can do this amazing thing directly, manipulating the Superbase database directly with your code and oh, it's updating the UI live. Don't do that in prod. God, please do not do that in prod. is fine in prototyping, but when you have real customers, make sure you separate your local environment and your production that allows you to do thorough testing as well as a whole bunch of tinkering to make sure your feature works before pushing it to prod. You can also open up your local superbase deployment extremely easily by just typing in the default port. And as you saw, we had here in the link in terms of the dashboard. There you have it, the full Superbase tools all deployed locally directly on your computer. You'll see that almost all of the functionalities and even the UX and color matches exactly like the Superbase cloud environment that makes it extremely easy for you to navigate through this and easily know your way around this whole thing. Most of you that have dealt with SQL in the past, you'll know that your project evolves over time and the structure of your data will also need to change over time. One of the most painful things previously with all SQLs is that you have to manually manage all of the migrations to say if you add a simple column to your users table, you want to add a notes column to record more information about users, you have to manually create the SQL and then deal with the migration of your production databases. Managing the versions for those data is extremely painful and if not done well, applying just a simple database change or sometimes accidentally applying a deletion change can cause the whole production deployment to go down. So managing migrations is the key to making great Postgress or just generally SQL workflows. Previously, I'd almost always wanted to start projects with a NoSQL database just so that I don't have to think about database structures. But now with all of the superbase tooling, I'm more than happy enough to start prototyping even with SQLs without having to worry about the complexities of dealing with migrations because most of that is actually very well hidden away inside the CLI tools and the SDK. On top of this, we can use AI and especially the AI agents built into amazing code editors like windsurf and cursor to directly call out to superbase CLIs to utilize those amazing tooling for us. So, we have to worry about even less of that hassle of dealing with SQL databases. I'll show you exactly how in just a moment. As mentioned earlier in the video, our core goal today is make sure that we can benefit from all of the amazing features of Postgress but without having to deal with any of the complexities. So the first thing that we got to do is when dealing with the migrations, we want AI to actually write the migrations for us, right? So you can do that fairly easily with Superbase. They actually give you a super simple command to run to create a migration file, but we don't even have to do that. We'll let AI handle all of this for us. So here you can see that inside the cursor rules file and I know for mine I've kept it in the rules.mmd file. So this file is currently sim linked into the winer rules file and also the cursor rules file as well as the copilot instructions and the client rules files so that I can use the same set of rules across all of the different code editors I use. And yes, I know you're probably thinking, why the hell are you using all of this AI editor tools all at the same time? Can you just pick one and stick with it? I personally quite like each one of them for specific set of features. So I actually constantly switch between the editors to use them for specific feature implementations and hence why I actually try to use the same set of rules across all of them and also in general I like experimenting with all of the new features that's getting released. This AI code editor war is just heating up. Everyone is racing to release new features all the time. I want to make sure that I'm not getting overly comfortable with a specific editor so that I can learn and try out all of the new features from the different ones as they get released. But going back into the video first, you can see that I tell the AI that I use Superbase for database queries and the schema and for all of database related tasks and ask it to read my existing data types file. I'll talk about why I use this types file and how you can generate it later to understand the current data schema and then making sure that it uses the CLI. It calls out to the CLI tool directly to generate the migration. After it generates the migration, it can apply the migration directly using the CLI and then it can generate the types and especially if you're using TypeScript, this is extremely convenient. You get all of the code editors autocomplete directly with this type generation. And also with most AI code editors doing auto lint fixing right now, the database types makes it much easier for the code editors to directly or quickly catch the errors in the generators code without having to wait for that runtime error when executing database queries. But we won't go into the details there because I'll dive into it a bit later in the video. And then when creating a new table, it must have the created data, updated ads, which is a fairly common standard for SQL, and make the AI actually deal with all of the trigger editing and function adding during the time of table creation. And this is another like a security feature, enable the superbase, the role level security, which we'll talk about later. and a bit on the specific SDK usage directly in code. Let's create a simple feature that leverages all of these rules to create something new in our product. Let's say in my admin page, let's add a notes column such that I can add custom nonuser facing role data for customer tracking and as always select clause 3.7 and we go. So obviously here it's first reading the code to understand the current structure and you can see there based on my instructions it's looking at the database schema to understand where things needs to be added and then first it knows oh it needs to add a nodes column creates a migration directly using the superbase command then it writes the migration TLS credentials and then it will know to apply the migrations and also generate the types at the same time. Actually, not sure why it errored out. Oh, it's trying to create the view when the view already exists and local database is up to date. Okay, so that used to create the view that used to exist and then now it knows to replace the view first. Okay, wonder what that issue was at the end. So it turns out and it didn't know the view had already existed. So it tried to recreate without replacing the view that already existed. So it's gone in to fix the issue itself and then now after doing all of the database migrations do the code modifications that adds in this feature specifically. So what's really cool here and we'll just let it complete over there. If we look at the migration file to alter the table to add in the new notes column and here we can also manually change any of the migration that we didn't like. If you want to be extremely vigorous, you can set it in your rules file to make sure it asks you to review the migration file before applying any one of them. I turned it off for now because in most of the time I found it to generally do the right thing and because this is running against a local deployment. So I'm not worried about it messing up data. So I'd much rather if it just continues with the flow without having to interrupt me, do the whole work and then for me to come back and review the data as a whole. Obviously in production use cases you always want to be careful with any keywords that includes drop right because that could be removing data but because here we are just updating a database view so that doesn't really matter too much here and then so we'll just let it be as is and because my admin dashboard view is actually fairly complex so don't worry about the actual code here but what's important is that a removes a and then updates it with the new notes column that we've just added. And you can also see the database types is also generated. So this is actually the most powerful feature I see for Superbase is especially for TypeScript is to generate those type mappings automatically and just makes it so much easier for you to capture errors for AI generated code at compile time rather than at runtime because there are so many things that might seem fine at compile time. you generate code, you run it once for the specific case, things worked out fine, but then there are other code passes that you haven't tested. And because there wasn't any strip type checking, things would just fail and then that is just so much more painful to fix while in production. So I say the generate types part definitely use it as much as you can. And now you can see that it's also added our UX features to update the nose. As mentioned, I won't go into the details for the testing here, but it's nice to see that it actually done the change and then it's also correctly done the database change first to allow the feature to be implemented before doing the rest of the work. Through this whole workflow, you'll realize that because everything is actually versioned inside code, it didn't even need to read the database to actually implement this feature correctly. This is the most powerful thing about this whole workflow together with the migrations and using types and using all of the rules is to make sure that everything is directly in code so that you never have to worry about your database going out of sync or you don't know how to recreate your database from scratch. This is a part that I'd really recommend you be extremely vigorous about is to make sure you have everything directly versioned as code so that if anything goes wrong, you can easily go back to the code to recreate the database from scratch should you ever need to. Hopefully, you never need to, but it's great to know that the feature is there. So what if you wanted to directly update doing a small change in the UI instead and then still benefits directly from the migration file and have everything versioned in code. You can also do that. So let's say in our example we add in for the payments table a notes column with a type text. We add the column. You can see that is successfully created. And if we just hide the notification the notes column here and to make sure everything could still be in sync with the file. Superbase also provides a very convenient command line. And you can call superbase db diff and then we also output this into a file. Let's say new payment notes. So this is the file title. It will apply all of the existing migrations directly from scratch, creating a shadow copy of the database, compare that schema with the current local one that we have, then generate the diff directly from the different data schema snapshots that it sees. Super convenient. And it will generate a new schema file directly. And you can see here inside the new payments notes, it's worked out that inside the payments table to add the column notes and of type text. How great is that? In most cases, this is how you can make sure that you never have any UI modifications that is not already in code. But even in the case where you have, you can use this to generate it very easily. But I would also say avoid this if you can. You never know what detail is missed where it's directly editable in UI, but somehow it's not reflected in the migration for simple changes like this. Obviously, you could have also gone in to write the migration yourself, but if you have done something a bit more complicated, you might want to use a generation. But in general, it's nice to have it there so that you can check if there are any known differences between the schemas and generate the migrations if needed. Most of the time, just start with code and stay with the code. Another side note is that this command also works if you want to diff your local database against your remote one. Let's say that maybe you have manually somehow applied some patches directly in production for a specific fix that you want to directly pull into your local database first of all to generate the migration to keep your codebase in sync. You can also use this to directly generate the migration. So you can then use this to generate the migration without having to write it manually. That was a lot said about migrations. I cannot hammer this enough. When using SQL databases, managing migrations is one of the most important or rather the most important thing that you can do. If you can do this well, you know for sure that your database experience is going to be great because all of the infra issues is already handled for you by the underlying database providers be it Superbase, AWS, GCP or whatever, right? And then so you need to focus just on the logic of how you use a database and apply the best practices. Now that you know how to deal with migrations, how can we further integrate this database workflow to make our life easier during development. So there's a few other things that you can still do on top of what we just talked about. The first thing that you want to add is the Postgress MCP. I know many of you might be surprised to ask hey like why doesn't superbase provide their own MCP and why use the Postgress one Postgress is the main thing that lies underneath a superbase even on their official documentation they recommend you use the Postgress MCP directly for integrating with a superbase databases will just be using the Postgress MCP directly I'm sure you've already seen way too many video that tells you how to configure this thing. So I won't dive into the details there. For me inside my projects, I just have a simple configuration that just has the Postgres MCP. You'll see that I don't have any other MCP configured. I haven't experimented with a few, but the Postgress one is the main one that I found extremely useful. Haven't touched on too many others just yet. With the Postgres one, it's fairly simple that you specify the connection URL here. Obviously, because this is a local database, you can I'm happy to just commit the code because here there's no sensitive data or nor in the one that I've just shared with you below because that's all credentials for my local superbase deployment. There's nothing you can connect to even if you got the keys because it's just running on my local machine. You definitely want to be careful with putting production credentials directly in here. MCP files have a built-in way to load environment variables so that you can pass in production credentials a bit safer than to hardcode it directly in the file. If you want me to dive a bit deeper into MCPS, please let me know in the comment below because I'd also love to experiment more with MCPS, I'd love to do it as an excuse for making another video. What the cool thing about configuring this is that when we have everything already versioned and committed in code all of the migrations database types so that our AI models can directly infer the data that's currently stored in our databases without having to do any live queries but during debugging sessions having direct access to the database extremely useful to help it find out maybe the issues in data discrepancy Maybe there was a data intricacy that you missed during debugging that was causing weird UI issues. That's why you should really give it access to the Postgress MCP there to allow it to access that. And just to show you very simply and we can say what columns are in the payments table here because it knows that I'm asking about Postgress. it will generate the MCP tool call directly to query the database about that data. You can see it even showed our newly created notes column directly there. Now we can even ask questions like does our code allow modifications of the notes column because it has access to the database. A knows the things to check and we know that it should not find any access to that column there because we didn't add any functionalities. And we'll wait for it to finish to see how well it does. And as I expected at the end, it found the fact that we've added the notes column directly in the migration that we've also just generated, but it's not finding any specific usages there. and it knows that the nodes for user records for TLS credentials table but not for the payment nodes. Isn't that great? Next thing inside of that workflow is the generated types. And just to show you what's currently in the generated types file, you can see it has all of the table schemas. So it generates a lot of the different types for the superbase SDK to give you that autocomplete for the different column names whether it's when you're doing queries or updates extremely convenient. I say because of the complexity of TypeScript types from time to time. This file is not exactly readable by humans, but it's perfectly fine for a claw 37 to analyze the file to understand really what's going on there. What's great here is that this file is always generated so that you never really have to touch it manually. As mentioned, the core goal of this file is to make it easy for us to capture the database type errors at compile time so that we don't have to wait until runtime to fix and catch those issues. Another part of my dev workflow is enforcing the created at and updated at columns directly in the migrations through AI as well. Um you'll remember that inside my rules file I had this line where when creating new tables it must have the created add and updated values and those things should be set by these functions. These functions were something that I created in a earlier migration file and we can go back to the functions and let's see if I can find the very original one where it's created. Yes, here was the original function that I created. Extremely simple. That's inside the updated that it triggers when a insert happens specifically on a given table and then it will update the it will set the new rows created at and updated at column directly to the current time. So you don't have to deal with this manually inside your code or the SDK. You can let the database directly handle this. So you know that everything is properly enforced. So this is another migrations file. You can see that it automatically added the trigger to say for the send veils table. It's updates the created at and then also before update it runs the handle updated at such that the columns are updated accordingly without any manual changes. This is also really nice where you can have all of these nice functionalities and never have to think about writing the main manually. Nor do you even need to implement this in code because this is done completely at the database level. Now you have a fully functional database with migrations and convenience functions already all generated and run for you. The next thing that you're going to run into straight away is C data. Now that you can recreate your database from scratch, how do you make sure it actually has data that you can use? Again, Superbase provides a very convenient functionality for this. When you first create the database, they have this file inside Superbase directory called C.SQL. This is the file it runs after it runs all of the migrations to create the basic table schemas. The general recommendation about migration is that you don't put any data inside the migration files. The migration should just be about your data structure changes but never about the data changes themselves. Seed.sql file is where you put the data that you insert into the table after you do the migration. But most of the times if you were to write this file manually it would be extremely painful because as your project is involved you probably have a tons of startup data that you need to keep in the database. In superbase you can also generate this very easily. Let's say you've already populated your database either directly in the UI or through using your implemented UI and you have you register the user data you have admin data and a whole bunch of interaction data. You can use the superbase command mpx superbase db dump dash. So we won't want data only because the schema changes has already been applied as migrations. And then we save it as a file. Let's just write it at the root for our cdsql. And I managed to run that with the pro database. We want to do it just with the local one. Now you have the CDSQL with all of the data that you currently have in your database dumped locally. Obviously, if you wanted to use your prod data to seed this, you can run this command without the - local command to get the pro data, but obviously they have far more sensitive things inside your pro database. You probably don't want to do that. Which is why I'd recommend when you're just starting out with the prototyping, do this with a local project generated and so that you keep your seeds SQL with a whole bunch of fake data should you need to and then use that for testing. Now that we have the seed data, how do you actually use it locally? The thing that you can run again superbase command that they've really done really well for, you can run db reset. What this does is it clears all of the current database data, resets it to a clean slate, applies the migration file one by one, and then runs your C.SQL files to populate the database. This way you know that after resetting, you always get the exact same database every single time. So you can play with data, deal with the local experimenting, but you know that should you ever want to go back to the vanilla database state, you can run this reset file. This thing is also extremely powerful when you're running tests. So that inside the CI, you know that you're always starting with the same database environment for your test. If anything goes wrong, ah you know that it's definitely not the database state that's actually causing the issue. Superbase actually provides also really good docs on how to leverage the Superbase CLI directly inside of CI these things like GitHub actions but I won't talk about that in detail in this video and also locally when you're testing if you have multiple team members or maybe you have different branches that you're running and that might be running and writing different data it's useful to just run this every time you're switching branch or maybe your teammate is actually working on the same project to know that you always have the exact same database state when working on it. So that's a great feature to have. Another thing I'd recommend you to try is just reset your database every now and then. This really makes sure that you keep everything inside code because if you actually did maybe even accidentally did any UI modifications just by running this reset would cause things to break. But if you were vigorous about keeping everything versioned inside code, running reset should never really cause any issues for you and your test to do still pass. Your end to end test should still run. Your UI should still function correctly. So it's a great way to test that you know exactly what's currently stored in the database. Oh wow, man. I'd originally planned for this video just to be less than 20 minutes long, but even just going through what I consider the basic functionalities of Superbase, I think it's already going well over 30 minutes for this video. Bear with me. There's a few more goodies to come. Database branching is another convenient function that Superbase actually provides. What it essentially does, it leverages the migration file that you currently have committed in code. When you create new migrations on new branches, say if you created a new pull request to add a new feature by leveraging a new column that you created to the database, it can use that production database and create a new database snapshot for you to actually run that test. It will use the C.SQL SQL to populate that database together with all of the migration that you currently have written so that even with every single one of your pull requests that relies on database changes, it can be directly run with the database itself. And also it means that any migration file you commit to your main branch, these automatically apply to your production database as well. so that you never have to worry about pushing and dealing with migration manually yourself. Otherwise, after you've actually done the migration to push the migration to your production database, you'll need to run this command. So, superbase db push to put all of the migration directly in your prod database. Another thing you have to be really careful with is also something I enforce in the rules file is that never make any backwards breaking changes. So this is a common rookie mistake and which I've made quite a few times in my early days with Postgress is when you change databases, you might just say, "Oh, I don't need this column anymore." And then you don't realize there's still some of your application logic that relies on reading this column. It might not be leveraging it, but it's trying to redirectly from the database. And after you call a drop, the code just starts breaking because it can't execute the queries anymore. So even when making those migrations, always be mindful that you are creating things in a backwards compatible way. It doesn't mean that you cannot do column removals. You still can. It just means that you have to really plan out the time to say remove the application logic that relies on the column first. Then after a while, you know that you will never roll back to the state where you still have the application code that uses that column. remove the column and then you can move forward with the rest of your database changes inside the rules file. You'd always want that specific line of do not make any backwards incompatible change together with the rest of your database rules to enforce AI to not do that stupid thing that's you shouldn't be doing either. The next important thing that we have to talk about data security. For those of you who have dealt with security, well really that's anyone who's ever dealt with customer data, you'll know that it's extremely dangerous and well bad when you have bad permission controls that allows one customer to accidentally access data from another customer. The Superbase has another convenient function for role level security that allows you to set specific role level rules to match customers to say, oh, if the user who's authenticated with this use ID, they can only access data that has their user ID attached to it. So, you can write those rules very easily as well. And if you remember in my rules, I originally had this line where for any newly created table, always make sure to enable role level security for newly created tables. So this just makes sure that AI knows to apply role level security directly on newly created tables without us having to manually do it. Again, this is one of the things that you can manually toggle on in the UI, but I'd really recommend always having this encode just so that when you create new production tables, you don't accidentally have something that's open to the public to access. So with that said, here's an example of AI actually creating that role level security and also adding the policies that restricts access to specific user ids. Here we can say users can only access their own credentials, right? And then obviously they can insert their own, they can update their own. But when you turn on role level security, it means that by default, no one has access to anything. And only if you specify policies, it will selectively expose things to users specifically. And also these RLS will restrict specific actions. If you just want people to insert but not see, you can do that. If you just want them to update but not create new ones, you can also do that. There's a lot of specific things that you can add here. Again, AI can actually deal with the generation of this really well. So, you don't have to worry about this too much. But it's good to be aware of the syntax so that whenever it generates new tables, you can always go and easily verify that it's got these things in correctly. There's a few caveats about RLS where things will just fail silently for a user that's trying to access data because RLS is specifically designed in way that users cannot use queries to determine if the datas exist or not. So if a user queries for something they don't have access to, they don't get an error. They will get an error for inserts and updates, but otherwise reads will just fail very silently because they don't have the access to the data. you won't get the standard 401 or 403 HTTP codes. So that's something that you have to watch out for. After all of that explanation, we've only just covered the database portion of a superbasis functionalities. We haven't even gone into how to use Superbase to deal with well user authentication and authorization. There's also a ton of features that's actually built into Superbase for doing this. Originally, I wanted to cover this briefly in this video, but now it's looking like it probably deserves another whole video. So, let me know in the comments below if you would be interested in that. Now that you've done all of the changes, generated your migrations by using AI, applied the right level of security, and then you've validated the access, and also you've run your tests to make sure that the database behaviors are consistent. you're happy with the data. The final thing you've got to do push all of the migrations and database changes directly into prod. Right? Again, super simple command that you can run superbase db push. What this will do is take all of your migrations and then just takes a migration diffs and then runs it against your production database that you've linked directly to the superbase project. And then after that you're done. So really here you can see once you've done the AI setup you probably don't need to touch any SQL during your feature coding sessions other than for maybe the occasional queries that's a bit complex or maybe just in the case where you need to check some specific database changes. But most of the time this is actually very well hidden away and is shown in our previous example in the feature implementation. It just went in to generate the migration, deal with the databases, deal with the types by itself without us having to do any of that manually. Isn't that great? The cherry on top of all of this is that you get access to all of these features on the free tier. And I'd say for any products that are just serving maybe even hundreds of customers but not at high throughput, you can probably survive on the free tiers of Superbase. For my project now after 3 month of usage, I've upgraded to the paid version mostly for the data backup. You get 7-day data retention, but otherwise I can probably still sit happily on the free tier. You get the amazing set of functionalities without having to deal with any of the complexities of managing or orchestrating your own Postgress. How great is that? Plus, there's so many other features I haven't got a chance to cover in this video. And I definitely think the Superbase team is cooking really hard and doing a stunning job to make databases extremely simple and invisible to most users so that you can enjoy all of the great functionalities without having to worry about any of the complexities. If you enjoy this video, I'm sure you'll also like my other videos on AI editor usage just up there. Until then, happy shipping and I'll see you in the next one.