Anthropic just killed the manual code review. The engineers building Claude Code, which itself is nearly 95% written by Claude Code, no longer review most of their changes line by line. They've replaced the human code reviewers with AI agents. And they've just open sourced their tooling for Cloud Code, so you can do it, too. If you're drowning in a sea of AI generated code, terrified that some security vulnerability is going to slip through and land you on the front page of hacker news, or you're waiting forever in PRJL for your team to get back to you with their review, this workflow will unlock the biggest blocker that most AI native engineers are experiencing right now and will bring with it massive velocity. It sounds radical, honestly, maybe even reckless, but listen to exactly how Anthropic CPO just described this shift. The team that works in the most futuristic way is the Cloud Code team, cuz they're using Cloud Code to build Cloud Code in a very self-improving kind of way. Early on in that project, they would do very line by line pull request reviews, you know, in the way that you would for any other, you know, project. And they've just realized like Claude is generally right and it's producing, you know, pull requests that are probably larger than most people are going to be able to review. So, can you use a different claw to review it and then do the human almost like acceptance testing more than trying to like review line by line? >> I heard you describe this as you guys are patient zero for this way of working. >> I honestly could not believe that clip when I first watched it and I've sent that to a ton of people. If Anthropic, the foundation lab known for being the most security focused, trust cloud code to handle the majority of their reviews, then why are you and I stuck spending so many hours manually pouring over diffs? In this video, I'm breaking down the agentic review system that Anthropic just opensourced. I will show you step by step how I've implemented their playbook using slash commands, custom sub aents, and the centerpiece fully automated GitHub action runners doing code review and security audits. By the way, I'm Patrick, the CTO and co-founder of an Seattlebased AI native startup that has had the privilege of working with companies like Google, Microsoft, Coca-Cola, Nike, and Disney. and we've been battle testing cloud code since the day of its release back in February. So with that, I'm excited to show you what's brought a ton of value to our team. Let's dive in. All right, so kicking things off here, I'm going to show you the best of the highle frameworks and principles and mental models that I've come across. Also, please feel free to jump around the timeline with the different chapters or look at the timestamps in the description. The biggest problem that we're experiencing right now is this shift in terms of where the bottleneck is in the software development life cycle or in our development process. So think of an original software development flow before these genai agents. You've got an engineer or team of engineers developing code. You've got a code review process. Even if this is just your own solo development process, you'll iterate. You'll go back and forth. And then when everything's looking great, you merge it in. Well, introduce these AI agents such as cloud code. All of a sudden, the amount of code that we're producing has gone up a ton. That's great, but that means that the code review process also has to scale up. And unfortunately, too, with these codegen agents, we actually need to be more disciplined and more diligent in order to catch different hallucinations that could be pretty subtle when it comes to security issues or not matching the patterns in our codebase or anything else. You hear people say a lot that Claude Code or these other agents don't scale or they don't do well with bigger code bases. What I've seen is that it's not the actual fundamental models problem. It's the environment that's messy that they're putting them in. And that brings us to the Anthropic AI code agent review flow. And this shows exactly what Enthropic just open sourced and where that fits into the life cycle. So you'll see here we have this same setup, but we have this addition of a broken up code review step where we first run all those automated reviewers and then we just have people doing what they do best. So with the AI code review process from Enthropic, we've got different models deployed when it comes to pattern matching, fast analysis, and consistent analysis. These are three areas where these models really excel over people. The tasks that we get them are security scanning, bug detection, syntax, completeness, style guide, adherence, a lot of the kind of blocking and tackling of what we're looking for in a code review. So think of like a llinter but on steroids. The nice thing with this is it scales to the volume of these large PRs and it can check against basic acceptance criteria. So what is it that we put in our initial issue or spec? So I can check and make sure are the UI mocks that were handed over are the business requirements is all of that shaped up in order to get this merged in. And then on the other side of things we have the human review process. So this is the highlevel strategic thinking. So thinking through what is the core business problem that we're trying to solve here. Zooming out. What do we actually want to accomplish with this PR being merged in? What is the acceptance criteria? I.e. what are we looking for when it comes to architecture, style guides, UI, and UX. Like this is actually fit the look and feel and we're freed up to look at these things and spend more time on that because a lot of the blocking tackling has already happened by the time that we're at this point in the review thanks to cloud code. In addition, I found that the architectural alignment being able to just take a few more cycles to think critically about where this is fitting into our codebase is extremely helpful. And then one really cool benefit of using a process like this is the amount of learning that we get by actually reading through what the code agent was iterating back and forth with and the review process. And that could happen either while we're developing in the back and forth or when we're at the acceptance side overlooking what the AI code review found. A really helpful framework I found is thinking about things through an inner loop and an outer loop. The inner lloop is where we are or our team is working with cloud code and iterating on the pull request that we're about to create. And this is all the quick feedback iterative working with slash commands and working with sub aents in order to shape up our PRs to as great of a state as possible. But now we have the addition of this super powerful intelligence that we can fit wherever we want into that flow and also learn as we have this pair programmer that's working with us. And then on the outer loop side of things, we are introducing this new pipeline where we've got GitHub action runners that are going through and again doing that first round for us. And this outer loop section is where we use the claude code headless mode within GitHub actions. One of the biggest mental shifts that I've made that has been so helpful when building out any sort of claude code agentic workflow. How can we get clawed code to be able to run for much longer? Because the more iteration cycles, the more times it can go out and use a tool or reference context about your codebase, the more they can run without us needing to be involved and have these longer agentic processes that scale. And in order to pull that off, this idea of the orchestration layer is critical. This is where you've got clawed code or any agent, but what is the environment that they live in? What is the the context that they have surrounding them? I bucketed this into three main areas and this is inspired by Sean Grove from OpenAI, a talk he gave at the Engineer Worlds Fair, but we've got context. So, think of that as prompts, as docs, as the codebase itself, anything in your cloud. MD file in any other context about what what is it trying to actually accomplish? What are the business objectives? Architecture of your entire tech stack documents of the different services it has to work with and then ultimately what is the task at hand. Does it have enough context in the issue or the ticket that you gave it in order to be successful. Think of it like going into a new team. You are only as good as what you're told and what you're able to search around and find out. You can't expect a person or an AI agent to be a competent senior level engineer if they don't have all that context. Additionally, they need tools in order to go out and search the codebase, do web searches, use MCPs like playwright in order to get additional context like screenshots and to just enable their thinking to be at a much higher level. And then the third step is the idea of validators. So, this is critical to allowing these AI agents to understand what they're actually trying to accomplish and how close they are to accomplishing that. I gave the example of the screenshot, which is a really clear version of that. What does the current website look like? What does the the finished pixel perfect mock that I'm trying to get to look like and allowing the model to have access to both. But this can also look like architecture guidelines, plain code styles, any like syntax or other definitions that we have in our codebase. One of the most powerful versions of this are just stating a couple examples of good output and bad output. So, as you'll see in the sub agents, the slash commands, and the GitHub action workers, I'll try to include a few examples in each of those to really make it clear to the agent what it's looking for when it's doing code review. So, if you have the context, tools, and validators, you'll be in an excellent spot. All right, I'm really excited to show you these high leverage mental frameworks that have been very very hard one insights as I've been exploring and just building a bunch within these AI agents over the last number of months. So the first thing we have here is the engineering process encapsulation. The idea here is taking your unique styles, architecture, and other principles that you're employing in your codebase and being able to package those up into sub aents or slash commands, cloud.md configuration or the GitHub action runners, which is one of my favorite ways to do this because it's totally automated or even within MCPs that you create and you take the style, the architectural principles, the expertise around, you know, security or UIUX design or whatever the case might be, SEO, and you're able to put all that knowledge into this little markdown file and send that to anybody on your team that wants to benefit from that knowledge. I don't have to be an expert in all of these disciplines. I just need to have the markdown files from my team members who are that can allow my agent to automatically incorporate their knowledge in a systematic and automatic way. One of the super super cool bits with this as well is you don't need to have all that knowledge and expertise in house. Let's say that you're a solo developer and you're really good at front end, but you're less great at architecture or SEO principles or UIUX design. There are some resources which I'll show you at the end of the video that allow you to just download these markdown files from other people that do have that expertise and I put a lot of time and energy into shaping them up into a markdown file that now with the magic of these LLMs and these AI agents we can just download and use which I think is absolutely wild. The ability to transfer knowledge that quickly is is so cool. A big insight too that I've had and one of the reasons I wanted to make this video is showing that companies like Enthropic are giving us absolute gold in some of these open- source repositories if we know where to look and how to find them. Like in the case of Enthropic where they've been working with Claude code since the last quarter of last year and have all of these patterns and principles that they've worked out built into the slash commands and sub agents and other workflows that they're sharing. In addition to the open source content with Enthropic, GitHub X if you have the algorithm working for you with AI content and YouTube are also great resources. So you don't need to be an expert, you just need to know where to find the expertise. These codegen agents are quickly becoming bottlenecked by manual workflows. In this case, the obvious one is code review and I'm trying to help you unlock that. But think of the product management process, understanding what specs you want to develop and other parts of the life cycle of software development where we can employ these agents to help eliminate these process bottlenecks. It is absolutely critical and so powerful to take these frameworks, these markdown files from our team members or that we find online or from anthropic and then to customize them for our specific repository or preferences or organization. Think of it as starting out on this super high baseline instead of whatever you're coming up with. You're able to work off the the shoulders of giants, people that have put a lot of thought into these markdown files, and then you're able to from there customize things to perfectly fit what you're looking for. This is where frameworks really come in and are helpful, such as the OASP top 10 in different categories, which is are just security principles to go off of. These agents will really mirror the environment they're in. So, it's also important to make sure that your codebase is very clean and shaped up and of course that you maintain that through these automated code review processes. And the last pro tip principle here is to give these LLMs a role. If you think of prompting or writing a cloud file or writing a new slash command or any of the other workflows I've mentioned, it is so critical to give them a specific role. So think of something like you're a staff level site reliability engineer focused on you know X Y and Z. By doing that you're helping in the neural net of the model kind of push over to the parameters or to the circuits the the training data that is most relevant to what you're trying to accomplish. You can tap further into the expertise within the model. So, that's just an easy way whether you're prompting or building MD files to help communicate to the agent what you're trying to tap into, what kind of expertise, and from what perspective to view the work. I just want to call out that subscribing really helps the channel. Also, liking if you're enjoying this video. And as a thank you for doing that, in the description, I've linked a bunch of free examples that I'm about to show you of different markdown files for sub aent/comands and these security runners. You can super easily download these, add them to your own repository, and be up and running in no time with these exact same workflows that have been super helpful to us. All right, so starting off here, I'm going to show you the two default slashcomands that are now included with Thin Cloud Code. If you update to the latest version, you will see these without having to do any extra work. And those are slash review. If I hit this, it'll go ahead and review a pull request. If you don't include a pull request, it will just look for the last couple commits that you've created. So, in this case, I'm actually just going to run the command, but I'm going to include a couple extra keywords here. So, I'll just say, please review the last three commits. By the way, this is Super Whisper, a text to speech platform that allows me to take uh what I'm saying and have it cleaned up by Nvidia's Parakeet local model and then GPT 4.1. Currently, I have an affiliate link in the description if you're interested, but it's been really helpful to just fly through a bunch of this stuff. All right, so I'll mention that. I'll hit enter here, and you can see it has just gone ahead and looking for the last three commits. By default, it'll look for the PR that you're working on or that you just created last, but you can modify it like I just did in any capacity. Just got back to me here with this coder view. And this is going through some security critical issues, uh, strengths, and then other improvements. Just for the record, this is a fake key that I made for testing. So, uh, yeah, don't worry. You can see it also gives some specific suggestions for what the change. Typically, what I'll do with something like this is I'll look through and I'll be like, you know what, I actually think that, uh, this performance thing in the code optimization we should use. So, I'll just copy that and I just paste it down here. A lot of times I won't even say anything other than that and we'll go through and address those changes. All right, so this next one is what Enthropic just released which is the security review. This one I'm really excited about. Again, I could write something in here like saying uh ignore the fake security key, but I want to go ahead and see what it comes up with. I'll show you the exact script that it's running in a second, but the config file is very robust and just knowing that Enthropics team put a lot of time and energy into thinking through all the different security considerations and came back with this structure for the sub agent and slashcomand is very reassuring. Amazing to just be able to take that insight and workflow and have it right here at my fingertips so that I can benefit from all that security research. While it's running that, I want to show you the cloud code action repository within Enthropic. So, Enthropic has of course a bunch of amazing uh resources that I would recommend checking out and the examples both in how they format things for markdown files in cloud code or GitHub action writers etc., but also the content of them are amazing resources. So, I'd highly recommend studying these. As you can see here in the cloud code action repository, we have a bunch of different workflows. They just released the v1 of this which allows us to have a very robust API for the cloud code SDK which basically means taking cloud code and being able to put it in any TypeScript, Python or CLI or bashtype environment and that is all that a GitHub action runner is is it's a YAML configuration file that runs in GitHub on their own virtual machines and will use cloud code in that environment. So you can specify what tools and permissions and everything else that you want and it will automatically use cloud code without you having to interact with it in a synchronous way. I want to show you another repository as well which is the cloud code security review. This is actually where I grabbed the source code for the slash command and how I was able to make the GitHub action. I found it all within this repository. So you can see in the docloud file and commands we've got the security review and this is what's happening is it's first looking for the recently diffed files which this workflow alone has been super helpful. I've used it for quite a few projects and it's going through this entire set of criteria to look for and audit your security. And what's really nice too is there's a whole methodology for this structured output as to how to show all of this and uh you know how to rate like the security uh severity level but also it's got a whole workflow for excluding different things that are false alarms. What I've also created is a pragmatic code review. The code review basis that Enthropic has. I actually was able to find the source code for it in one of the repos which I'll show you in a second. And then I created a deep research report along with our own security principles and compiled it all into this repository. So you'll see we've got the slash command here which basically just sets everything up, looks at the last commits. You'll recognize this from the security uh slash command that Enthropic has. And then I just am calling a sub agent. The reason I'm doing this is to preserve context and to give additional configuration because sub aents use different context than the main thread so you're not eating up your 200k tokens which is really helpful especially for a review where that context doesn't really help the main thread. You just want to go off do its thing and come back with the executive summary of what you're looking for and then you can iterate from there. Perfect use of a sub a sub sub aent. So you can see in the sub agent itself, I'm giving a description of what it's doing, a ton of different bash commands and other MCPs that I'm surfacing just for this sub agent. And then I'm going through, and again, I've got this linked in the description for you as a thank you for subscribing. And I am just outlining all of these different steps that you can reference. And I would highly recommend checking it out and downloading it. At the bottom here, too, I've got a different format for structuring all of this, which I find to be more helpful. So you might be asking how exactly did I figure out all that I want to review in here and also how to structure this. Well to show you that I will walk through the process. So I will first start out with a deep research report. I as you can see here in chat GPT I've got I'll just go ahead and add and then deep research and that gives it into the research mode. I personally actually use Gemini almost entirely for deep research reports. one because it uses Google instead of Bing and I find that their search prowess is just a lot better and two they've got a bunch of first-party integrations for example with YouTube that allow it to just aggregate information much better. So in Gemini here I've got the deep analysis of code review for our specific startup giving a bunch of specifics there and asking for something that would be lightweight in the in the sense of something that a San Francisco based you know VC backed early stage startup would use. So, we've got robust, you know, professional code review, but in a way that's very light. And I let it create this entire deep research report outlining the best principles for how to go about a framework for code review and in specifically what to look for. I then took this entire thing and you can see all the, you know, usually hundreds of different uh sources that it pulled from to compile this. I then took it into Gemini deep think which is an amazing way to use 10 Gemini 2.5 Pros or eight I forget exactly uh at the same time that go out in parallel they think for a lot longer and they come back and aggregate the results. So amazing way to distill a bunch of information. I fed it that deep research report along with a couple other things including the exact way that anthropic is compiling prompts. So, if you go into their cloud code action repo, I this took me a while to find, but I dug down and I found this source create prompt uh TypeScript file and what this is doing is it's taking a bunch of these different workflows and it's compiling the master prompt or basically the system prompt that gets fed in whenever you are using the agent or the code reviewer that run in GitHub using the SDK for cloud code. But what's most important here is if you scroll down, you'll see that there is a whole uh process that they use for instructing cloud code. Like for example, how does it use the to-do list and manage a to-do list in the system prompt? Very very interesting. How does it gather context? All of this. So I went ahead and copied a good portion of this as a way to kind of show the type of prompting I want in the structure that's most helpful for the code review process. And I went ahead and fed that along with a few more details about our startup in the exact YAML format which I pulled from one of the GitHub actions that they have open source which I'll show you in just a second here. Between this file and the other two I just showed you. It came back and it compiled this entire security review for me and then I modified it a bit to just fit our use case specifically. But all the heavy lifting was already done. I will go ahead and open up the file and that is exactly what you are are witnessing here. So it's just a cleaned up version of that workflow and that saved me a tremendous amount of time formatting, researching and honestly there were some great ideas in here around the code review process that I hadn't thought about explicitly adding prior to doing the deep research report. Going back to our security review that we kicked off a little earlier, you can see it came back with a very concise output. It did thankfully pick up on the fake API key that I created here. That was just in order to test the security prompt. Super helpful. Obviously, this is a pretty blatant error, but just knowing that this is there, especially when I'm working on more sensitive parts of the codebase, is a huge help. I am excited to show you the next outer loop portion here, which is all about the GitHub action runners. So, you might be wondering how do you get these GitHub action runners and what exactly did Entropic open source here. If you go to your terminal, you can easily run slashinstall github app. And what that will do is if you if I hit enter there, it will open up GitHub and it will go ahead and create an ooth connection. And then what will happen is you'll have a PR created in your repository that has two configuration files. The cloud code YAML, so claw.l and then the code review. As well, these are just GitHub action runners that are added into the github/workflows directory. This is what it looks like. This is pulling from a much bigger repo, but these are all the configurations around the Claude code action repository that we're just looking at. One pro tip that I'd highly recommend is using Opus. So, I uncommented this and then just added the uh param for this model. If you look at the code review, very very similar, but we've got a entire prompt here. But all I had to do is go in here and replace the couple bullet points that it gives you by default with that prompt that I had distilled in the earlier step that I showed you. So, super easy to modify. I also added a few things such as tracking progress and I gave it access to the uh GitHub MCPs which are included by default. You can add your own custom MCPS as well which is really really powerful. But in this case, all I needed was the the already included GitHub MCP that allows it to basically edit comments so that it can track in real time and and read other comments within my codebase. And then you'll see here the security review which is also a workflow that has been added. All right. So what does this look like in practice? As you can see, I've got in my personal website here just a demo example. I've got a new poll request that I created and automatically the code reviewer that you just saw, the extra configured one went ahead and added a comment. As soon as I pushed this up and started analyzing the security, as you can see, it caught the same fake API key that I had earlier. It gave me some other suggestions that are really helpful and very similar to what we saw with the slash command. And then you'll also see down here that we've got the security action that ran. And what's really nice is it went ahead and it showed me the exact line and commented on that, which is one of the configuration details that I added to the security runner. And I don't know why it uh added both of these. I thought that was kind of funny, but it called out that one major security issue. What's nice with this too is this runner will run, but if it doesn't see any security problems, it just will will not comment anything. Showing you behind the scenes, here's a log of this code review runner. It's got a lot of analysis that it does to kind of gather context before we'll go out and for example use the GitHub MCP in order to write the comment that you saw earlier. Really interesting to audit and see exactly what's going on. Another super helpful pro tip is referencing the Cloud Code action repository specifically within this examples folder. They've got a bunch of great formats that you can borrow from. These in theory are all YAML files that you could pull right into your GitHub uh workflows section and just copy right from anthropic. So in fact I based some of what you saw off of this um comprehensive PR here that included the comment changes in the GitHub MCP. So I just went ahead and borrowed some of the exact prompt setup from these and then added my own details to the deep research workflow. One other powerful thing to think about are using other agents that others have developed or code review processes or GitHub action YAML configurations or slash commands etc. A couple of the best repositories I've found are this uh this agents one right here as you can see with over 11,000 stars. This went viral on X a couple weeks ago and they've got all kinds of different, you know, data engineer, data scientist, debugger, uh, you know, security specialists, like all kinds of different, uh, workflows that you can borrow from. Sometimes I would grab like a couple of these like out of this repo and, uh, this other repo here. You can see cloud code templates with, uh, 5.4K stars. And then I can uh aggregate all these together in a prompt like I showed you with deep think but you could use any LLM to take all that synthesize all that together pull the best ideas from each of these configuration files and then aggregate that into one review process for example. These are just a few ideas for ways that you can leverage the power of the knowledge of the workflows that other people have created. And then my friend Nod created this cloud code commands site where you can view and download different commands such as this amazing one for optimizing prompts Lara which is one of my favorites. Just borrow from the best of what people have developed and aggregate that all into your own customized workflow. So, I hope that has been super helpful and just thinking of being able to build out this suite of automations that help us keep up with the insanity when it comes to the speed and the volume of the output that these LLMs can give us and allow us to fight the the tricky risk of hallucinations and other mistakes alongside the the sheer volume of of code. If you enjoyed this video, I think you would absolutely love this Claude code designer video that I created that shows how to use a workflow similar to this to give Claude Code superpowers when it comes to UI to be able to use the visual side of its LLM, the vision modality in order to iterate to much better UIs by using screenshots through the Playright MCP. With that, thank you so much for watching and please comment below if you have any ideas or suggestions for future videos you'd like to see.