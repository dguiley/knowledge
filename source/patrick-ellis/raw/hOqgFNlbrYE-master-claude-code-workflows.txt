If you're using Claude Code by just typing in prompts as though it's another ChachiBeT, you're missing 90% of its value. Claude Code comes off deceptively as just another lightweight command line tool. But really, under the hood, it's much more than that. It's the first in the coming wave of highly powered AI agents. Understanding how to harness that power is critical. And I think what might be holding you back from being fully blown away by Cloud Code's capabilities. My name is Patrick, a CTO and co-founder of an AI native startup who's been using Claude code since February. Earlier this week, I spoke to a group of founders in Seattle alongside my friends Anod and Galen about the core principles to employ and the tactical frameworks and tools to get the most out of cloud code. We will cover a range of what we feel are the most valuable topics, including using MCPS to give eyes to Claude code, using the double escape method and resume method in order to fork Claude code context and spin up multiple instances. One of my favorites and extremely helpful automated code review, a quick state of the union of the top codegen tools including Codeex and others, the my developer prompting trick that Galen demos towards the end of the talk. That was actually one of my biggest takeaways. how we structure validation steps to ensure Claude knows what is good and what is bad output and many others. In the description, I've greatly detailed each section and the topics that we spoke through. So, if you're already familiar with the topic, I'd highly recommend just skipping through to what's interesting and relevant as there's a lot of good gems in there in my unbiased opinion. And I've also linked the slides from all three of our talks so you can reference that. I hope this helps you unlock the next level out of cloud code. And with that, here's a nod to kick us off. This is the main question that I get asked uh quite a lot. What's the difference between cloud code and cursor? I think it sums up to this. Cloud code is really good at multi-step processing. So you have a large task you want to get done. It can break it down into subtasks, execute them one by one. I use it for starting projects constantly. I post new project like every every other couple of days. It's just because I'm able to create a really really good spec, really good planning document, which I'll get into later. give it to cloud code and I just let cloud code run freely which is probably not great to do but you know there's no stakes to like these random side projects and it's really good at doing that and it's again because of that reflective loop um if there's a lot of complexity this is what I mentioned just a minute before where you have to pull in a lot of things from different files it's good at doing that as well and if you have a very long running process I tend to prefer cloud code cursor is still really good though at specific solving specific problems addressing very specific files or lines of code because you can select those things very easily. Let's go into the cloud MD file. So this file is your main context file. So when you run /init and we'll get into what that means in a sec, it will generate an overview of your codebase. It'll go through all your files, figure out how everything is set up, make detailed notes about, you know, the startup processes, where everything is located. And uh I like the quote from the documentation which is that your cloudmd files become a part of clause prompts. I mean that's what literally happens. Um so they should be refined like any frequently used prompt. So effectively you have to think of it like a readme for specifically built for claude code. The init function is great but I have my own set of commands that I run where I ask it to go through every file and first of all extract the file structure and the folder structure. Then for each of those folders I have a new cloud MD file. So in each subfolder I have a cloud MD file and effectively you can create detailed notes for every single detailed readme you know for every single subfolder to the point where you can track every single function and file and then when quad needs to do an operation it's no longer you know gpping like crazy you can have it just look at those cloud MD files as long as they're updated it will drastically reduce you know how much cognitive load it's over it's it's taking on yeah >> so the cloud MD file is it more like a me or is it more like uh the cursor you know rules file like how do I >> it's kind of a mix it's more like the pro it's like a you know cursor rules are effectively prompts that go into cursor right so it's but it's also a read me of your entire codebase so it can act as both but here's a good example of what I have in my actual cloud files right so I have this is like the the main you know uh I have a back end and a front end so I say okay here's how it's set up I have my front end has this kind of setup my back end has this kind of setup um and I have it cover the front end structure which actually then also has a much more detailed list in the frontend folder itself. Then I have my backend structure which is in the backend folder uh but you know in more detail but I can extract extract those up into my you know project folder or company folder. So it's easy for me to also see but also easy for like if I have to run thing cross code uh codebase or cross repo it's a lot easier to manage them and some other MD files that are not as uh talked about I feel but are very useful are things like the change log if you have a good change log it's easy for cloud to over time realize what were the changes that were made and why. So every time you make a change, just ask it to update the change log separately from the cloud MD file. And that just gives you a gives it a good understanding of okay, here's why I changed it and here's why we shouldn't go back to doing this. I use a plan MD file for every new project that I start or every new task that I start. It's effectively the list of things that I actually want to get done in a single document. All right. Now, here's the one major thing that I want to touch upon that I think is really interesting. You can attach cloud code into your GitHub repo. This basically replaces well effectively it is Devon but using your you know your your anthropic key. It's super easy to set up. You just run this command. They automate everything for you. It's really there is no hassle. Um and then you can go in and create an issue. Like I had some project I wanted to do is I was all right create this component for the library and I tag Claude like I would tag some developer. And it goes in it creates the issue. It creates this to-do list, this checklist, executes that, and later when I actually am like, you know, hey, this was like not that great, I can just tag it again in the same in the PR like I would with an actual developer, and it'll go through and do that again. I can do way more things using this process than I can just on my own uh computer running one agent or even multiple agents at the same time. It's extremely convenient. Um, it also has cloud code has builtin commands like review PR comments that allow you to effectively automate the review process from your console. You know, fetching the comments so that you can then it can then operate in your local environment. These commands are built. I I would really recommend exploring them. And I could just run 30 different commands at the same time. I use cloud code to say here's a list of all the features I want to build. generate PRs for every single one of these and make sure you tag Claude at Claude at the end of it so that it will you know spin off the job and it was able to do that. This is my phone like by the time I got home it was all complete. It was just amazing. >> This did with the integration with GitHub or on the CLI. >> No from the CLI you can integrate it into GitHub. Yeah. So it's effectively an app that running that's running there but it is the cloud code bot. And now we'll get to commands. This is a this is what a command is. command is just a prompt. It's just a prompt that you can save in a file, share amongst projects, share amongst your team. You can develop these like very comprehensive step-by-step things and you can actually run it just like you would the PR the install GitHub that were built in from cloud. You can you can write your own. And why this is great is because you might have specific things you want your team to know or somebody on your team is just a claude expert or a or AI prompt expert. They write something amazing. You can now share that with the entire team. super easy to use. You should really look into using commands which allow you to create these comprehensive workflows just using a prompt that claude can then easily follow. This is for example the codebase analyze prompt that I can use to set up my really really comprehensive uh analysis in a cloud MD file. And you can see like it's super long but it works. I even made a GitHub action that can then be run you know right right from your GitHub uh uh interface. And these are this is just all the commands that I that I've collected. Anyway, here's the uh website and cure code if you're interested. I've just made it really easy to add this. Disclaimer is that I made this. Um but I really do believe it's a really good way to share those ideas. You can also launch sub agents. If you're just starting off with cloud code, don't even think about this, but just like you can basically run two things at the same time. It's pretty cool. >> So the analyzer command was a command you were talking about to generate the code MD files. >> That's one I use to make a more comprehensive cloud MD file. It can just be to analyze the codebase in general. Hey everybody, my name is Patrick. I have been using Cloud Code since it came out back in February 24th, uh, which feels like forever ago, but it's been amazing to see just the constant evolution of new features. One of the coolest things I feel like with cloud code is how the anthropic team very obviously works closely together on the ML and AI side. So the actual machine learning researchers that are doing the post-raining and the fine-tuning and everything uh along with the actual product team. So you see this close coupling with claude code that I think really sets it apart from any other experience. I was just listening to the Klein founders on the latent space podcast. Excellent recommendation by the way that podcast episode and just the podcast overall. And they were talking about how Opus 4 and Sonnet 4 just so badly want to use bash commands uh to GP around as as Anad was speaking to. So there's a there's a whole realm of uh preferred bits of what Opus 4 and Sonnet 4 try to do that fit really really well with cloud code given that the application and machine learning teams are are speaking closely together. So that's one reason why cloud code is is fantastic to use. But I'll speak a little bit to the the fundamentals of cloud code. So what makes this more exciting and interesting and like thus all the hype recently over a cursor or other platform? From my perspective, the biggest pieces here are we're we're actually we're doing much more than just codegen. We're really working with one of the first in production um like agentic tools that can do multi-step processing on the order of roughly an hour or so. You can think a lot broader than just codegen in terms of applications for this, which I'll get to a few of my favorite non-coding workflows in a second here. But I think just feeling the character, the nature, uh what helps these agents run for longer and to get more accurate towards what we're actually trying to execute with them. All of these factors are really really helpful lessons for us to be learning and internalizing now as we're building agentic workflows in other domains in our own products or using tools such as Gemini to summarize YouTube videos or whatever other workflows we might have. So that's one amazing part of cloud code. Cloud is also fine-tuned for as I mentioned tools that cloud code takes great advantage of. So you've got the bash type commands. So, being able to GP your codebase and use the GH or GitHub CLI tool, but we also have the native tooling. So, web search, uh, file search. One of the my favorite bits is this to-do list. Back in the day, I'd always create these PRDs, which is still a helpful workflow, but for most things, I can just defer to cloud code doing the uh shift tab tab to get it into planning mode, think through and iterate on the the spec of what we're trying to accomplish, and then allow it to create a little to-do list to kind of keep it on track, especially when it's handing off between different steps and using sub agents to summarize different parts of the codebase or think through and do research. Being able to pull that back and keep grounded in a to-do list, even its little like six bullet to-do list is super super helpful. A few other tools such as its ability to reflect on what it's outputting is a absolute game changer. I see this happen quite a bit where it'll work through something be like, "Wait a second, this actually isn't the best approach to this or this assumption was mistaken." And that ability, as you can imagine, when you're trying to let it run on a task and come back in 15 minutes or whatever to verify the output is super helpful. It's just one less touch point and usually multiple less touch points that you're having to go in and babysit the model for in addition to the output just being much better with that reflection piece. So there's a number of reasons why that pairing of Opus 4 specifically, but Sonet 4 as well with cloud code is a really incredible and productive workflow. One note too, uh, when you're using cloud code, if you run forward slashmodel, you can choose sonnet versus opus. So, just in case you're not aware of that, the default is sonnet. Opus is four times more expensive, but if you're on the max plan, it's uh, which is a hundred or $200 a month plan, which I'd highly recommend. The amount of inference we get is ridiculous. I mean, I would I would estimate if I'm fully using cloud in a month, it's, you know, in the order of three to 5k in terms of like API cost, but it's $200 a month flat. So I don't know how long this is going to be along around or if they're going to try to water it down like cursor or others. So I wanted to mention the different types of agents to just give a quick overview. We've got of course chatbased agents which we're all familiar with chatbt gemini etc. We also have these CLI and IDE based agents cloud code of course being an example cursor windsurf uh the brand new Cairo or Kro from u AWS or Amazon client etc. And then we have background agents which are just starting to kind of roll out over the last couple months. So Codeex which also has a CLI tool but of course they've got uh with OpenAI's pro plan at least you can kick off agentic processes that will run anywhere from one to four different instances of 03. And then I would also loop the GitHub integration which I won't belabor since Anod talked about it but it's incredible. One of our friends Sam just walked me through this absolutely mind-blowing workflow that he's got. He basically took the integration that you can build with cloud code with GitHub and then he uh modified the YAML file because basically what it's doing is it's just creating a YAML file that's a GitHub action configuration file and then you can add additional details. So you can modify the prompt that it's running. You can might I would highly recommend uncomment the uh the model it uses so that you can use opus instead of sonnet the default which is it's it's in there as a default. You just have to uncomment it. With this you can add additional parameters. For example, basically sneaking in MCPs into your config file and also give it permission to use different bash tooling and then give it access to other configuration files like markdown files. So all this to say just through that GitHub integration there's a lot you can really squeeze out of it to essentially create as Anad was saying a Devon type experience but with much more control and with better models. What's also so cool about this is as Anod was saying, you can embed any process that you have internally around uh like amazing ways to go about code review and uh also based on the user that's submitting the PR, you can uh you you could change things up as well. So you could really get these parts of your workflow embodied within these commands or these uh these runners. And that's one thing I love about cloud code and also MCPs is being able to encapsulate these different workflows that we have internally which even just as one dev it's helpful but across the team incredibly helpful to embody that uh that knowledge and that uh ability to be super productive and hand that off to less junior folks that don't have to understand all the underlying details. So we have background agents. Again, GitHub integration being what I would consider part of that. And then kind of connected but separately uh a little bit more advanced is our agent swarms. These are really cool. It's basically spinning off a bunch of containers. Um codeex is essentially this where you can go from one to four. If you have four of them running at the same time, you've got all these agents running and then they're um coming up with a solution and then you can compare either manually or through uh LLM as judge. My friend Sam was walking me through this workflow as I was mentioning where he's got three Opus instances that kick off and then they've got acceptance criteria that they can look at for what good code looks like, different style guides, examples of of of uh like API documentation and um API spec standards and it will compare outputs against that and an LLM will choose which version of those, you know, three outputs it likes the best and then we'll automatically merge that in, you know, build a CI/CD pip pipeline and then he can review it at that point. So you can get pretty sophisticated with the swarm idea. That's a more basic version. And then at the AI engineer worlds fair that a few of us went to down in SF about a month ago, uh we saw examples of I mean hundreds of these containers, you know, being kicked off. Now of course that's uh would bankrupt me with Opus 4, but it's it's exciting to think about. Then of course non-engineering agents as well such as uh Manis and Deep Research and others we're familiar with. my favorite cloud code in the CLI and also in headless mode. They've got an SDK for TypeScript, Python, and then of course just uh in the CLI as well. You've got this little intelligence that you can, you know, pipe things into in your terminal. You can put it in your build pipeline. You can have it review and build all kinds of different stuff that's like right where you're at or within your application. And I think kind of thinking about cloud code not as just a code gen tool but as this agent that you can deploy in a bunch of different contexts is really powerful. I also love Gemini CLI very similar to cloud code doesn't have the magic but for other tasks. One of one of the coolest ones I found was somebody using Gemini to basically watch uh a YouTube video which they can see one frame a second and they have of course a transcript as well through Google's uh first-party integration with their YouTube uh tool. I do this all the time. Even before I watch like a hour talk, I'll just summarize it. Domini.google.com and we'll basically get a sense of what it's talking about. Or if I like for this talk, there was one detail I remembered from a a a claude talk, but I didn't want to go through the entire hour to try to find it. I just quickly asked like, "Hey, I remember this point like roughly speaking, where is the time code for this?" And it pulled it up. Super helpful. And and this is just one workflow with YouTube, but super helpful. Another cool one though is with the Gemini CLI, you can take a tutorial and then have it try to execute and build that locally on your computer if it's something that would be doable from like a command line or using you know different tooling that you're exposed to it. So very versatile. Okay. So what agents need for great uh performance context. Context is everything. Context truly is everything. As you guys probably know, prompt engineering just got rebranded to context engineering. given that what we fit into the model, what we give them, the analogy I I pulled from, I believe it was the anthropic CPO, uh, who's also the Instagram founder, but the way he was talking about it is imagine your cloud code. You wake up, you're in this box, and all you have is what some person just handed you, i.e. the prompt. It's going to be extremely hard to to do anything productive with that if you've got limited context, limited tooling. So giving the context of the codebase, architectural style, what our preferred libraries are, different like UI mocks and style guides. I mean anything that can help it understand like examples of of good output and bad output, what it needs to do along with evaluate the output. So again, examples of good and bad llinters are are super helpful. I mean, I I just have it run eslint every time it's doing anything because that uh just saves me a ton of time. You just want to keep that agentic loop going as as long as you can and give it as much uh feedback in real time as possible. Any standards so uh like you know around commits and branch naming for example acceptance criteria automated tests and then also tools. So different MCPS are the easiest way to expose these but also the built-in web search and bash uh GitHub CLI. There's a lot of other tools you can give these models to perform and uh do much better. There's a lot beyond engineering too that these agents are great at. Second brain uh which is basically like a methodology around personal knowledge management and like note-taking which can be really helpful along with different computer administrative tasks. So like naming screenshots based off the content and what's in there organizing files automatically of course you know pipe oper operator. There's a uh MCP for blender which is really fun to create 3D models. I haven't used it myself but I've seen some amazing demos. Getting close to time so I'll just really quickly go through the rest of the slides here. Different types of MCPs that can be really helpful. These are the main categories of uh functioning the type of MCP. These are some of the best registries of MCPS where you can find them. >> There's like behaviors with React that are terrible for users that end up happening when you kind of just throw a lot of code together. >> Yeah. >> Um and fixing it is hard. And I figured there's some way to do it if you have some MCP that's going to load it and then kind of output, you know, the progress to some format that's going to be read by by an LLM. I don't know what that is yet. >> You know, I don't have a good solution to this. One thought though is maybe having it input break points to get it to kind of pause at different UI states and then take a screenshot is maybe one interesting approach just to throw out there. But uh great question. All right, I I'm I'm out of time. Uh unfortunately, uh I'll share these slides though. There's there's a lot of stuff in here that I'm really passionate about, but I want to make sure we have we have time here. So big picture, I can't believe we didn't cover this yet. This is what you want to do every time you are using claude code on the command line. How many of you actually use cloud code? Have used it. You've all used it. Okay. So, you know this. So hopefully I'll give you something a little more interesting, but explore plan execute. If you jump straight to execute, I do this sometimes. I'm like this this is going to be so easy. Claude is dumb and it will screw it up. Um I actually find that Sonnet 4 with thinking hard is better than Opus for a lot of tasks and it's faster. Um, so but you know your tax comp task complexity may be different than mine. So my goal is to make here is to make claude spend tokens to build up context. You can read the markdown file. I don't mine's never up to date or it reads it and it starts you know imagine you read 300 lines and someone's and about like how someone's codebase works and they're like now build this. You're going to mess something up. So prepare to work on this. Claude starts with an idea of what it's going to work on and it's like okay it's just like you like all of you you're like okay you start reading and then you're like okay I know how to build this and you stop reading and you're like I'm ready to build if you're like read the code it will read a little bit more but if you're like prepared to discuss how our front end works will spend 50,000 tokens over seven minutes just being like okay and then it'll give you a nice overview of how it works and when you do that cloud is much smarter and if it If the overview is wrong, escape escape or slashcle. Start over. Don't try to correct. You can try to correct it. I do it sometimes. Sometimes it works. But you're just basically chewing through tokens in your context window trying to push back on somebody who on a bad contractor. Just fire the contractor, get a new one. >> If it is wrong, what else can you put in there to make it right the second time? You just like rerun it. See? >> Just rerun it. It's gonna reuse a bunch of sub aents. It's going to get it right. It's going to be right nine out of 10en times. This is a great like this is a great gambling game and you just when you lose you're not like oh why did I lose you're like no I win almost all the time I make just markdown files I have claude write them so like talk about how our architecture works for you know and then make a checklist of like this is what we're working on this is like an old one obviously don't write any code uh this is like maybe if you have a PR consider the next one review relevant but I actually think this is a lot better we're going to work on the document identification part of the app dig and read relevant files, prepare to discuss the ins and outs of how it works. Sometimes I'll follow up with questions just to make sure it actually has the context. Um, and often I will double escape to remove that from the context if I think it's doing a good job just because I burn like I don't want to I like a lot of room in the context window. So double escape. How many of you use double escape with cloud? Okay, you should use all you should use this all the time. So I just spent seven minutes building up context. this person's this this contractor is really good at this. I can double escape and just fork the conversation. Like I can have it do a bunch of work, double escape and go back to this same point where they have all this context. Saves me money and time like mostly like I won't get kicked out of my max plans as soon as mostly just like I don't have to sit there and wait and maybe get the get a bad gamble. If you get a smart cloud, you should keep it and reuse it over and over and over. Um so this is what it looks like. You double escape and you can just go back to any previous conversation. This is a crazy branching multiverse. So you can open up a new tab. You just built up a bunch of context. Open up a new tab. Hit resume and you get all that context in the new tab in terminal. So you can do like five terminals all with all of that exact amazing front end or backend or API context. You can you can ask a couple of questions and start there. Whatever you wherever you want. Just don't do this and then start like having it write three different things on the front end. Go. Do you prefer like git work trees or just different directories or how do you go back? >> I prefer to not work on more than two tasks at a time because my brain gets fried. I end up with 15 tabs open. I go back to a tab. I'm like, wait, what's that tab? And I'm like, oh my god, I cannot make this decision right now. This is like, why did I even start down this path? I just have like two work trees, which is just like your entire git library like in a parallel case. And I will just like merge them into master. I just keep them open. They're just sitting there because I don't care. Uh I don't use get appropriately. Um so plan I don't use plan mode. The the three or four times I've tried it like didn't do as good a plan as me asking it to do a plan. I like to think hardest. This is where you really have to think. Claude needs to think hard to plan. Um so this is my generic instructions. I really like this. Write the function names in one to three sentences about what they do. write the test names five to 10 words like about the behavior they cover, but really the short overview if like because Claude's default for plan is often like here's a bunch of code that I'm going to write and you're like no I want you to think higher level than that. I want you to tell me conceptually what you're doing because when you start doing code like that you're starting to get into the weeds and you're not thinking architecturally. So this is like a this is a different example I have like I actually have built up this whole system for adding new PDF types. So, I have like a like a whole system where I like basically take a PDF and I throw out a Gemini, but I have different types and I have different verifications I want run on them. I just have it read a couple of guides and then I just let this run. So, there's no context on this. I can just put this into GitHub and then I go to cloud and I'm like, do gh issue 140. Close it when you're done. And then I just hit like auto accept. Goodbye. Risk based planning. If it's small, don't overthink. Just write the code. medium to large, you've got to break it into like testable, deployable PRs. Um, and that's I think of this in terms of context windows, you know, and that's like about a PR sized for me. It's about a PR- sized chunk of work. Um, and then high-risisk I'd take I think you should take three shots at the plan, two or three shots. Um, you should really work over it like with Claude. I'm not making the plan again. Like I'm just looking at its plan and I'm like this smells bad. Like this is a terrible like if an engineer came to me, you are an engineer. You come to me with this. Like I'm like this is really complicated. It's going to be like you're going to screw it up. It's going to mess up the codebase. So once I've done the plan, I open up a new tab, pull up that same amazing context, but don't dive into the plan. Don't don't don't get like once once it's made the plan, it's not going to critique itself. But if you go back to the amazing context and you're like, "Yo, my developer came up with this plan to do this." Claude's like, "Yeah, all right. let me tell you about this plan. I'm with you. I'm on your team, not on your developers team. If you're like, I came up with this plan, it'll like tell you a lot of nice stuff. It'll be like, great job. You did a great plan. Here's a couple little things you might do differently. But this case, it's going to be like, yeah, your developer, you know, like I don't know, I wouldn't have done it that way. Um, and try to get specific. If you're just like they made this plan, it's like it's not going to do a good job. But so ask the questions you would ask yourself. Um, so get feedback on the plan. You can make have two clouds make the plan. Um, you can have a third cloud decide between them. Um, I tend to put them into markdown and have cloud work on them and then I have it break them up into PR sized chunks and then we execute. and those PR size chunks, you might as well use that same context that you've already built up because it's so valuable to have those 50,000 tokens about your database or sorry your your app in the in the context window. It's going to write much better code than if you just like bring up a blank claude with 200 lines of the cloud MD. So pull up that 50,000 token context window. Say work on PR1. This is my example prompt. Think hard. Write elegant code that completes this. This is a real big one. It loves backwards compatibility, which I don't. I'm like, "No." I And like it's like, "And we'll have graceful fallbacks." And I'm like, "No, that's just junk that will break." And then it will gracefully fall back. When you say that, that means to me that the app is going to silently fail and I will not know about it because it will just start leaning on some old code that you should be deleting. Um, this is you can tell where I get frustrated with cloud. Um, so I try to this is a little overkill like the testing and sometime but I think linting compiling and and writing corresponding tests is good for really simple stuff. I actually just say like do TDD and it writes the test. It writes the failing test. It writes the code that makes the test pass and it does a great job. It's really good. TDD is terrible. When I did code I remember trying it for like a week and being like hate TDD. This is just like this is worse than writing tests. Um, so I like thinking hard or think for this. Um, I write have Claude write lots of scripts to check its own work. So like I gave it a script to call Gemini with PDFs or I had it write that script and now I'm like test to make sure that like when you verify like you created a new markdown file that verifies PDFs, make sure it actually works and it verifies with this one. Or if you need to view a PDF file, Cloud's terrible at that. It can't do it. Ask Gemini or ask Unstruct. It will give you a markdown file. go, you know, look at that. Then you can read it and understand what's going on and you can like figure out what what to do. This is a big question to watch or not to watch. Do you like because Claude will make in my case like one out of 10 to 20 times it's going to start copying code and just doing some dumb stuff and I'm not going to look at the commit. I'm going to watch it as it goes pretty much or I'm not going to watch it at all. I'm going to be like commit it. It works. It's good. Um, so I've seen 200 lines of copied code go through. I have a weird config that's in five different places in my app and I'm just like every time like Claude, could we just can we just put this config into one place and it's like oh yeah here's a plan and I'm like all right to your stupid like this okay this is harder than it looks I guess. Um return true was a 3.7 problem. You will not get that anymore. Um but usually if you just hit go like you kind of get a feel for it. I have a feel for it now where I'm like this is an easy enough thing for claw just to do. go shift tab, you know, puts it on to autocomplete. Um, I don't know if you all have heard how this came to be like why we have amazing coding agents now, but it's because of RL and it's because once the models once you're once you move up the tech tree enough where models can write good compilable code, you can actually then start to have give them coding problems and then and figure out if like basically they have to be able to create the solution like 80% of the time or tasks like the way that they did thinking was they were just like write a bunch of stuff and at the end if you get the right answer you get a cookie and we're going to reward that circuit and if you don't you don't get a cookie right and if it like so we got to like GPT4 and claude 35 level models and you could start actually like turning thinking on but because the models were good enough to get all the way through but uh the problem is like you're creating software engineering problems and you're just like and they're verifiable. So like write this code, does it compile? Does it answer the right question at the end? Very easy to test back, right? But does does that make for good edits? No. That makes for really good writing fresh new codes, new methods. Claude prefers that. I don't know if you've all noticed that, but you're like in my case, you're probably if you're editing stuff like cursor style, it doesn't matter. But in my case, I'm like, write this. Edit the code. figure out where you can edit because you really have to prompt that because Claude is still really tuned into like, okay, I'm gonna write some new code. This is gonna be fun. We're going to do a new method, guys. Uh, so, uh, sorry, it got cut off here, but yeah, Cloud37 was like over RLED on just like completing tasks and they dialed that back. That's where the ret return true came from. But we still have this problem where claude is like just trying to finish like it's trying to finish tasks and get its cookie by writing new code, not by editing or elegantly integrating code. Uh so then I go back to the developer thing, right? I lean on the developer. Uh so I go back to that the planner. So I have my planner tab open and I'm just like, yo, my developer just finished step two. Give them lowlevel like feedback and highle feedback. If you don't say that, it's like they did a great job. Um, so and then I get feedback and then I go back to the developer and I'm like, "Hey, I got this feedback. What do you think?" And it's like, "It's good feedback. Yeah, I'll do it." Um, and this is the problem with like Claude, I don't know if you've hit the slashre on cloud zone code. It's like this code's great. Review doesn't cloud likes cla's code. Um, I use this sometimes, but uh, prepare. So I'm like at the end as I'm running out of my context window or we're finishing up the poll request like pay like I say to Claude like tell some give the next you're not working on the next step of this give advice to the next developer put it in the markdown file and cloud is usually like you you're off to an excellent start here but uh it can be helpful uh context window management I'm sure do you all get this? I like I never compact anymore. Compact is a waste of time. it generates like a page and a half and it tells Claude to read four files and you're like you you end up with a very like kind of off offkilter dumb cloud. Uh so I just try to rewind. I try once I get to 5% I'm like document what you've done and we're rewinding back to 40%. And we're going to like and I'm going to be like here's what here's what I've done so far. Continue. >> Sorry. So you rewind instead of fighting with >> Yeah. I hate compact. I hate And yeah, starting with clear. >> Yeah. Yeah, I mean you could use clear but then you don't have any context and so I like and you can use resume to get that context back or double escape so like why not use I mean it is more expensive because you're you already have all that like you're I don't know that you're actually getting charged for you only get new tokens right so you're not getting charged it's much more expensive for them but for us it's just new tokens so it's great >> um so you jump straight to execution go ahead >> oh yeah yeah um other tips and tricks. I get merge. That's the problem with work trees is I'm like, "All right, we're going to do this over here. We're going to do this over here." And then we're merge them. And I'm like, "Oh, Jesus." Now we have a merge conflict. I'm just like, "Cloud, there's deal with git." And it's like, "Okay." And it gets it right every time. I'm like, "Oh, this is should I like trust it?" And I'm like, "I don't know." Every time it works. Uh I skip the ceremony for simple tasks. Just like do it. Um, Claude loves to be enterprise ready. Just you have to fight that because like it's, you know, built by an enterprise for enterprises. So, um, so this is one of my like if if it gave me a plan that's too bulky, I love I love this and it's just like totally right and it cuts it in half and it makes it a much better plan for me. Um, explore plan, execute, resume my developer. Um, and then Claude made up this joke at the end for me. I didn't add this, but I like it. It's a good one. All right, so that's my talk. I hope you found our talks helpful. And if you did, I'm sure you would enjoy one of these two videos on how to become AI native as a software engineer and a founder, specifically within codegen tools like cloud code. And with that, don't forget to subscribe for more content like this. Thank you.